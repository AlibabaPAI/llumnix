#!/usr/bin/env bash

# 1: config, 2: imean, 3: omean, 4: enable_load_control_prefill, 5: dispatch_mode
# 6: global_dispatch_strategy, 7: qps, 8: query_distribution, 9: coefficient_variation, 10: enable_migrate
source $1 $2 $3 $4 $5 $6 $7 $8 $9 ${10} ${11}

# Run real test

export PYTHONPATH=$PYTHONPATH:/mnt/wencong.xwc/sunbiao/develop/vllm/vllm/simulator

if [ ! -d "./results/"$results_path ]; then
    mkdir -p "./results/"$results_path
fi

config=$1
config_filepath=$(readlink -f "$config")
config_basename=$(basename "$config")
target_filepath="./results/"$results_path"/"$config_basename"_$(date '+%Y-%m-%d_%H:%M:%S').config"
cp "$config_filepath" "$target_filepath"
echo "cp $config_filepath to $target_filepath"

function start_model_server_nsys {
    local max_num_batched_tokens=$1
    declare -g output_filename="./results/"$results_path"/"nsys_$(date '+%Y-%m-%d_%H:%M:%S').log

    ulimit -n 65536 && RAY_DEDUP_LOGS=0 \
    nsys profile --trace=cuda,nvtx -y 5 -d 60 --sample=none --cpuctxsw=none -o outlier_test python -m vllm.entrypoints.api_server \
        --port $PORT \
        --model /mnt/wencong.xwc/models/huggingface-models/llama-trained/llama-7b/ \
        --swap-space 16 \
        --engine-use-ray \
        --worker-use-ray \
        --instance-parallel-size $instance_parallel_size \
        --use-np-weights \
        --max-num-batched-tokens $max_num_batched_tokens \
        --load-metric $load_metric \
        --enable-load-control-prefill $enable_load_control_prefill \
        --prefill-SLO 5 \
        --dispatch-strategy $dispatch_strategy \
        --need-migrate-frequency $need_migrate_frequency \
        --migrate-out-threshold $migrate_out_threshold \
        --migrate-in-threshold $migrate_in_threshold \
        --disable-log-requests-engine \
        --results-filename $output_filename \
        --dispatch-mode $dispatch_mode \
        --need-dispatch-frequency $need_dispatch_frequency \
        --global-dispatch-strategy $global_dispatch_strategy \
        --migrate-strategy $migrate_strategy \
        2>&1 | tee $output_filename &
        # --tokenizer 'facebook/opt-13b' \

    while [ "$(curl -s http://localhost:${PORT}/is_ready | grep true | wc -l)" -eq 0 ]; do
        sleep 1
    done
    echo "model server started on port $PORT"
}

function start_model_server_migrate {
    local max_num_batched_tokens=$1
    declare -g output_filename="./results/"$results_path"/"enable_migrate_$(date '+%Y-%m-%d_%H:%M:%S').log

    echo "enable migrate"

    ulimit -n 65536 && RAY_DEDUP_LOGS=0 && CUDA_VISIBLE_DEVICES=0,1,2,3 \
    python -m vllm.entrypoints.api_server \
        --port $PORT \
        --model /mnt/wencong.xwc/models/huggingface-models/llama-trained/llama-7b/ \
        --swap-space 16 \
        --engine-use-ray \
        --worker-use-ray \
        --instance-parallel-size $instance_parallel_size \
        --use-np-weights \
        --max-num-batched-tokens $max_num_batched_tokens \
        --load-metric $load_metric \
        --enable-load-control-prefill $enable_load_control_prefill \
        --prefill-SLO 5 \
        --dispatch-strategy $dispatch_strategy \
        --need-migrate-frequency $need_migrate_frequency \
        --migrate-out-threshold $migrate_out_threshold \
        --migrate-in-threshold $migrate_in_threshold \
        --disable-log-requests-engine \
        --results-filename $output_filename \
        --enable-migrate \
        --async-migrate \
        --dispatch-mode $dispatch_mode \
        --need-dispatch-frequency $need_dispatch_frequency \
        --global-dispatch-strategy $global_dispatch_strategy \
        --migrate-strategy $migrate_strategy \
        2>&1 | tee $output_filename &
        # --tokenizer 'facebook/opt-13b' \

    while [ "$(curl -s http://localhost:${PORT}/is_ready | grep true | wc -l)" -eq 0 ]; do
        sleep 1
    done
    echo "model server started on port $PORT"
}

function start_model_server {
    local max_num_batched_tokens=$1
    declare -g output_filename="./results/"$results_path"/"disable_migrate_$(date '+%Y-%m-%d_%H:%M:%S').log

    echo "disable migrate"

    ulimit -n 65536 && RAY_DEDUP_LOGS=0 && CUDA_VISIBLE_DEVICES=0,1,2,3 \
    python -m vllm.entrypoints.api_server \
        --port $PORT \
        --model /mnt/wencong.xwc/models/huggingface-models/llama-trained/llama-7b/ \
        --swap-space 16 \
        --engine-use-ray \
        --worker-use-ray \
        --instance-parallel-size $instance_parallel_size \
        --use-np-weights \
        --max-num-batched-tokens $max_num_batched_tokens \
        --load-metric $load_metric \
        --enable-load-control-prefill $enable_load_control_prefill \
        --prefill-SLO 5 \
        --dispatch-strategy $dispatch_strategy \
        --need-migrate-frequency $need_migrate_frequency \
        --migrate-out-threshold $migrate_out_threshold \
        --migrate-in-threshold $migrate_in_threshold \
        --gpu-memory-utilization 0.9 \
        --disable-log-requests-engine \
        --results-filename $output_filename \
        --dispatch-mode $dispatch_mode \
        --need-dispatch-frequency $need_dispatch_frequency \
        --global-dispatch-strategy $global_dispatch_strategy \
        --migrate-strategy $migrate_strategy \
        2>&1 | tee $output_filename &
        # --tokenizer 'facebook/opt-13b' \

    while [ "$(curl -s http://localhost:${PORT}/is_ready | grep true | wc -l)" -eq 0 ]; do
        sleep 1
    done
    echo "model server started on port $PORT"
}

function kill_model_server {
    echo 'killing model server'
    ps aux | grep 'vllm.entrypoints.api_server ' | grep -v 'vim' | awk '{print $2}' | xargs kill -9
    wait
}

trap kill_model_server EXIT

# # Catch OOMs early.
# for i in ${!ranges[@]}; do
#     range=${ranges[$i]}
#     max_num_batched_tokens=${max_batch_total_tokens_vals[$i]}
#     echo "range $range, max_num_batched_tokens $max_num_batched_tokens"

#     # start_model_server $max_num_batched_tokens

#     ./benchmark_throughput.py \
#         --port $PORT \
#         --backend vLLM  \
#         --random_prompt_lens_mean 512 \
#         --random_prompt_lens_range 0 \
#         --random_prompt_count 30 \
#         --gen_random_prompts \
#         --fixed_max_tokens $range
#     kill_model_server

# done

# Run real test
for i in ${!oranges[@]}; do
    orange=${oranges[$i]}
    max_num_batched_tokens=${max_batch_total_tokens_vals[$i]}

    results_filename="./results/"$results_path"/vllm_maxtok_${max_num_batched_tokens}_idist_${idist}_imean_${imean}_irange_${irange}_odist_${odist}_omean_${omean}_orange${orange}_$(date '+%Y-%m-%d_%H:%M:%S').log"
    declare -g output_filename="./results/"$results_path"/disable_migrate_$(date '+%Y-%m-%d_%H:%M:%S').log"
    touch $results_filename

    if [ $enable_migrate -gt 0 ]; then
        start_model_server_migrate $max_num_batched_tokens
    else
        start_model_server $max_num_batched_tokens
    fi

    ulimit -n 65536 && python ./benchmark_throughput.py \
        --port $PORT \
        --backend vLLM \
        --tokenizer /mnt/wencong.xwc/models/huggingface-models/llama-trained/llama-7b/ \
        --random_prompt_lens_mean $imean \
        --random_prompt_lens_range $irange \
        --variable_prompt_lens_distribution $idist \
        --random_prompt_count $num_prompts \
        --gen_random_prompts \
        --variable_response_lens_mean $omean \
        --variable_response_lens_range $orange \
        --variable_response_lens_distribution $odist \
        --allow_variable_generation_length \
        --gen_random_session_id \
        --new_session_ratio $new_session_ratio \
        --session_0_ratio $session_0_ratio \
        --qps $qps \
        --distribution $query_distribution \
        --coefficient_variation $coefficient_variation \
        --prefill_SLO 5 \
        --log_latencies \
        --fail_on_response_failure \
        --results_filename $output_filename \
        2>&1 | tee -a $output_filename
    # --tokenizer 'facebook/opt-13b' \
    kill_model_server

done

python plot_utils.py --log-file1 $output_filename
