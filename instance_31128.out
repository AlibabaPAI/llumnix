WARNING 07-16 12:29:36 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.
Warning: Your installation of OpenCV appears to be broken: module 'cv2.dnn' has no attribute 'DictValue'.Please follow the instructions at https://github.com/opencv/opencv-python/issues/884 to correct your environment. The import of cv2 has been skipped.
2025-07-16 12:29:38,963	INFO worker.py:1694 -- Connecting to existing Ray cluster at address: 172.23.75.208:6379...
2025-07-16 12:29:38,972	INFO worker.py:1888 -- Connected to Ray cluster.
WARNING 07-16 12:29:43 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
WARNING 07-16 12:29:43 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 2025-07-16 12:29:43,921 arg_utils.py:129] entrypoints_args: EntrypointsArgs(host='172.23.75.208', port=31128, ssl_keyfile=None, ssl_certfile=None, server_log_level='info', launch_ray_cluster=False, ray_cluster_port=6379, disable_log_to_driver=False, request_output_queue_type='zmq', disable_log_requests_server=False, config_file='configs/vllm.yml', disable_keep_serve_process_alive=False)
INFO 2025-07-16 12:29:43,921 arg_utils.py:130] manager_args: ManagerArgs(initial_instances=1, polling_interval=0.05, dispatch_policy='load', scaling_load_metric='remaining_steps', topk_random_dispatch=1, enable_migration=True, pair_migration_frequency=1, pair_migration_policy='defrag', migrate_out_threshold=-3.0, enable_scaling=False, min_instances=-1, max_instances=4, scaling_interval=10, scaling_policy='avg_load', scale_up_threshold=-10, scale_down_threshold=-60, log_instance_info=False, log_filename='server.log', enable_port_increment=True, enable_port_offset_store=False, enable_pd_disagg=True, enable_adaptive_pd=True, pd_ratio=[1, 1], load_registered_service=False, load_registered_service_path=None, enable_pdd_node_affinity_scheduling=False, is_group_kind_migration_backend=True, enable_engine_pd_disagg=False, enable_engine_semi_pd_disagg=None, dispatch_load_metric='remaining_steps', dispatch_prefill_load_metric='kv_blocks_ratio', dispatch_prefill_as_decode_load_metric='adaptive_decode', dispatch_decode_load_metric='remaining_steps', dispatch_decode_as_prefill_load_metric='kv_blocks_ratio', cache_meta_client_config_path='llumnix/config/mock_query_client_config.json')
INFO 2025-07-16 12:29:43,922 arg_utils.py:131] instance_args: InstanceArgs(instance_type='neutral', simulator_mode=False, profiling_result_file_path=None, enable_defrag=True, max_migration_concurrency=1, request_migration_policy='SR', migration_load_metric='remaining_steps', migration_backend='gloo', migration_buffer_blocks=32, migration_num_layers=1, migration_backend_init_timeout=10.0, kvtransfer_migration_backend_transfer_type='rdma', kvtransfer_migration_backend_naming_url='file:/tmp/llumnix/naming/', migration_last_stage_max_blocks=16, migration_max_stages=3, engine_disagg_inst_id_env_var=None, request_output_forwarding_mode='thread', enable_engine_pd_disagg=False, enable_engine_semi_pd_disagg=None, enable_migration=True, enable_adaptive_pd=True)
INFO 2025-07-16 12:29:43,922 arg_utils.py:132] engine_args: AsyncEngineArgs(model='/mnt/model/Qwen2.5-7B', served_model_name=None, tokenizer='/mnt/model/Qwen2.5-7B', skip_tokenizer_init=False, tokenizer_mode='auto', trust_remote_code=True, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, seed=0, max_model_len=4096, worker_use_ray=True, distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, max_num_batched_tokens=16000, max_num_seqs=512, max_logprobs=20, disable_log_stats=False, revision=None, code_revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, quantization=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, enable_lora=False, max_loras=1, max_lora_rank=16, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, fully_sharded_loras=False, lora_extra_vocab_size=256, long_lora_scaling_factors=None, lora_dtype='auto', max_cpu_loras=None, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, ray_workers_use_nsight=False, num_gpu_blocks_override=None, num_lookahead_slots=0, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, guided_decoding_backend='outlines', speculative_model=None, speculative_model_quantization=None, speculative_draft_tensor_parallel_size=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, qlora_adapter_name_or_path=None, disable_logprobs_during_spec_decoding=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, mm_processor_kwargs=None, scheduling_policy='fcfs', disable_log_requests=False)
INFO 2025-07-16 12:29:43,993 setup.py:129] Init Scaler on current node.
[36m(Scaler pid=22179)[0m WARNING 07-16 12:29:46 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.
[36m(Scaler pid=22179)[0m Warning: Your installation of OpenCV appears to be broken: module 'cv2.dnn' has no attribute 'DictValue'.Please follow the instructions at https://github.com/opencv/opencv-python/issues/884 to correct your environment. The import of cv2 has been skipped.
[36m(Scaler pid=22179)[0m INFO 2025-07-16 12:29:48,370 scaler.py:116] Init Manager actor.
[36m(Scaler pid=22179)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[36m(Manager pid=22370)[0m WARNING 07-16 12:29:50 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.
[36m(Manager pid=22370)[0m Warning: Your installation of OpenCV appears to be broken: module 'cv2.dnn' has no attribute 'DictValue'.Please follow the instructions at https://github.com/opencv/opencv-python/issues/884 to correct your environment. The import of cv2 has been skipped.
[36m(Scaler pid=22179)[0m INFO 2025-07-16 12:29:52,734 scaler.py:825] num_instances: 0, instances: []
[36m(Scaler pid=22179)[0m INFO 2025-07-16 12:29:52,736 scaler.py:247] Auto scale up loop starts, service name: neutral
[36m(Manager pid=22370)[0m INFO 2025-07-16 12:29:52,723 ray_utils.py:299] Manager(actor_name=manager, actor_id=05d8df846631a91599ee6f1c19000000, placement_group_id=None, namespace=llumnix, job_id=19000000, worker_id=9bb2445b8dd171819ccad1ca58c2ebb44a23362f2487662888dca98c, node_id=efde01d4aad6ea33c7e355bb10aa1c9161589aa44f8fcb02c9903b4f, gpu_ids=[])
[36m(Manager pid=22370)[0m INFO 2025-07-16 12:29:52,724 ray_utils.py:307] Manager(log_dir=['/tmp/ray/session_2025-07-16_12-15-34_687273_108543/logs/worker-9bb2445b8dd171819ccad1ca58c2ebb44a23362f2487662888dca98c-19000000-22370.err', '/tmp/ray/session_2025-07-16_12-15-34_687273_108543/logs/worker-9bb2445b8dd171819ccad1ca58c2ebb44a23362f2487662888dca98c-19000000-22370.out'])
[36m(Manager pid=22370)[0m INFO 2025-07-16 12:29:52,728 manager.py:90] Launch mode: GLOBAL, backend type: vLLM
[36m(Manager pid=22370)[0m DEBUG 2025-07-16 12:29:52,779 manager.py:242] Polling instance infos of 0 instances starts.
[36m(Manager pid=22370)[0m DEBUG 2025-07-16 12:29:52,779 manager.py:245] Polling instance infos of 0 instances ends.
[36m(Scaler pid=22179)[0m WARNING 07-16 12:29:57 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
[36m(Scaler pid=22179)[0m WARNING 07-16 12:29:57 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:29:57,267 ray_utils.py:121] placement_group_specs: [{'CPU': 3, 'GPU': 1}]
[36m(Scaler pid=22179)[0m INFO 2025-07-16 12:29:58,160 scaler.py:330] Deploy server and instance to new placement group done, instance_id: 57a22bb52ba144a392776f524f531ad4.
[36m(Scaler pid=22179)[0m WARNING 07-16 12:29:58 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
[36m(Scaler pid=22179)[0m WARNING 07-16 12:29:58 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:29:58,181 ray_utils.py:121] placement_group_specs: [{'CPU': 3, 'GPU': 1}]
[36m(Scaler pid=22179)[0m INFO 2025-07-16 12:29:58,196 scaler.py:330] Deploy server and instance to new placement group done, instance_id: 51147abf42184526b2c0f62977c34d74.
[36m(Scaler pid=22179)[0m WARNING 07-16 12:29:58 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
[36m(Scaler pid=22179)[0m WARNING 07-16 12:29:58 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:29:58,217 ray_utils.py:121] placement_group_specs: [{'CPU': 3, 'GPU': 1}]
[36m(Scaler pid=22179)[0m INFO 2025-07-16 12:29:58,233 scaler.py:330] Deploy server and instance to new placement group done, instance_id: 68140c7aeecd46d69cad41d358d58f2d.
[36m(Scaler pid=22179)[0m WARNING 07-16 12:29:58 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
[36m(Scaler pid=22179)[0m WARNING 07-16 12:29:58 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:29:58,255 ray_utils.py:121] placement_group_specs: [{'CPU': 3, 'GPU': 1}]
[36m(Scaler pid=22179)[0m INFO 2025-07-16 12:29:58,269 scaler.py:330] Deploy server and instance to new placement group done, instance_id: 1ca1230e98474189bce13b5bfdcd056d.
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:29:58,290 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:29:59,316 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:00,342 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet pid=22645)[0m WARNING 07-16 12:30:00 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.
[36m(Llumlet pid=22646)[0m WARNING 07-16 12:30:00 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.
[36m(Llumlet pid=22647)[0m WARNING 07-16 12:30:00 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.
[36m(Llumlet pid=22648)[0m WARNING 07-16 12:30:00 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:01,367 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet pid=22645)[0m Warning: Your installation of OpenCV appears to be broken: module 'cv2.dnn' has no attribute 'DictValue'.Please follow the instructions at https://github.com/opencv/opencv-python/issues/884 to correct your environment. The import of cv2 has been skipped.
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:02,392 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet pid=22647)[0m Warning: Your installation of OpenCV appears to be broken: module 'cv2.dnn' has no attribute 'DictValue'.Please follow the instructions at https://github.com/opencv/opencv-python/issues/884 to correct your environment. The import of cv2 has been skipped.
[36m(Llumlet pid=22646)[0m Warning: Your installation of OpenCV appears to be broken: module 'cv2.dnn' has no attribute 'DictValue'.Please follow the instructions at https://github.com/opencv/opencv-python/issues/884 to correct your environment. The import of cv2 has been skipped.
[36m(Llumlet pid=22648)[0m Warning: Your installation of OpenCV appears to be broken: module 'cv2.dnn' has no attribute 'DictValue'.Please follow the instructions at https://github.com/opencv/opencv-python/issues/884 to correct your environment. The import of cv2 has been skipped.
[36m(Llumlet pid=22645)[0m INFO 2025-07-16 12:30:02,516 ray_utils.py:299] Llumlet(actor_name=instance_57a22bb52ba144a392776f524f531ad4, actor_id=f0cf417aedafeeadbe86fd8319000000, placement_group_id=7b8972c346aecacb8ffb436d320319000000, namespace=llumnix, job_id=19000000, worker_id=fd392502712eb77974fa11ae7c0d19f9533319196a81245732b9747d, node_id=efde01d4aad6ea33c7e355bb10aa1c9161589aa44f8fcb02c9903b4f, gpu_ids=['0'])
[36m(Llumlet pid=22645)[0m INFO 2025-07-16 12:30:02,517 ray_utils.py:307] Llumlet(log_dir=['/tmp/ray/session_2025-07-16_12-15-34_687273_108543/logs/worker-fd392502712eb77974fa11ae7c0d19f9533319196a81245732b9747d-19000000-22645.err', '/tmp/ray/session_2025-07-16_12-15-34_687273_108543/logs/worker-fd392502712eb77974fa11ae7c0d19f9533319196a81245732b9747d-19000000-22645.out'])
[36m(Llumlet pid=22645)[0m INFO 2025-07-16 12:30:02,517 llumlet.py:50] Llumlet(instance_id=57a22bb52ba144a392776f524f531ad4, backend_type=vLLM, instance_type=prefill)
[36m(Llumlet pid=22647)[0m INFO 2025-07-16 12:30:02,681 ray_utils.py:299] Llumlet(actor_name=instance_68140c7aeecd46d69cad41d358d58f2d, actor_id=9676aa444b8cdea1083cc52419000000, placement_group_id=58c4f27e78ebe07f1d1248a7e68419000000, namespace=llumnix, job_id=19000000, worker_id=c48860e5737c2f1c19d3c999bd7c1a19ea8053071c8a450a13ea1ece, node_id=efde01d4aad6ea33c7e355bb10aa1c9161589aa44f8fcb02c9903b4f, gpu_ids=['2'])
[36m(Llumlet pid=22647)[0m INFO 2025-07-16 12:30:02,682 ray_utils.py:307] Llumlet(log_dir=['/tmp/ray/session_2025-07-16_12-15-34_687273_108543/logs/worker-c48860e5737c2f1c19d3c999bd7c1a19ea8053071c8a450a13ea1ece-19000000-22647.err', '/tmp/ray/session_2025-07-16_12-15-34_687273_108543/logs/worker-c48860e5737c2f1c19d3c999bd7c1a19ea8053071c8a450a13ea1ece-19000000-22647.out'])
[36m(Llumlet pid=22647)[0m INFO 2025-07-16 12:30:02,682 llumlet.py:50] Llumlet(instance_id=68140c7aeecd46d69cad41d358d58f2d, backend_type=vLLM, instance_type=prefill)
[36m(Llumlet pid=22646)[0m INFO 2025-07-16 12:30:02,749 ray_utils.py:299] Llumlet(actor_name=instance_51147abf42184526b2c0f62977c34d74, actor_id=a17e4e9d645d71ac3ebe6fda19000000, placement_group_id=413a75b296bad9c19bcec309149719000000, namespace=llumnix, job_id=19000000, worker_id=0ed4f5f6f2423ed9c2d854e7dc1f88c63298ae290b18aa9052298f09, node_id=efde01d4aad6ea33c7e355bb10aa1c9161589aa44f8fcb02c9903b4f, gpu_ids=['1'])
[36m(Llumlet pid=22646)[0m INFO 2025-07-16 12:30:02,750 ray_utils.py:307] Llumlet(log_dir=['/tmp/ray/session_2025-07-16_12-15-34_687273_108543/logs/worker-0ed4f5f6f2423ed9c2d854e7dc1f88c63298ae290b18aa9052298f09-19000000-22646.out', '/tmp/ray/session_2025-07-16_12-15-34_687273_108543/logs/worker-0ed4f5f6f2423ed9c2d854e7dc1f88c63298ae290b18aa9052298f09-19000000-22646.err'])
[36m(Llumlet pid=22646)[0m INFO 2025-07-16 12:30:02,750 llumlet.py:50] Llumlet(instance_id=51147abf42184526b2c0f62977c34d74, backend_type=vLLM, instance_type=decode)
[36m(Llumlet pid=22648)[0m INFO 2025-07-16 12:30:02,748 ray_utils.py:299] Llumlet(actor_name=instance_1ca1230e98474189bce13b5bfdcd056d, actor_id=26fa726a471ddc54a867eb0019000000, placement_group_id=4bb2a38f3f4252566e8f876a37d219000000, namespace=llumnix, job_id=19000000, worker_id=9b33fcad908583bd386d82d94e030303d6fb92381577b85010e1d006, node_id=efde01d4aad6ea33c7e355bb10aa1c9161589aa44f8fcb02c9903b4f, gpu_ids=['3'])
[36m(Llumlet pid=22648)[0m INFO 2025-07-16 12:30:02,749 ray_utils.py:307] Llumlet(log_dir=['/tmp/ray/session_2025-07-16_12-15-34_687273_108543/logs/worker-9b33fcad908583bd386d82d94e030303d6fb92381577b85010e1d006-19000000-22648.out', '/tmp/ray/session_2025-07-16_12-15-34_687273_108543/logs/worker-9b33fcad908583bd386d82d94e030303d6fb92381577b85010e1d006-19000000-22648.err'])
[36m(Llumlet pid=22648)[0m INFO 2025-07-16 12:30:02,749 llumlet.py:50] Llumlet(instance_id=1ca1230e98474189bce13b5bfdcd056d, backend_type=vLLM, instance_type=decode)
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:03,418 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:04,443 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:05,468 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:06,493 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet pid=22645)[0m WARNING 07-16 12:30:07 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
[36m(Llumlet pid=22645)[0m WARNING 07-16 12:30:07 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(Llumlet pid=22645)[0m INFO 07-16 12:30:07 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/mnt/model/Qwen2.5-7B', speculative_config=None, tokenizer='/mnt/model/Qwen2.5-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/mnt/model/Qwen2.5-7B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:07,515 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet pid=22647)[0m WARNING 07-16 12:30:07 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
[36m(Llumlet pid=22647)[0m WARNING 07-16 12:30:07 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(Llumlet pid=22647)[0m INFO 07-16 12:30:07 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/mnt/model/Qwen2.5-7B', speculative_config=None, tokenizer='/mnt/model/Qwen2.5-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/mnt/model/Qwen2.5-7B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
[36m(Llumlet pid=22648)[0m WARNING 07-16 12:30:07 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
[36m(Llumlet pid=22648)[0m WARNING 07-16 12:30:07 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(Llumlet pid=22648)[0m INFO 07-16 12:30:07 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/mnt/model/Qwen2.5-7B', speculative_config=None, tokenizer='/mnt/model/Qwen2.5-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/mnt/model/Qwen2.5-7B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
[36m(Llumlet pid=22645)[0m INFO 2025-07-16 12:30:07,701 executor.py:61] use_ray_spmd_worker: False
[36m(Llumlet pid=22645)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[36m(Llumlet pid=22647)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[36m(Llumlet pid=22647)[0m INFO 2025-07-16 12:30:07,872 executor.py:61] use_ray_spmd_worker: False
[36m(Llumlet pid=22648)[0m INFO 2025-07-16 12:30:07,871 executor.py:61] use_ray_spmd_worker: False
[36m(Llumlet pid=22648)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[36m(Llumlet pid=22646)[0m WARNING 07-16 12:30:08 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
[36m(Llumlet pid=22646)[0m WARNING 07-16 12:30:08 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(Llumlet pid=22646)[0m INFO 07-16 12:30:08 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/mnt/model/Qwen2.5-7B', speculative_config=None, tokenizer='/mnt/model/Qwen2.5-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/mnt/model/Qwen2.5-7B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
[36m(Llumlet pid=22646)[0m INFO 2025-07-16 12:30:08,272 executor.py:61] use_ray_spmd_worker: False
[36m(Llumlet pid=22646)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:08,540 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:09,565 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(pid=23026)[0m WARNING 07-16 12:30:09 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.
[36m(pid=23027)[0m WARNING 07-16 12:30:09 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.
[36m(pid=23028)[0m WARNING 07-16 12:30:09 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.
[36m(pid=23074)[0m WARNING 07-16 12:30:10 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:10,590 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(pid=23026)[0m Warning: Your installation of OpenCV appears to be broken: module 'cv2.dnn' has no attribute 'DictValue'.Please follow the instructions at https://github.com/opencv/opencv-python/issues/884 to correct your environment. The import of cv2 has been skipped.
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:11,617 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(pid=23027)[0m Warning: Your installation of OpenCV appears to be broken: module 'cv2.dnn' has no attribute 'DictValue'.Please follow the instructions at https://github.com/opencv/opencv-python/issues/884 to correct your environment. The import of cv2 has been skipped.
[36m(pid=23028)[0m Warning: Your installation of OpenCV appears to be broken: module 'cv2.dnn' has no attribute 'DictValue'.Please follow the instructions at https://github.com/opencv/opencv-python/issues/884 to correct your environment. The import of cv2 has been skipped.
[36m(Llumlet pid=22645)[0m DEBUG 2025-07-16 12:30:12,092 executor.py:100] workers: []
[36m(Llumlet pid=22645)[0m DEBUG 2025-07-16 12:30:12,092 executor.py:101] driver_dummy_worker: Actor(RayWorkerWrapper, 84044e7174a637cf31267aab19000000)
[36m(pid=23074)[0m Warning: Your installation of OpenCV appears to be broken: module 'cv2.dnn' has no attribute 'DictValue'.Please follow the instructions at https://github.com/opencv/opencv-python/issues/884 to correct your environment. The import of cv2 has been skipped.
[36m(Llumlet pid=22647)[0m DEBUG 2025-07-16 12:30:12,277 executor.py:100] workers: []
[36m(Llumlet pid=22647)[0m DEBUG 2025-07-16 12:30:12,278 executor.py:101] driver_dummy_worker: Actor(RayWorkerWrapper, 889b113f7d226e76266fc2d919000000)
[36m(Llumlet pid=22648)[0m DEBUG 2025-07-16 12:30:12,283 executor.py:100] workers: []
[36m(Llumlet pid=22648)[0m DEBUG 2025-07-16 12:30:12,283 executor.py:101] driver_dummy_worker: Actor(RayWorkerWrapper, bdb79ce58dd4fbd8d0b562b919000000)
[36m(Llumlet pid=22645)[0m INFO 2025-07-16 12:30:12,450 ray_utils.py:299] MigrationWorker(actor_name=instance_57a22bb52ba144a392776f524f531ad4, actor_id=f0cf417aedafeeadbe86fd8319000000, placement_group_id=7b8972c346aecacb8ffb436d320319000000, namespace=llumnix, job_id=19000000, worker_id=fd392502712eb77974fa11ae7c0d19f9533319196a81245732b9747d, node_id=efde01d4aad6ea33c7e355bb10aa1c9161589aa44f8fcb02c9903b4f, gpu_ids=['0'])
[36m(Llumlet pid=22645)[0m INFO 2025-07-16 12:30:12,451 ray_utils.py:307] MigrationWorker(log_dir=['/tmp/ray/session_2025-07-16_12-15-34_687273_108543/logs/worker-fd392502712eb77974fa11ae7c0d19f9533319196a81245732b9747d-19000000-22645.err', '/tmp/ray/session_2025-07-16_12-15-34_687273_108543/logs/worker-fd392502712eb77974fa11ae7c0d19f9533319196a81245732b9747d-19000000-22645.out'])
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:12,643 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet pid=22647)[0m INFO 2025-07-16 12:30:12,653 ray_utils.py:299] MigrationWorker(actor_name=instance_68140c7aeecd46d69cad41d358d58f2d, actor_id=9676aa444b8cdea1083cc52419000000, placement_group_id=58c4f27e78ebe07f1d1248a7e68419000000, namespace=llumnix, job_id=19000000, worker_id=c48860e5737c2f1c19d3c999bd7c1a19ea8053071c8a450a13ea1ece, node_id=efde01d4aad6ea33c7e355bb10aa1c9161589aa44f8fcb02c9903b4f, gpu_ids=['2'])
[36m(Llumlet pid=22646)[0m DEBUG 2025-07-16 12:30:12,656 executor.py:100] workers: []
[36m(Llumlet pid=22646)[0m DEBUG 2025-07-16 12:30:12,656 executor.py:101] driver_dummy_worker: Actor(RayWorkerWrapper, fd2abd978d556787cf6da1d819000000)
[36m(Llumlet pid=22647)[0m INFO 2025-07-16 12:30:12,655 ray_utils.py:307] MigrationWorker(log_dir=['/tmp/ray/session_2025-07-16_12-15-34_687273_108543/logs/worker-c48860e5737c2f1c19d3c999bd7c1a19ea8053071c8a450a13ea1ece-19000000-22647.err', '/tmp/ray/session_2025-07-16_12-15-34_687273_108543/logs/worker-c48860e5737c2f1c19d3c999bd7c1a19ea8053071c8a450a13ea1ece-19000000-22647.out'])
[36m(Llumlet pid=22648)[0m INFO 2025-07-16 12:30:12,654 ray_utils.py:299] MigrationWorker(actor_name=instance_1ca1230e98474189bce13b5bfdcd056d, actor_id=26fa726a471ddc54a867eb0019000000, placement_group_id=4bb2a38f3f4252566e8f876a37d219000000, namespace=llumnix, job_id=19000000, worker_id=9b33fcad908583bd386d82d94e030303d6fb92381577b85010e1d006, node_id=efde01d4aad6ea33c7e355bb10aa1c9161589aa44f8fcb02c9903b4f, gpu_ids=['3'])
[36m(Llumlet pid=22648)[0m INFO 2025-07-16 12:30:12,655 ray_utils.py:307] MigrationWorker(log_dir=['/tmp/ray/session_2025-07-16_12-15-34_687273_108543/logs/worker-9b33fcad908583bd386d82d94e030303d6fb92381577b85010e1d006-19000000-22648.out', '/tmp/ray/session_2025-07-16_12-15-34_687273_108543/logs/worker-9b33fcad908583bd386d82d94e030303d6fb92381577b85010e1d006-19000000-22648.err'])
[36m(Llumlet pid=22645)[0m INFO 07-16 12:30:12 model_runner.py:1056] Starting to load model /mnt/model/Qwen2.5-7B...
[36m(Llumlet pid=22645)[0m Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[36m(Llumlet pid=22646)[0m INFO 2025-07-16 12:30:13,084 ray_utils.py:299] MigrationWorker(actor_name=instance_51147abf42184526b2c0f62977c34d74, actor_id=a17e4e9d645d71ac3ebe6fda19000000, placement_group_id=413a75b296bad9c19bcec309149719000000, namespace=llumnix, job_id=19000000, worker_id=0ed4f5f6f2423ed9c2d854e7dc1f88c63298ae290b18aa9052298f09, node_id=efde01d4aad6ea33c7e355bb10aa1c9161589aa44f8fcb02c9903b4f, gpu_ids=['1'])
[36m(Llumlet pid=22646)[0m INFO 2025-07-16 12:30:13,085 ray_utils.py:307] MigrationWorker(log_dir=['/tmp/ray/session_2025-07-16_12-15-34_687273_108543/logs/worker-0ed4f5f6f2423ed9c2d854e7dc1f88c63298ae290b18aa9052298f09-19000000-22646.out', '/tmp/ray/session_2025-07-16_12-15-34_687273_108543/logs/worker-0ed4f5f6f2423ed9c2d854e7dc1f88c63298ae290b18aa9052298f09-19000000-22646.err'])
[36m(Llumlet pid=22647)[0m INFO 07-16 12:30:13 model_runner.py:1056] Starting to load model /mnt/model/Qwen2.5-7B...
[36m(Llumlet pid=22648)[0m INFO 07-16 12:30:13 model_runner.py:1056] Starting to load model /mnt/model/Qwen2.5-7B...
[36m(Llumlet pid=22647)[0m Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[36m(Llumlet pid=22648)[0m Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[36m(Llumlet pid=22645)[0m Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.74it/s]
[36m(Llumlet pid=22646)[0m INFO 07-16 12:30:13 model_runner.py:1056] Starting to load model /mnt/model/Qwen2.5-7B...
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:13,670 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet pid=22646)[0m Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[36m(Llumlet pid=22647)[0m Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.36it/s]
[36m(Llumlet pid=22648)[0m Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.36it/s]
[36m(Llumlet pid=22645)[0m Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.70it/s]
[36m(Llumlet pid=22646)[0m Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.76it/s]
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:14,695 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet pid=22645)[0m Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.70it/s]
[36m(Llumlet pid=22647)[0m Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.39it/s]
[36m(Llumlet pid=22648)[0m Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.39it/s]
[36m(Llumlet pid=22646)[0m Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.65it/s]
[36m(Llumlet pid=22645)[0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.78it/s]
[36m(Llumlet pid=22645)[0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.75it/s]
[36m(Llumlet pid=22645)[0m 
[36m(Llumlet pid=22645)[0m INFO 07-16 12:30:15 model_runner.py:1067] Loading model weights took 14.2716 GB
[36m(Llumlet pid=22647)[0m Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.29it/s]
[36m(Llumlet pid=22646)[0m Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.58it/s]
[36m(Llumlet pid=22648)[0m Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.29it/s]
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:15,721 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet pid=22647)[0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.30it/s]
[36m(Llumlet pid=22647)[0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.32it/s]
[36m(Llumlet pid=22647)[0m 
[36m(Llumlet pid=22646)[0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.47it/s]
[36m(Llumlet pid=22646)[0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.53it/s]
[36m(Llumlet pid=22646)[0m 
[36m(Llumlet pid=22648)[0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.30it/s]
[36m(Llumlet pid=22648)[0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.32it/s]
[36m(Llumlet pid=22648)[0m 
[36m(Llumlet pid=22646)[0m INFO 07-16 12:30:16 model_runner.py:1067] Loading model weights took 14.2716 GB
[36m(Llumlet pid=22647)[0m INFO 07-16 12:30:16 model_runner.py:1067] Loading model weights took 14.2716 GB
[36m(Llumlet pid=22648)[0m INFO 07-16 12:30:16 model_runner.py:1067] Loading model weights took 14.2716 GB
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:16,745 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:17,768 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:18,793 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet pid=22645)[0m INFO 07-16 12:30:19 distributed_gpu_executor.py:57] # GPU blocks: 2170, # CPU blocks: 4681
[36m(Llumlet pid=22645)[0m INFO 07-16 12:30:19 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 8.48x
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:19,818 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet pid=22647)[0m INFO 07-16 12:30:20 distributed_gpu_executor.py:57] # GPU blocks: 2170, # CPU blocks: 4681
[36m(Llumlet pid=22647)[0m INFO 07-16 12:30:20 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 8.48x
[36m(Llumlet pid=22648)[0m INFO 07-16 12:30:20 distributed_gpu_executor.py:57] # GPU blocks: 2170, # CPU blocks: 4681
[36m(Llumlet pid=22648)[0m INFO 07-16 12:30:20 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 8.48x
[36m(Llumlet pid=22646)[0m INFO 07-16 12:30:20 distributed_gpu_executor.py:57] # GPU blocks: 2170, # CPU blocks: 4681
[36m(Llumlet pid=22646)[0m INFO 07-16 12:30:20 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 8.48x
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:20,845 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet pid=22645)[0m WARNING 07-16 12:30:21 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
[36m(Llumlet pid=22645)[0m WARNING 07-16 12:30:21 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:21,870 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet pid=22645)[0m INFO 2025-07-16 12:30:21,916 llm_engine.py:368] engine 57a22bb52ba144a392776f524f531ad4 current state: INIT
[36m(Llumlet pid=22645)[0m INFO 2025-07-16 12:30:21,916 llm_engine.py:388] engine 57a22bb52ba144a392776f524f531ad4 change state: INIT -> RUNNING
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:22,896 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet pid=22647)[0m WARNING 07-16 12:30:22 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
[36m(Llumlet pid=22647)[0m WARNING 07-16 12:30:22 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(Llumlet pid=22648)[0m WARNING 07-16 12:30:22 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
[36m(Llumlet pid=22648)[0m WARNING 07-16 12:30:22 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(Llumlet pid=22647)[0m INFO 2025-07-16 12:30:23,191 llm_engine.py:368] engine 68140c7aeecd46d69cad41d358d58f2d current state: INIT
[36m(Llumlet pid=22647)[0m INFO 2025-07-16 12:30:23,192 llm_engine.py:388] engine 68140c7aeecd46d69cad41d358d58f2d change state: INIT -> RUNNING
[36m(Llumlet pid=22646)[0m WARNING 07-16 12:30:23 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
[36m(Llumlet pid=22646)[0m WARNING 07-16 12:30:23 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(Llumlet pid=22648)[0m INFO 2025-07-16 12:30:23,245 llm_engine.py:368] engine 1ca1230e98474189bce13b5bfdcd056d current state: INIT
[36m(Llumlet pid=22648)[0m INFO 2025-07-16 12:30:23,246 llm_engine.py:388] engine 1ca1230e98474189bce13b5bfdcd056d change state: INIT -> RUNNING
[36m(Llumlet pid=22646)[0m INFO 2025-07-16 12:30:23,544 llm_engine.py:368] engine 51147abf42184526b2c0f62977c34d74 current state: INIT
[36m(Llumlet pid=22646)[0m INFO 2025-07-16 12:30:23,544 llm_engine.py:388] engine 51147abf42184526b2c0f62977c34d74 change state: INIT -> RUNNING
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:23,922 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(pid=23614)[0m WARNING 07-16 12:30:24 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:24,948 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(pid=23858)[0m WARNING 07-16 12:30:25 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.
[36m(pid=23821)[0m WARNING 07-16 12:30:25 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.
[36m(pid=23909)[0m WARNING 07-16 12:30:25 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:25,974 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(pid=23614)[0m Warning: Your installation of OpenCV appears to be broken: module 'cv2.dnn' has no attribute 'DictValue'.Please follow the instructions at https://github.com/opencv/opencv-python/issues/884 to correct your environment. The import of cv2 has been skipped.
[36m(APIServerActorVLLM pid=23614)[0m INFO 2025-07-16 12:30:26,550 ray_utils.py:299] APIServerActorVLLM(actor_name=server_57a22bb52ba144a392776f524f531ad4, actor_id=2486f65ca6a892ecba34710e19000000, placement_group_id=7b8972c346aecacb8ffb436d320319000000, namespace=llumnix, job_id=19000000, worker_id=8727c0e28129f4dd2644d7141867e2b0ac29b27f8d33f4f07e907b98, node_id=efde01d4aad6ea33c7e355bb10aa1c9161589aa44f8fcb02c9903b4f, gpu_ids=[])
[36m(APIServerActorVLLM pid=23614)[0m INFO 2025-07-16 12:30:26,551 ray_utils.py:307] APIServerActorVLLM(log_dir=['/tmp/ray/session_2025-07-16_12-15-34_687273_108543/logs/worker-8727c0e28129f4dd2644d7141867e2b0ac29b27f8d33f4f07e907b98-19000000-23614.err', '/tmp/ray/session_2025-07-16_12-15-34_687273_108543/logs/worker-8727c0e28129f4dd2644d7141867e2b0ac29b27f8d33f4f07e907b98-19000000-23614.out'])
[36m(Llumlet(iid=57a22) pid=22645)[0m INFO 07-16 12:30:26 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:27,000 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(APIServerActorVLLM pid=23614)[0m INFO 2025-07-16 12:30:26,978 zmq_server.py:69] QueueServer's socket bind to: tcp://172.23.75.208:11614
[36m(APIServerActorVLLM pid=23614)[0m INFO:     Started server process [23614]
[36m(APIServerActorVLLM pid=23614)[0m INFO:     Waiting for application startup.
[36m(APIServerActorVLLM pid=23614)[0m INFO:     Application startup complete.
[36m(APIServerActorVLLM pid=23614)[0m INFO:     Uvicorn running on http://172.23.75.208:31128 (Press CTRL+C to quit)
[36m(Scaler pid=22179)[0m INFO 2025-07-16 12:30:27,195 scaler.py:825] num_instances: 1, instances: ['57a22bb52ba144a392776f524f531ad4']
[36m(Scaler pid=22179)[0m INFO 2025-07-16 12:30:27,200 scaler.py:599] Init server and instance done, instance_id: 57a22bb52ba144a392776f524f531ad4, instance_type: prefill.
[36m(Manager pid=22370)[0m INFO 2025-07-16 12:30:27,199 global_scheduler.py:171] Scale up instance 57a22bb52ba144a392776f524f531ad4.
[36m(Manager pid=22370)[0m INFO 2025-07-16 12:30:27,199 global_scheduler.py:176] num_instances: 1, instances: {'57a22bb52ba144a392776f524f531ad4'}
[36m(APIServerActorVLLM pid=23614)[0m INFO:     172.23.75.208:42740 - "GET /health HTTP/1.1" 200 OK
[36m(pid=23858)[0m Warning: Your installation of OpenCV appears to be broken: module 'cv2.dnn' has no attribute 'DictValue'.Please follow the instructions at https://github.com/opencv/opencv-python/issues/884 to correct your environment. The import of cv2 has been skipped.
[36m(pid=23821)[0m Warning: Your installation of OpenCV appears to be broken: module 'cv2.dnn' has no attribute 'DictValue'.Please follow the instructions at https://github.com/opencv/opencv-python/issues/884 to correct your environment. The import of cv2 has been skipped.
[36m(pid=23909)[0m Warning: Your installation of OpenCV appears to be broken: module 'cv2.dnn' has no attribute 'DictValue'.Please follow the instructions at https://github.com/opencv/opencv-python/issues/884 to correct your environment. The import of cv2 has been skipped.
[36m(APIServerActorVLLM pid=23858)[0m INFO 2025-07-16 12:30:27,648 ray_utils.py:299] APIServerActorVLLM(actor_name=server_1ca1230e98474189bce13b5bfdcd056d, actor_id=879d33f7c80f292688f6280519000000, placement_group_id=4bb2a38f3f4252566e8f876a37d219000000, namespace=llumnix, job_id=19000000, worker_id=46530c0a94c3667aa27063887a2e049026f8c6b23135871567c768ea, node_id=efde01d4aad6ea33c7e355bb10aa1c9161589aa44f8fcb02c9903b4f, gpu_ids=[])
[36m(APIServerActorVLLM pid=23858)[0m INFO 2025-07-16 12:30:27,649 ray_utils.py:307] APIServerActorVLLM(log_dir=['/tmp/ray/session_2025-07-16_12-15-34_687273_108543/logs/worker-46530c0a94c3667aa27063887a2e049026f8c6b23135871567c768ea-19000000-23858.out', '/tmp/ray/session_2025-07-16_12-15-34_687273_108543/logs/worker-46530c0a94c3667aa27063887a2e049026f8c6b23135871567c768ea-19000000-23858.err'])
[36m(APIServerActorVLLM pid=23821)[0m INFO 2025-07-16 12:30:27,702 ray_utils.py:299] APIServerActorVLLM(actor_name=server_68140c7aeecd46d69cad41d358d58f2d, actor_id=6cff8281a0e9eaa58aebc62c19000000, placement_group_id=58c4f27e78ebe07f1d1248a7e68419000000, namespace=llumnix, job_id=19000000, worker_id=174b7683062a9dd0bd737f4449ba4650d26a6bfccdb424be79357f46, node_id=efde01d4aad6ea33c7e355bb10aa1c9161589aa44f8fcb02c9903b4f, gpu_ids=[])
[36m(APIServerActorVLLM pid=23821)[0m INFO 2025-07-16 12:30:27,703 ray_utils.py:307] APIServerActorVLLM(log_dir=['/tmp/ray/session_2025-07-16_12-15-34_687273_108543/logs/worker-174b7683062a9dd0bd737f4449ba4650d26a6bfccdb424be79357f46-19000000-23821.out', '/tmp/ray/session_2025-07-16_12-15-34_687273_108543/logs/worker-174b7683062a9dd0bd737f4449ba4650d26a6bfccdb424be79357f46-19000000-23821.err'])
[36m(Llumlet(iid=68140) pid=22647)[0m INFO 07-16 12:30:27 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:28,025 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet(iid=1ca12) pid=22648)[0m INFO 07-16 12:30:27 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
[36m(APIServerActorVLLM(iid=57a22) pid=23614)[0m INFO:     172.23.75.208:42746 - "GET /is_ready HTTP/1.1" 200 OK
[36m(APIServerActorVLLM pid=23909)[0m INFO 2025-07-16 12:30:27,980 ray_utils.py:299] APIServerActorVLLM(actor_name=server_51147abf42184526b2c0f62977c34d74, actor_id=ed365a001fb3b2a0df4533ab19000000, placement_group_id=413a75b296bad9c19bcec309149719000000, namespace=llumnix, job_id=19000000, worker_id=e9601e65031e565a31eab98d64692d381c11485cad1171d7c83255d7, node_id=efde01d4aad6ea33c7e355bb10aa1c9161589aa44f8fcb02c9903b4f, gpu_ids=[])
[36m(APIServerActorVLLM pid=23909)[0m INFO 2025-07-16 12:30:27,981 ray_utils.py:307] APIServerActorVLLM(log_dir=['/tmp/ray/session_2025-07-16_12-15-34_687273_108543/logs/worker-e9601e65031e565a31eab98d64692d381c11485cad1171d7c83255d7-19000000-23909.err', '/tmp/ray/session_2025-07-16_12-15-34_687273_108543/logs/worker-e9601e65031e565a31eab98d64692d381c11485cad1171d7c83255d7-19000000-23909.out'])
[36m(Llumlet(iid=51147) pid=22646)[0m INFO 07-16 12:30:28 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
[36m(APIServerActorVLLM pid=23858)[0m INFO 2025-07-16 12:30:28,346 zmq_server.py:69] QueueServer's socket bind to: tcp://172.23.75.208:11858
[36m(APIServerActorVLLM pid=23909)[0m INFO 2025-07-16 12:30:28,331 zmq_server.py:69] QueueServer's socket bind to: tcp://172.23.75.208:11909
[36m(APIServerActorVLLM pid=23821)[0m INFO 2025-07-16 12:30:28,432 zmq_server.py:69] QueueServer's socket bind to: tcp://172.23.75.208:11821
[36m(APIServerActorVLLM pid=23909)[0m INFO:     Started server process [23909]
[36m(APIServerActorVLLM pid=23909)[0m INFO:     Waiting for application startup.
[36m(APIServerActorVLLM pid=23909)[0m INFO:     Application startup complete.
[36m(APIServerActorVLLM pid=23909)[0m INFO:     Uvicorn running on http://172.23.75.208:31129 (Press CTRL+C to quit)
[36m(Scaler pid=22179)[0m INFO 2025-07-16 12:30:28,544 scaler.py:825] num_instances: 2, instances: ['57a22bb52ba144a392776f524f531ad4', '51147abf42184526b2c0f62977c34d74']
[36m(Scaler pid=22179)[0m INFO 2025-07-16 12:30:28,548 scaler.py:599] Init server and instance done, instance_id: 51147abf42184526b2c0f62977c34d74, instance_type: decode.
[36m(Scaler pid=22179)[0m INFO 2025-07-16 12:30:28,560 scaler.py:825] num_instances: 3, instances: ['57a22bb52ba144a392776f524f531ad4', '51147abf42184526b2c0f62977c34d74', '1ca1230e98474189bce13b5bfdcd056d']
[36m(Scaler pid=22179)[0m INFO 2025-07-16 12:30:28,563 scaler.py:599] Init server and instance done, instance_id: 1ca1230e98474189bce13b5bfdcd056d, instance_type: decode.
[36m(Manager pid=22370)[0m INFO 2025-07-16 12:30:28,548 global_scheduler.py:171] Scale up instance 51147abf42184526b2c0f62977c34d74.
[36m(Manager pid=22370)[0m INFO 2025-07-16 12:30:28,548 global_scheduler.py:176] num_instances: 2, instances: {'57a22bb52ba144a392776f524f531ad4', '51147abf42184526b2c0f62977c34d74'}
[36m(Manager pid=22370)[0m INFO 2025-07-16 12:30:28,562 global_scheduler.py:171] Scale up instance 1ca1230e98474189bce13b5bfdcd056d.
[36m(Manager pid=22370)[0m INFO 2025-07-16 12:30:28,562 global_scheduler.py:176] num_instances: 3, instances: {'57a22bb52ba144a392776f524f531ad4', '51147abf42184526b2c0f62977c34d74', '1ca1230e98474189bce13b5bfdcd056d'}
[36m(APIServerActorVLLM pid=23858)[0m INFO:     172.23.75.208:33092 - "GET /health HTTP/1.1" 200 OK
[36m(APIServerActorVLLM pid=23821)[0m INFO:     Started server process [23821]
[36m(APIServerActorVLLM pid=23821)[0m INFO:     Waiting for application startup.
[36m(APIServerActorVLLM pid=23821)[0m INFO:     Application startup complete.
[36m(APIServerActorVLLM pid=23821)[0m INFO:     Uvicorn running on http://172.23.75.208:31130 (Press CTRL+C to quit)
[36m(APIServerActorVLLM pid=23858)[0m INFO:     Started server process [23858]
[36m(APIServerActorVLLM pid=23858)[0m INFO:     Waiting for application startup.
[36m(APIServerActorVLLM pid=23858)[0m INFO:     Application startup complete.
[36m(APIServerActorVLLM pid=23858)[0m INFO:     Uvicorn running on http://172.23.75.208:31131 (Press CTRL+C to quit)
[36m(APIServerActorVLLM pid=23909)[0m INFO:     172.23.75.208:33998 - "GET /health HTTP/1.1" 200 OK
[36m(Scaler pid=22179)[0m INFO 2025-07-16 12:30:28,647 scaler.py:825] num_instances: 4, instances: ['57a22bb52ba144a392776f524f531ad4', '51147abf42184526b2c0f62977c34d74', '1ca1230e98474189bce13b5bfdcd056d', '68140c7aeecd46d69cad41d358d58f2d']
[36m(Scaler pid=22179)[0m INFO 2025-07-16 12:30:28,651 scaler.py:599] Init server and instance done, instance_id: 68140c7aeecd46d69cad41d358d58f2d, instance_type: prefill.
[36m(Manager pid=22370)[0m INFO 2025-07-16 12:30:28,650 global_scheduler.py:171] Scale up instance 68140c7aeecd46d69cad41d358d58f2d.
[36m(Manager pid=22370)[0m INFO 2025-07-16 12:30:28,650 global_scheduler.py:176] num_instances: 4, instances: {'68140c7aeecd46d69cad41d358d58f2d', '57a22bb52ba144a392776f524f531ad4', '51147abf42184526b2c0f62977c34d74', '1ca1230e98474189bce13b5bfdcd056d'}
[36m(APIServerActorVLLM pid=23821)[0m INFO:     172.23.75.208:36266 - "GET /health HTTP/1.1" 200 OK
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:29,048 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet(iid=57a22) pid=22645)[0m INFO 2025-07-16 12:30:29,267 migration_backend.py:325] Migration backend init_backend success (group_name: 8a6fd5a37f6b4f26bc2c161e8a99a05a, world_size: 1, rank: 0, backbend: gloo)
[36m(Llumlet(iid=57a22) pid=22645)[0m INFO 2025-07-16 12:30:29,277 migration_backend.py:325] Migration backend destory_backend success (group_name: 8a6fd5a37f6b4f26bc2c161e8a99a05a, world_size: 1, rank: 0, backbend: gloo)
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:30,074 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet(iid=57a22) pid=22645)[0m INFO 2025-07-16 12:30:30,402 migration_backend.py:325] Migration backend init_backend success (group_name: 1586e062165343b4a62eddc99b20425d, world_size: 4, rank: 2, backbend: gloo)
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:31,100 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=22370)[0m INFO 2025-07-16 12:30:31,326 manager.py:491] Rebuild migration backend done, group_name: 1586e062165343b4a62eddc99b20425d, alive instance (4): ['1ca1230e98474189bce13b5bfdcd056d', '51147abf42184526b2c0f62977c34d74', '57a22bb52ba144a392776f524f531ad4', '68140c7aeecd46d69cad41d358d58f2d'].
[36m(Llumlet(iid=57a22) pid=22645)[0m INFO 2025-07-16 12:30:31,325 migration_backend.py:325] Migration backend warmup success (group_name: 1586e062165343b4a62eddc99b20425d, world_size: 4, rank: 2, backbend: gloo)
[36m(Llumlet(iid=51147) pid=22646)[0m INFO 2025-07-16 12:30:31,312 migration_backend.py:325] Migration backend init_backend success (group_name: 1586e062165343b4a62eddc99b20425d, world_size: 4, rank: 1, backbend: gloo)
[36m(Llumlet(iid=51147) pid=22646)[0m INFO 2025-07-16 12:30:31,325 migration_backend.py:325] Migration backend warmup success (group_name: 1586e062165343b4a62eddc99b20425d, world_size: 4, rank: 1, backbend: gloo)
[36m(Llumlet(iid=68140) pid=22647)[0m INFO 2025-07-16 12:30:31,312 migration_backend.py:325] Migration backend init_backend success (group_name: 1586e062165343b4a62eddc99b20425d, world_size: 4, rank: 3, backbend: gloo)
[36m(Llumlet(iid=68140) pid=22647)[0m INFO 2025-07-16 12:30:31,325 migration_backend.py:325] Migration backend warmup success (group_name: 1586e062165343b4a62eddc99b20425d, world_size: 4, rank: 3, backbend: gloo)
[36m(Llumlet(iid=1ca12) pid=22648)[0m INFO 2025-07-16 12:30:31,321 migration_backend.py:325] Migration backend init_backend success (group_name: 1586e062165343b4a62eddc99b20425d, world_size: 4, rank: 0, backbend: gloo)
[36m(Llumlet(iid=1ca12) pid=22648)[0m INFO 2025-07-16 12:30:31,325 migration_backend.py:325] Migration backend warmup success (group_name: 1586e062165343b4a62eddc99b20425d, world_size: 4, rank: 0, backbend: gloo)
[36m(Llumlet(iid=57a22) pid=22645)[0m INFO 07-16 12:30:31 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:32,128 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet(iid=68140) pid=22647)[0m INFO 07-16 12:30:32 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
[36m(Llumlet(iid=1ca12) pid=22648)[0m INFO 07-16 12:30:33 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
[36m(APIServerActorVLLM(iid=57a22) pid=23614)[0m INFO:     172.23.75.208:42748 - "GET /is_ready HTTP/1.1" 200 OK
[36m(APIServerActorVLLM(iid=1ca12) pid=23858)[0m INFO:     172.23.75.208:33108 - "GET /is_ready HTTP/1.1" 200 OK
[36m(APIServerActorVLLM(iid=68140) pid=23821)[0m INFO:     172.23.75.208:36270 - "GET /is_ready HTTP/1.1" 200 OK
[36m(APIServerActorVLLM(iid=51147) pid=23909)[0m INFO:     172.23.75.208:34006 - "GET /is_ready HTTP/1.1" 200 OK
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:33,154 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet(iid=51147) pid=22646)[0m INFO 07-16 12:30:33 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:34,179 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:35,206 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=22370)[0m INFO 2025-07-16 12:30:36,022 dispatch_policy.py:138] dispatch request to 57a22bb52ba144a392776f524f531ad4, load: KvBlocksRatioLoad(demand_factor=0.0,is_busy=False)
[36m(Manager pid=22370)[0m INFO 2025-07-16 12:30:36,023 global_scheduler.py:101] dispath request 67d92d4f71694d53a6a82d35526fc3b1 to prefill instance (57a22bb52ba144a392776f524f531ad4), expected_steps: 1.
[36m(APIServerActorVLLM(iid=57a22) pid=23614)[0m INFO 2025-07-16 12:30:36,019 client.py:50] Client received request 67d92d4f71694d53a6a82d35526fc3b1
[36m(Llumlet(iid=57a22) pid=22645)[0m INFO 2025-07-16 12:30:36,098 migration_coordinator.py:288] 57a22bb52ba144a392776f524f531ad4->1ca1230e98474189bce13b5bfdcd056d begin migrate out
[36m(Llumlet(iid=57a22) pid=22645)[0m DEBUG 2025-07-16 12:30:36,098 migration_coordinator.py:252] Max migration concurrency (1) reached, reject new migrate out request attempt.
[36m(Llumlet(iid=57a22) pid=22645)[0m INFO 2025-07-16 12:30:36,123 scheduler.py:200] free request 67d92d4f71694d53a6a82d35526fc3b1 (seq 0)
[36m(Llumlet(iid=57a22) pid=22645)[0m INFO 2025-07-16 12:30:36,123 migration_coordinator.py:318] Instance 57a22bb52ba144a392776f524f531ad4->1ca1230e98474189bce13b5bfdcd056d migrate done, migration type: PD_MIGRATION, migrate request ['67d92d4f71694d53a6a82d35526fc3b1'], migration status: MigrationStatus.FINISHED, len: 1 blocks, cost: 25.541067123413086 ms
[36m(Llumlet(iid=1ca12) pid=22648)[0m INFO 2025-07-16 12:30:36,118 worker.py:145] Recv kv cache done, num_blocks: 1, total_kv_cache_size: 896.00 KB, time: 0.01s, speed: 0.06839GB/s.
[36m(Llumlet(iid=1ca12) pid=22648)[0m INFO 2025-07-16 12:30:36,122 llm_engine.py:432] pop request 67d92d4f71694d53a6a82d35526fc3b1 from pre-allocated cache
[36m(Llumlet(iid=1ca12) pid=22648)[0m INFO 2025-07-16 12:30:36,122 llm_engine.py:434] add seq 0 to block table
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:36,232 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=22370)[0m INFO 2025-07-16 12:30:36,671 dispatch_policy.py:138] dispatch request to 57a22bb52ba144a392776f524f531ad4, load: KvBlocksRatioLoad(demand_factor=0.0,is_busy=False)
[36m(Manager pid=22370)[0m INFO 2025-07-16 12:30:36,671 global_scheduler.py:101] dispath request 0d4c9f63137645cdae5b04971f824924 to prefill instance (57a22bb52ba144a392776f524f531ad4), expected_steps: 1.
[36m(Llumlet(iid=57a22) pid=22645)[0m INFO 07-16 12:30:36 metrics.py:349] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 0.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
[36m(Llumlet(iid=57a22) pid=22645)[0m INFO 2025-07-16 12:30:36,722 migration_coordinator.py:288] 57a22bb52ba144a392776f524f531ad4->1ca1230e98474189bce13b5bfdcd056d begin migrate out
[36m(Llumlet(iid=57a22) pid=22645)[0m DEBUG 2025-07-16 12:30:36,723 migration_coordinator.py:252] Max migration concurrency (1) reached, reject new migrate out request attempt.
[36m(Llumlet(iid=57a22) pid=22645)[0m INFO 2025-07-16 12:30:36,737 scheduler.py:200] free request 0d4c9f63137645cdae5b04971f824924 (seq 1)
[36m(Llumlet(iid=57a22) pid=22645)[0m INFO 2025-07-16 12:30:36,738 migration_coordinator.py:318] Instance 57a22bb52ba144a392776f524f531ad4->1ca1230e98474189bce13b5bfdcd056d migrate done, migration type: PD_MIGRATION, migrate request ['0d4c9f63137645cdae5b04971f824924'], migration status: MigrationStatus.FINISHED, len: 1 blocks, cost: 15.302181243896484 ms
[36m(Llumlet(iid=1ca12) pid=22648)[0m INFO 2025-07-16 12:30:36,666 llm_engine.py:216] Engine finished request 67d92d4f71694d53a6a82d35526fc3b1
[36m(Llumlet(iid=1ca12) pid=22648)[0m INFO 2025-07-16 12:30:36,735 worker.py:145] Recv kv cache done, num_blocks: 1, total_kv_cache_size: 896.00 KB, time: 0.01s, speed: 0.10750GB/s.
[36m(Llumlet(iid=1ca12) pid=22648)[0m INFO 2025-07-16 12:30:36,737 llm_engine.py:432] pop request 0d4c9f63137645cdae5b04971f824924 from pre-allocated cache
[36m(Llumlet(iid=1ca12) pid=22648)[0m INFO 2025-07-16 12:30:36,737 llm_engine.py:434] add seq 1 to block table
[36m(APIServerActorVLLM(iid=57a22) pid=23614)[0m INFO 2025-07-16 12:30:36,667 client.py:165] Client finished request 67d92d4f71694d53a6a82d35526fc3b1.
[36m(APIServerActorVLLM(iid=57a22) pid=23614)[0m INFO:     172.23.75.208:42756 - "POST /generate HTTP/1.1" 200 OK
[36m(APIServerActorVLLM(iid=57a22) pid=23614)[0m INFO 2025-07-16 12:30:36,669 client.py:50] Client received request 0d4c9f63137645cdae5b04971f824924
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:37,257 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=22370)[0m INFO 2025-07-16 12:30:37,257 dispatch_policy.py:138] dispatch request to 57a22bb52ba144a392776f524f531ad4, load: KvBlocksRatioLoad(demand_factor=0.0,is_busy=False)
[36m(Manager pid=22370)[0m INFO 2025-07-16 12:30:37,257 global_scheduler.py:101] dispath request a5b5601888624b23aaf54c2a99857c27 to prefill instance (57a22bb52ba144a392776f524f531ad4), expected_steps: 1.
[36m(Llumlet(iid=1ca12) pid=22648)[0m INFO 2025-07-16 12:30:37,253 llm_engine.py:216] Engine finished request 0d4c9f63137645cdae5b04971f824924
[36m(APIServerActorVLLM(iid=57a22) pid=23614)[0m INFO 2025-07-16 12:30:37,253 client.py:165] Client finished request 0d4c9f63137645cdae5b04971f824924.
[36m(APIServerActorVLLM(iid=57a22) pid=23614)[0m INFO:     172.23.75.208:34930 - "POST /generate HTTP/1.1" 200 OK
[36m(APIServerActorVLLM(iid=57a22) pid=23614)[0m INFO 2025-07-16 12:30:37,255 client.py:50] Client received request a5b5601888624b23aaf54c2a99857c27
[36m(Llumlet(iid=57a22) pid=22645)[0m INFO 2025-07-16 12:30:37,347 migration_coordinator.py:288] 57a22bb52ba144a392776f524f531ad4->1ca1230e98474189bce13b5bfdcd056d begin migrate out
[36m(Llumlet(iid=57a22) pid=22645)[0m DEBUG 2025-07-16 12:30:37,347 migration_coordinator.py:252] Max migration concurrency (1) reached, reject new migrate out request attempt.
[36m(Llumlet(iid=57a22) pid=22645)[0m INFO 2025-07-16 12:30:37,366 scheduler.py:200] free request a5b5601888624b23aaf54c2a99857c27 (seq 2)
[36m(Llumlet(iid=57a22) pid=22645)[0m INFO 2025-07-16 12:30:37,366 migration_coordinator.py:318] Instance 57a22bb52ba144a392776f524f531ad4->1ca1230e98474189bce13b5bfdcd056d migrate done, migration type: PD_MIGRATION, migrate request ['a5b5601888624b23aaf54c2a99857c27'], migration status: MigrationStatus.FINISHED, len: 1 blocks, cost: 19.265174865722656 ms
[36m(Llumlet(iid=1ca12) pid=22648)[0m INFO 2025-07-16 12:30:37,363 worker.py:145] Recv kv cache done, num_blocks: 1, total_kv_cache_size: 896.00 KB, time: 0.01s, speed: 0.11227GB/s.
[36m(Llumlet(iid=1ca12) pid=22648)[0m INFO 2025-07-16 12:30:37,365 llm_engine.py:432] pop request a5b5601888624b23aaf54c2a99857c27 from pre-allocated cache
[36m(Llumlet(iid=1ca12) pid=22648)[0m INFO 2025-07-16 12:30:37,365 llm_engine.py:434] add seq 2 to block table
[36m(Manager pid=22370)[0m INFO 2025-07-16 12:30:37,883 dispatch_policy.py:138] dispatch request to 57a22bb52ba144a392776f524f531ad4, load: KvBlocksRatioLoad(demand_factor=0.0,is_busy=False)
[36m(Manager pid=22370)[0m INFO 2025-07-16 12:30:37,883 global_scheduler.py:101] dispath request 9f3d7e9451cb497894f4df9fc2a87135 to prefill instance (57a22bb52ba144a392776f524f531ad4), expected_steps: 1.
[36m(Llumlet(iid=1ca12) pid=22648)[0m INFO 2025-07-16 12:30:37,879 llm_engine.py:216] Engine finished request a5b5601888624b23aaf54c2a99857c27
[36m(APIServerActorVLLM(iid=57a22) pid=23614)[0m INFO 2025-07-16 12:30:37,880 client.py:165] Client finished request a5b5601888624b23aaf54c2a99857c27.
[36m(APIServerActorVLLM(iid=57a22) pid=23614)[0m INFO:     172.23.75.208:34936 - "POST /generate HTTP/1.1" 200 OK
[36m(APIServerActorVLLM(iid=57a22) pid=23614)[0m INFO 2025-07-16 12:30:37,882 client.py:50] Client received request 9f3d7e9451cb497894f4df9fc2a87135
[36m(Llumlet(iid=57a22) pid=22645)[0m INFO 2025-07-16 12:30:37,971 migration_coordinator.py:288] 57a22bb52ba144a392776f524f531ad4->1ca1230e98474189bce13b5bfdcd056d begin migrate out
[36m(Llumlet(iid=57a22) pid=22645)[0m DEBUG 2025-07-16 12:30:37,971 migration_coordinator.py:252] Max migration concurrency (1) reached, reject new migrate out request attempt.
[36m(Llumlet(iid=57a22) pid=22645)[0m INFO 2025-07-16 12:30:37,985 scheduler.py:200] free request 9f3d7e9451cb497894f4df9fc2a87135 (seq 3)
[36m(Llumlet(iid=57a22) pid=22645)[0m INFO 2025-07-16 12:30:37,985 migration_coordinator.py:318] Instance 57a22bb52ba144a392776f524f531ad4->1ca1230e98474189bce13b5bfdcd056d migrate done, migration type: PD_MIGRATION, migrate request ['9f3d7e9451cb497894f4df9fc2a87135'], migration status: MigrationStatus.FINISHED, len: 1 blocks, cost: 14.017105102539062 ms
[36m(Llumlet(iid=68140) pid=22647)[0m INFO 07-16 12:30:37 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
[36m(Llumlet(iid=1ca12) pid=22648)[0m INFO 2025-07-16 12:30:37,983 worker.py:145] Recv kv cache done, num_blocks: 1, total_kv_cache_size: 896.00 KB, time: 0.01s, speed: 0.11426GB/s.
[36m(Llumlet(iid=1ca12) pid=22648)[0m INFO 2025-07-16 12:30:37,984 llm_engine.py:432] pop request 9f3d7e9451cb497894f4df9fc2a87135 from pre-allocated cache
[36m(Llumlet(iid=1ca12) pid=22648)[0m INFO 2025-07-16 12:30:37,984 llm_engine.py:434] add seq 3 to block table
[36m(Llumlet(iid=1ca12) pid=22648)[0m INFO 07-16 12:30:38 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:38,283 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet(iid=51147) pid=22646)[0m INFO 07-16 12:30:38 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
[36m(Llumlet(iid=1ca12) pid=22648)[0m INFO 2025-07-16 12:30:38,506 llm_engine.py:216] Engine finished request 9f3d7e9451cb497894f4df9fc2a87135
[36m(APIServerActorVLLM(iid=57a22) pid=23614)[0m INFO 2025-07-16 12:30:38,506 client.py:165] Client finished request 9f3d7e9451cb497894f4df9fc2a87135.
[36m(APIServerActorVLLM(iid=57a22) pid=23614)[0m INFO:     172.23.75.208:34938 - "POST /generate HTTP/1.1" 200 OK
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:39,309 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:40,335 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=22179)[0m DEBUG 2025-07-16 12:30:41,361 scaler.py:275] The number of alive placement groups has reached the max_instances.
*** SIGTERM received at time=1752669041 on cpu 12 ***
PC: @     0x7fe3bf1a082d  (unknown)  select
    @     0x7fe3bf0c7520  (unknown)  (unknown)
[2025-07-16 12:30:41,565 E 21841 21841] logging.cc:496: *** SIGTERM received at time=1752669041 on cpu 12 ***
[2025-07-16 12:30:41,565 E 21841 21841] logging.cc:496: PC: @     0x7fe3bf1a082d  (unknown)  select
[2025-07-16 12:30:41,566 E 21841 21841] logging.cc:496:     @     0x7fe3bf0c7520  (unknown)  (unknown)
[36m(Llumlet(iid=57a22) pid=22645)[0m INFO 07-16 12:30:41 metrics.py:349] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 0.4 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
