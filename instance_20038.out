2025-07-16 11:50:57,839	INFO worker.py:1723 -- Connecting to existing Ray cluster at address: 172.18.0.3:6379...
2025-07-16 11:50:57,849	INFO worker.py:1908 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://127.0.0.1:8265 [39m[22m
INFO 07-16 11:50:59.553 [__init__.py:244] Automatically detected platform cuda.
INFO 07-16 11:51:02.865 [config.py:2145] Data parallel size: 1, local size: 1, rank: 0, local rank: None, master ip: 127.0.0.1, master port: 0
INFO 07-16 11:51:09.550 [config.py:934] This model supports multiple tasks: {'classify', 'reward', 'embed', 'generate'}. Defaulting to 'generate'.
INFO 07-16 11:51:09.550 [config.py:1578] Using max model len 4096
WARNING 07-16 11:51:09.551 [config.py:807] `disable_mm_preprocessor_cache` is only supported for multimodal models.
INFO 07-16 11:51:09.551 [arg_utils.py:1414] CK/AITER is disabled for FP8/MoE since we are not on ROCm.
INFO 07-16 11:51:09.551 [config.py:2145] Data parallel size: 1, local size: 1, rank: 0, local rank: None, master ip: 127.0.0.1, master port: 0
INFO 07-16 11:51:09.552 [config.py:2510] Chunked prefill is enabled with max_num_batched_tokens=16000.
INFO 2025-07-16 11:51:09,594 arg_utils.py:182] entrypoints_args: VLLMV1EntrypointsArgs(host='172.18.0.3', port=20038, ssl_keyfile=None, ssl_certfile=None, server_log_level='info', launch_ray_cluster=False, ray_cluster_port=6379, disable_log_to_driver=False, request_output_queue_type='zmq', disable_log_requests_server=False, log_request_timestamps=None, config_file=None, disable_keep_serve_process_alive=False, ssl_ca_certs=None, ssl_cert_reqs=0, allowed_origins=['*'], allow_credentials=None, allowed_methods=['*'], allowed_headers=['*'], tool_call_parser=None, tool_parser_plugin='', log_config_file=None, root_path=None, disable_fastapi_docs=False, api_key=None, enable_request_id_headers=False, middleware=[], enable_ssl_refresh=False, uvicorn_log_level='info', disable_uvicorn_access_log=None, disable_frontend_multiprocessing=None, max_log_len=None, chat_template=None, lora_modules=None, prompt_adapters=None, enable_auto_tool_choice=False, enable_prompt_tokens_details=False, return_tokens_as_token_ids=False, response_role='assistant', chat_template_content_format='auto', enable_server_load_tracking=False, enable_force_include_usage=False, expand_tools_even_if_tool_choice_none=False, client_index=0)
INFO 2025-07-16 11:51:09,594 arg_utils.py:183] manager_args: ManagerArgs(initial_instances=1, polling_interval=0.05, dispatch_policy='load', scaling_load_metric='remaining_steps', topk_random_dispatch=1, enable_migration=False, pair_migration_frequency=1, pair_migration_policy='defrag', migrate_out_threshold=-3.0, enable_scaling=False, min_instances=-1, max_instances=4, scaling_interval=10, scaling_policy='avg_load', scale_up_threshold=-10, scale_down_threshold=-60, log_instance_info=False, log_filename='server.log', enable_port_increment=True, enable_port_offset_store=False, enable_pd_disagg=False, enable_adaptive_pd=False, pd_ratio=[1, 1], load_registered_service=False, load_registered_service_path=None, enable_pdd_node_affinity_scheduling=False, is_group_kind_migration_backend=True, enable_engine_pd_disagg=False, enable_engine_semi_pd_disagg=None)
INFO 2025-07-16 11:51:09,594 arg_utils.py:184] instance_args: InstanceArgs(instance_type='no_constraints', simulator_mode=False, profiling_result_file_path=None, dispatch_load_metric='remaining_steps', dispatch_prefill_load_metric='kv_blocks_ratio', dispatch_prefill_as_decode_load_metric='adaptive_decode', dispatch_decode_load_metric='remaining_steps', dispatch_decode_as_prefill_load_metric='kv_blocks_ratio', enable_defrag=False, max_migration_concurrency=1, request_migration_policy='SR', migration_load_metric='remaining_steps', migration_backend='gloo', migration_buffer_blocks=32, migration_num_layers=1, migration_backend_init_timeout=10.0, kvtransfer_migration_backend_transfer_type='rdma', kvtransfer_migration_backend_naming_url='file:/tmp/llumnix/naming/', migration_last_stage_max_blocks=16, migration_max_stages=3, engine_disagg_inst_id_env_var=None, request_output_forwarding_mode='thread', enable_engine_pd_disagg=False, enable_engine_semi_pd_disagg=None, enable_migration=False, enable_adaptive_pd=False)
INFO 2025-07-16 11:51:09,595 arg_utils.py:185] engine_args: AsyncEngineArgs(model='/mnt/model/Qwen2.5-7B', served_model_name=None, tokenizer='/mnt/model/Qwen2.5-7B', hf_config_path=None, task='auto', skip_tokenizer_init=False, enable_prompt_embeds=False, tokenizer_mode='auto', processor='/mnt/model/Qwen2.5-7B', trust_remote_code=True, allowed_local_media_path='', download_dir=None, load_format='auto', config_format='auto', dtype='auto', kv_cache_dtype='auto', seed=None, max_model_len=4096, cuda_graph_sizes=[512], distributed_executor_backend='ray', pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, data_parallel_rank=None, expert_parallel_size=None, enable_expert_parallel=False, data_parallel_size_local=None, data_parallel_address='127.0.0.1', data_parallel_rpc_port=29550, data_parallel_backend='mp', enable_eplb=False, num_redundant_experts=0, eplb_window_size=1000, eplb_step_interval=3000, eplb_log_balancedness=False, max_parallel_loading_workers=None, block_size=256, enable_prefix_caching=True, enable_chunked_prefix_caching=None, prefix_caching_hash_algo='builtin', block_allocator='CpuGpuBlockAllocator', enable_hidden_state_caching=False, disable_sliding_window=False, disable_cascade_attn=False, use_v2_block_manager=True, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, max_num_batched_tokens=16000, max_num_initial_batched_tokens=None, max_num_subsequent_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_mm_batched_tokens=None, max_num_seqs=None, max_num_prefill_seqs=1, max_logprobs=20, disable_log_stats=False, revision=None, code_revision=None, rope_scaling={}, rope_theta=None, dual_chunk_attention_config=None, hf_token=None, hf_overrides={}, tokenizer_revision=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, disable_message_queue_broadcaster=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, media_io_kwargs={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=True, enable_lora=None, enable_lora_bias=False, max_loras=1, max_lora_rank=16, fully_sharded_loras=False, max_cpu_loras=None, lora_dtype='auto', lora_extra_vocab_size=256, long_lora_scaling_factors=None, enable_prompt_adapter=None, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, ray_workers_use_nsight=False, num_gpu_blocks_override=None, num_cpu_blocks_override=None, num_lookahead_slots=0, model_loader_extra_config={}, ignore_patterns=None, preemption_mode=None, scheduler_delay_factor=0.0, enable_chunked_prefill=True, disable_chunked_mm_input=False, disable_hybrid_kv_cache_manager=False, guided_decoding_backend='auto', guided_decoding_disable_fallback=False, guided_decoding_disable_any_whitespace=False, guided_decoding_disable_additional_properties=False, logits_processor_pattern=None, speculative_config={}, speculative_model=None, speculative_method=None, speculative_model_quantization=None, speculative_draft_tensor_parallel_size=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_max_model_len=None, speculative_disable_by_batch_size=None, speculative_hf_overrides={}, speculative_kv_cache_dtype=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', heartbeat_interval=0.15, batch_timeout=2700, scheduler_cls='vllm.v1.core.sched.scheduler.Scheduler', enable_scheduler_prefetch=False, enable_moe_expert_stats=False, enable_moe_comm_ops_stats=False, enable_expert_stats_logging=False, moe_expert_stats_logging_interval=1, moe_expert_stats_window_size=1024, moe_expert_rebalance_interval=-1, moe_expert_rebalance_count_per_step=-1, override_neuron_config={}, override_pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["+rms_norm"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}, worker_cls='auto', worker_extension_cls='', kv_transfer_config=None, kv_events_config=None, v6d_socket=None, generation_config='auto', enable_sleep_mode=False, override_generation_config={}, model_impl='auto', override_attention_dtype=None, calculate_kv_scales=False, additional_config={}, enable_reasoning=None, reasoning_parser='', use_tqdm_on_load=True, pt_load_map_location='cpu', enable_multimodal_encoder_data_parallel=False, profile=False, profile_torch=False, profile_step_start=16, profile_step_end=32, profile_prefill_and_decode=False, disable_log_requests=False)
INFO 2025-07-16 11:51:09,656 setup.py:129] Init Scaler on current node.
[36m(Scaler pid=286112)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[36m(Scaler pid=286112)[0m INFO 2025-07-16 11:51:12,175 scaler.py:116] Init Manager actor.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:14,624 scaler.py:160] First time set scaler.client_index to 0.
[36m(Scaler pid=286112)[0m INFO 2025-07-16 11:51:14,624 ray_utils.py:277] Put data scaler.client_index to ray internal key-value storage, value: 0.
[36m(Scaler pid=286112)[0m INFO 2025-07-16 11:51:14,625 scaler.py:825] num_instances: 0, instances: []
[36m(Scaler pid=286112)[0m INFO 2025-07-16 11:51:14,627 scaler.py:247] Auto scale up loop starts, service name: no_constraints
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:14,629 ray_utils.py:121] placement_group_specs: [{'CPU': 3, 'GPU': 1}]
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:51:14,617 ray_utils.py:299] Manager(actor_name=manager, actor_id=84c6d31faac2ff7aa539ca8902000000, placement_group_id=None, namespace=llumnix, job_id=02000000, worker_id=11ee262f70637243716ce655f22448341dd0cb71bcb61d403f3caa96, node_id=bbb26e1cd3886c3e1cefaf42390b7ad5038fb045b136b20edd28e191, gpu_ids=[])
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:51:14,617 ray_utils.py:307] Manager(log_dir=['/tmp/ray/session_2025-07-16_11-50-40_561788_277830/logs/worker-11ee262f70637243716ce655f22448341dd0cb71bcb61d403f3caa96-02000000-286358.out', '/tmp/ray/session_2025-07-16_11-50-40_561788_277830/logs/worker-11ee262f70637243716ce655f22448341dd0cb71bcb61d403f3caa96-02000000-286358.err'])
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:51:14,620 manager.py:90] Launch mode: LaunchMode.GLOBAL, backend type: BackendType.VLLM_V1
[36m(Manager pid=286358)[0m DEBUG 2025-07-16 11:51:14,670 manager.py:241] Polling instance infos of 0 instances starts.
[36m(Manager pid=286358)[0m DEBUG 2025-07-16 11:51:14,670 manager.py:244] Polling instance infos of 0 instances ends.
[36m(Scaler pid=286112)[0m INFO 2025-07-16 11:51:16,046 ray_utils.py:277] Put data scaler.client_index to ray internal key-value storage, value: 1.
[36m(Scaler pid=286112)[0m INFO 2025-07-16 11:51:16,047 scaler.py:330] Deploy server and instance to new placement group done, instance_id: 3dee03162adb41bb8d822a9094d655e7.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:16,049 ray_utils.py:121] placement_group_specs: [{'CPU': 3, 'GPU': 1}]
[36m(Scaler pid=286112)[0m INFO 2025-07-16 11:51:16,062 ray_utils.py:277] Put data scaler.client_index to ray internal key-value storage, value: 2.
[36m(Scaler pid=286112)[0m INFO 2025-07-16 11:51:16,063 scaler.py:330] Deploy server and instance to new placement group done, instance_id: 03542b716540448590d4d3071ed2b720.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:16,065 ray_utils.py:121] placement_group_specs: [{'CPU': 3, 'GPU': 1}]
[36m(Scaler pid=286112)[0m INFO 2025-07-16 11:51:16,077 ray_utils.py:277] Put data scaler.client_index to ray internal key-value storage, value: 3.
[36m(Scaler pid=286112)[0m INFO 2025-07-16 11:51:16,077 scaler.py:330] Deploy server and instance to new placement group done, instance_id: b27ed7dece7d4c62a9bb7762232a997a.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:16,080 ray_utils.py:121] placement_group_specs: [{'CPU': 3, 'GPU': 1}]
[36m(Scaler pid=286112)[0m INFO 2025-07-16 11:51:16,091 ray_utils.py:277] Put data scaler.client_index to ray internal key-value storage, value: 4.
[36m(Scaler pid=286112)[0m INFO 2025-07-16 11:51:16,092 scaler.py:330] Deploy server and instance to new placement group done, instance_id: bfc7af3298cf48cca665d1745eaf17b6.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:16,095 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:17,099 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:18,109 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:19,119 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:20,129 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:21,135 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet pid=287181)[0m INFO 2025-07-16 11:51:21,443 ray_utils.py:299] Llumlet(actor_name=instance_909ad421560a46629feb76815bafea61, actor_id=3899d863a72ba721b8711ea702000000, placement_group_id=124c0e93b8b8f6e8f4f111b66cc402000000, namespace=llumnix, job_id=02000000, worker_id=f5fac0c180ab59247cb27a8ed1ba2a5d79046d624a586d38b3d874a0, node_id=bbb26e1cd3886c3e1cefaf42390b7ad5038fb045b136b20edd28e191, gpu_ids=['3'])
[36m(Llumlet pid=287181)[0m INFO 2025-07-16 11:51:21,444 ray_utils.py:307] Llumlet(log_dir=['/tmp/ray/session_2025-07-16_11-50-40_561788_277830/logs/worker-f5fac0c180ab59247cb27a8ed1ba2a5d79046d624a586d38b3d874a0-02000000-287181.err', '/tmp/ray/session_2025-07-16_11-50-40_561788_277830/logs/worker-f5fac0c180ab59247cb27a8ed1ba2a5d79046d624a586d38b3d874a0-02000000-287181.out'])
[36m(Llumlet pid=287181)[0m INFO 2025-07-16 11:51:21,444 llumlet.py:50] Llumlet(instance_id=909ad421560a46629feb76815bafea61, backend_type=BackendType.VLLM_V1, instance_type=no_constraints)
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 2025-07-16 11:51:21,459 ray_utils.py:299] APIServerActorVLLMV1(actor_name=server_909ad421560a46629feb76815bafea61, actor_id=0700aa49b6d5c081e3e7acf302000000, placement_group_id=124c0e93b8b8f6e8f4f111b66cc402000000, namespace=llumnix, job_id=02000000, worker_id=361f73ade5c4da9edf56233b9c44f3296745820a3b77a91216087d29, node_id=bbb26e1cd3886c3e1cefaf42390b7ad5038fb045b136b20edd28e191, gpu_ids=['3'])
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 2025-07-16 11:51:21,459 ray_utils.py:307] APIServerActorVLLMV1(log_dir=['/tmp/ray/session_2025-07-16_11-50-40_561788_277830/logs/worker-361f73ade5c4da9edf56233b9c44f3296745820a3b77a91216087d29-02000000-287182.out', '/tmp/ray/session_2025-07-16_11-50-40_561788_277830/logs/worker-361f73ade5c4da9edf56233b9c44f3296745820a3b77a91216087d29-02000000-287182.err'])
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:22,143 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 2025-07-16 11:51:22,680 zmq_server.py:69] QueueServer's socket bind to: tcp://172.18.0.3:11182
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:23,151 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet pid=287181)[0m INFO 07-16 11:51:24.089 [__init__.py:244] Automatically detected platform cuda.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:24,159 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:51:24.305 [__init__.py:244] Automatically detected platform cuda.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:25,163 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:26,171 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:27,179 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:28,187 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:29,195 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:30,203 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:31,211 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:32,219 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet pid=287181)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:33,228 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet pid=287181)[0m INFO 07-16 11:51:33.323 [config.py:934] This model supports multiple tasks: {'embed', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.
[36m(Llumlet pid=287181)[0m INFO 07-16 11:51:33.323 [config.py:1578] Using max model len 4096
[36m(Llumlet pid=287181)[0m WARNING 07-16 11:51:33.323 [config.py:807] `disable_mm_preprocessor_cache` is only supported for multimodal models.
[36m(Llumlet pid=287181)[0m WARNING 07-16 11:51:33.324 [arg_utils.py:2153] Detected VLLM_USE_V1=1 with Engine in background thread. Usage should be considered experimental. Please report any issues on Github.
[36m(Llumlet pid=287181)[0m INFO 07-16 11:51:33.325 [arg_utils.py:1414] CK/AITER is disabled for FP8/MoE since we are not on ROCm.
[36m(Llumlet pid=287181)[0m INFO 07-16 11:51:33.325 [config.py:2145] Data parallel size: 1, local size: 1, rank: 0, local rank: None, master ip: 127.0.0.1, master port: 0
[36m(Llumlet pid=287181)[0m INFO 07-16 11:51:33.326 [config.py:2510] Chunked prefill is enabled with max_num_batched_tokens=16000.
[36m(Llumlet pid=287181)[0m INFO 07-16 11:51:33.389 [core.py:73] Initializing a V1 LLM engine (v0.9.1) with config: model='/mnt/model/Qwen2.5-7B', speculative_config=None, tokenizer='/mnt/model/Qwen2.5-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, block_size=256, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, profile=False, profile_torch=False, profile_step_start=16, profile_step_end=32, profile_prefill_and_decode=False), seed=0, served_model_name=/mnt/model/Qwen2.5-7B, num_scheduler_steps=1, multi_step_stream_outputs=True, policy=fcfs, heartbeat_interval=0.15, batch_timeout=2700, enable_prefix_caching=True, enable_hidden_state_caching=False, enable_chunked_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["+rms_norm","+rms_norm"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}, vllm path: /test_workspace/pai-vllm/vllm/v1/engine/core.py
[36m(Llumlet pid=287181)[0m INFO 07-16 11:51:33.391 [ray_utils.py:287] Ray is already initialized. Skipping Ray initialization.
[36m(Llumlet pid=287181)[0m INFO 07-16 11:51:33.391 [ray_utils.py:315] Using the existing placement group
[36m(Llumlet pid=287181)[0m INFO 2025-07-16 11:51:33,392 executor.py:90] use_ray_spmd_worker: True
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:51:33.645 [config.py:934] This model supports multiple tasks: {'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:51:33.646 [config.py:1578] Using max model len 4096
[36m(APIServerActorVLLMV1 pid=287182)[0m WARNING 07-16 11:51:33.646 [config.py:807] `disable_mm_preprocessor_cache` is only supported for multimodal models.
[36m(APIServerActorVLLMV1 pid=287182)[0m WARNING 07-16 11:51:33.646 [arg_utils.py:2153] Detected VLLM_USE_V1=1 with Engine in background thread. Usage should be considered experimental. Please report any issues on Github.
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:51:33.646 [arg_utils.py:1414] CK/AITER is disabled for FP8/MoE since we are not on ROCm.
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:51:33.647 [config.py:2145] Data parallel size: 1, local size: 1, rank: 0, local rank: None, master ip: 127.0.0.1, master port: 0
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:51:33.647 [config.py:2510] Chunked prefill is enabled with max_num_batched_tokens=16000.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:34,231 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:35,237 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:36,244 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:37,252 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:38,259 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(pid=287638)[0m INFO 07-16 11:51:38.413 [__init__.py:244] Automatically detected platform cuda.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:39,267 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:40,275 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet pid=287181)[0m DEBUG 2025-07-16 11:51:40,934 executor.py:203] workers: [RayWorkerMetaData(worker=Actor(RayWorkerWrapper, 9c7492752445668b0b29673202000000), created_rank=0, adjusted_rank=-1, ip='172.18.0.3')]
[36m(Llumlet pid=287181)[0m DEBUG 2025-07-16 11:51:40,935 executor.py:204] driver_dummy_worker: None
[36m(Llumlet pid=287181)[0m INFO 2025-07-16 11:51:40,937 executor.py:304] non_carry_over_env_vars from config: set()
[36m(Llumlet pid=287181)[0m INFO 2025-07-16 11:51:40,938 executor.py:306] Copying the following environment variables to workers: ['MAX_JOBS', 'NVCC_THREADS', 'CUDA_HOME', 'LD_LIBRARY_PATH', 'VLLM_USE_RAY_SPMD_WORKER', 'VLLM_USE_RAY_COMPILED_DAG', 'VLLM_USE_V1', 'VLLM_ROCM_USE_AITER', 'VLLM_ROCM_USE_AITER_LINEAR', 'VLLM_ROCM_USE_AITER_MOE', 'VLLM_ROCM_USE_AITER_RMSNORM', 'VLLM_ROCM_USE_AITER_MLA', 'VLLM_FORCE_DETOKENIZE', 'VLLM_ENABLE_LLUMNIX']
[36m(Llumlet pid=287181)[0m INFO 2025-07-16 11:51:40,938 executor.py:309] If certain env vars should NOT be copied to workers, add them to /root/.config/vllm/ray_non_carry_over_env_vars.json file
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:41,283 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(RayWorkerWrapper pid=287638)[0m INFO 07-16 11:51:41.368 [fa_utils.py:99] Set flash-attn version to 2
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:42,291 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:43,299 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:44,307 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:45,313 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:46,319 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:47,327 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(RayWorkerWrapper pid=287638)[0m Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[36m(RayWorkerWrapper pid=287638)[0m Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.69it/s]
[36m(RayWorkerWrapper pid=287638)[0m Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.61it/s]
[36m(RayWorkerWrapper pid=287638)[0m Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.67it/s]
[36m(RayWorkerWrapper pid=287638)[0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.69it/s]
[36m(RayWorkerWrapper pid=287638)[0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.68it/s]
[36m(RayWorkerWrapper pid=287638)[0m 
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:   0%|                                                                                   | 0/67 [00:00<?, ?it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:   1%|█                                                                          | 1/67 [00:00<00:13,  4.88it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:   3%|██▏                                                                        | 2/67 [00:00<00:13,  4.81it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:   4%|███▎                                                                       | 3/67 [00:00<00:13,  4.79it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:   6%|████▍                                                                      | 4/67 [00:00<00:13,  4.78it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:   7%|█████▌                                                                     | 5/67 [00:01<00:13,  4.67it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:   9%|██████▋                                                                    | 6/67 [00:01<00:12,  4.72it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  10%|███████▊                                                                   | 7/67 [00:01<00:12,  4.76it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  12%|████████▉                                                                  | 8/67 [00:01<00:12,  4.77it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  13%|██████████                                                                 | 9/67 [00:01<00:12,  4.74it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  15%|███████████                                                               | 10/67 [00:02<00:11,  4.75it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  16%|████████████▏                                                             | 11/67 [00:02<00:11,  4.79it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  18%|█████████████▎                                                            | 12/67 [00:02<00:11,  4.83it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  19%|██████████████▎                                                           | 13/67 [00:02<00:11,  4.85it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  21%|███████████████▍                                                          | 14/67 [00:02<00:10,  4.82it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  22%|████████████████▌                                                         | 15/67 [00:03<00:10,  4.85it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  24%|█████████████████▋                                                        | 16/67 [00:03<00:10,  4.89it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  25%|██████████████████▊                                                       | 17/67 [00:03<00:09,  5.11it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  27%|███████████████████▉                                                      | 18/67 [00:03<00:09,  5.28it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  28%|████████████████████▉                                                     | 19/67 [00:03<00:08,  5.40it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  30%|██████████████████████                                                    | 20/67 [00:04<00:08,  5.49it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  31%|███████████████████████▏                                                  | 21/67 [00:04<00:08,  5.59it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  33%|████████████████████████▎                                                 | 22/67 [00:04<00:07,  5.66it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  34%|█████████████████████████▍                                                | 23/67 [00:04<00:07,  5.74it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  36%|██████████████████████████▌                                               | 24/67 [00:04<00:07,  5.78it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  37%|███████████████████████████▌                                              | 25/67 [00:04<00:07,  5.78it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  39%|████████████████████████████▋                                             | 26/67 [00:05<00:07,  5.82it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  40%|█████████████████████████████▊                                            | 27/67 [00:05<00:06,  5.86it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  42%|██████████████████████████████▉                                           | 28/67 [00:05<00:06,  5.90it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  43%|████████████████████████████████                                          | 29/67 [00:05<00:06,  5.94it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  45%|█████████████████████████████████▏                                        | 30/67 [00:05<00:06,  5.95it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  46%|██████████████████████████████████▏                                       | 31/67 [00:05<00:06,  5.96it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  48%|███████████████████████████████████▎                                      | 32/67 [00:06<00:05,  5.99it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  49%|████████████████████████████████████▍                                     | 33/67 [00:06<00:05,  6.33it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  51%|█████████████████████████████████████▌                                    | 34/67 [00:06<00:04,  6.60it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  52%|██████████████████████████████████████▋                                   | 35/67 [00:06<00:04,  6.84it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  54%|███████████████████████████████████████▊                                  | 36/67 [00:06<00:04,  7.03it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  55%|████████████████████████████████████████▊                                 | 37/67 [00:06<00:04,  7.14it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  57%|█████████████████████████████████████████▉                                | 38/67 [00:06<00:04,  7.23it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  58%|███████████████████████████████████████████                               | 39/67 [00:06<00:03,  7.34it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  60%|████████████████████████████████████████████▏                             | 40/67 [00:07<00:03,  7.41it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  61%|█████████████████████████████████████████████▎                            | 41/67 [00:07<00:03,  7.75it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  63%|██████████████████████████████████████████████▍                           | 42/67 [00:07<00:03,  8.22it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  64%|███████████████████████████████████████████████▍                          | 43/67 [00:07<00:02,  8.60it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  66%|████████████████████████████████████████████████▌                         | 44/67 [00:07<00:02,  8.89it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  67%|█████████████████████████████████████████████████▋                        | 45/67 [00:07<00:02,  9.10it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  69%|██████████████████████████████████████████████████▊                       | 46/67 [00:07<00:02,  9.25it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  70%|███████████████████████████████████████████████████▉                      | 47/67 [00:07<00:02,  9.41it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  72%|█████████████████████████████████████████████████████                     | 48/67 [00:07<00:02,  9.44it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  75%|███████████████████████████████████████████████████████▏                  | 50/67 [00:08<00:01, 10.79it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  78%|█████████████████████████████████████████████████████████▍                | 52/67 [00:08<00:01, 11.63it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  81%|███████████████████████████████████████████████████████████▋              | 54/67 [00:08<00:01, 12.22it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  84%|█████████████████████████████████████████████████████████████▊            | 56/67 [00:08<00:00, 12.31it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  87%|████████████████████████████████████████████████████████████████          | 58/67 [00:08<00:00, 13.01it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  90%|██████████████████████████████████████████████████████████████████▎       | 60/67 [00:08<00:00, 13.55it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  93%|████████████████████████████████████████████████████████████████████▍     | 62/67 [00:08<00:00, 14.13it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  96%|██████████████████████████████████████████████████████████████████████▋   | 64/67 [00:09<00:00, 14.48it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes:  99%|████████████████████████████████████████████████████████████████████████▉ | 66/67 [00:09<00:00, 14.83it/s]
[36m(RayWorkerWrapper pid=287638)[0m Capturing CUDA graph shapes: 100%|██████████████████████████████████████████████████████████████████████████| 67/67 [00:09<00:00,  7.20it/s]
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:48,335 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(RayWorkerWrapper pid=287638)[0m [0/1]INFO 07-16 11:51:48.958 [parallel_state.py:1176] using initialize model parallel backend=nccl
[36m(RayWorkerWrapper pid=287638)[0m [0/1]INFO 07-16 11:51:48.958 [parallel_state.py:1098] TP ranks: [[0]]
[36m(RayWorkerWrapper pid=287638)[0m [0/1]INFO 07-16 11:51:48.959 [parallel_state.py:1116] PP ranks: [[0]]
[36m(RayWorkerWrapper pid=287638)[0m [0/1]INFO 07-16 11:51:48.960 [parallel_state.py:1133] DP ranks: [[0]]
[36m(RayWorkerWrapper pid=287638)[0m [0/1]INFO 07-16 11:51:48.961 [parallel_state.py:1147] EP ranks: [[0]]
[36m(RayWorkerWrapper pid=287638)[0m [0/1]INFO 07-16 11:51:48.961 [parallel_state.py:1156] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[36m(RayWorkerWrapper pid=287638)[0m [0/1]INFO 07-16 11:51:48.961 [cuda.py:282] Using Flash Attention backend on V1 engine.
[36m(RayWorkerWrapper pid=287638)[0m [0/1]INFO 07-16 11:51:48.966 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.
[36m(RayWorkerWrapper pid=287638)[0m [0/1]INFO 07-16 11:51:48.981 [gpu_model_runner.py:1954] Starting to load model /mnt/model/Qwen2.5-7B...
[36m(RayWorkerWrapper pid=287638)[0m [0/1]INFO 07-16 11:51:49.203 [gpu_model_runner.py:1958] using quick model loader to load model
[36m(RayWorkerWrapper pid=287638)[0m [0/1]WARNING 07-16 11:51:49.203 [gpu_model_runner.py:1965] using quick model loader fails: Quick model loader is not imported.
[36m(RayWorkerWrapper pid=287638)[0m [0/1]INFO 07-16 11:51:49.203 [gpu_model_runner.py:1969] Loading model from scratch...
[36m(RayWorkerWrapper pid=287638)[0m [0/1]INFO 07-16 11:51:49.203 [qwen2.py:412] Using rope scaling: None
[36m(RayWorkerWrapper pid=287638)[0m [0/1]INFO 07-16 11:51:49.203 [qwen2.py:439] Initialize Qwen2Model: rope_theta = 1000000.0, rope_scaling = None, dual_chunk_attention_config = None
[36m(RayWorkerWrapper pid=287638)[0m [0/1]WARNING 07-16 11:51:49.317 [config.py:5182] `torch.compile` is turned on, but the model /mnt/model/Qwen2.5-7B does not support it. Please open an issue on GitHub if you want it to be supported.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:49,343 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:50,351 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:51,359 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(RayWorkerWrapper pid=287638)[0m [0/1]INFO 07-16 11:51:51.750 [default_loader.py:280] Loading weights took 2.43 seconds
[36m(RayWorkerWrapper pid=287638)[0m [0/1]INFO 07-16 11:51:52.242 [gpu_model_runner.py:1995] Model loading took 14.2419 GiB and 2.551503 seconds
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:52,367 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:53,375 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:54,383 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:55,387 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(RayWorkerWrapper pid=287638)[0m [0/1]INFO 07-16 11:51:56.265 [gpu_worker.py:257] Available KV cache memory: 2.96 GiB
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:56,395 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet pid=287181)[0m INFO 07-16 11:51:56.480 [kv_cache_utils.py:721] GPU KV cache size: 55,296 tokens
[36m(Llumlet pid=287181)[0m INFO 07-16 11:51:56.480 [kv_cache_utils.py:725] Maximum concurrency for 4,096 tokens per request: 13.50x
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:57,403 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:58,411 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:51:59,419 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:00,427 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:01,435 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:02,443 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:03,451 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:04,459 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m DEBUG 2025-07-16 11:52:04,946 manager.py:241] Polling instance infos of 0 instances starts.
[36m(Manager pid=286358)[0m DEBUG 2025-07-16 11:52:04,946 manager.py:244] Polling instance infos of 0 instances ends.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:05,464 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet pid=287181)[0m INFO 07-16 11:52:05.838 [core.py:182] init engine (profile, create kv cache, warmup model) took 13.59 seconds
[36m(RayWorkerWrapper pid=287638)[0m [0/1]INFO 07-16 11:52:05.798 [gpu_model_runner.py:2575] Graph capturing finished in 9 secs, took 0.12 GiB
[36m(Llumlet pid=287181)[0m WARNING 07-16 11:52:06.161 [core.py:108] Using configured V1 scheduler class <class 'llumnix.backends.vllm_v1.scheduler.SchedulerLlumnix'>. This scheduler interface is not public and compatibility may not be maintained.
[36m(Llumlet pid=287181)[0m INFO 2025-07-16 11:52:06,163 core.py:392] Migration is disabled, skip migration initialization.
[36m(Llumlet pid=287181)[0m INFO 2025-07-16 11:52:06,163 core.py:395] engine 909ad421560a46629feb76815bafea61 current state: EngineState.INIT
[36m(Llumlet pid=287181)[0m DEBUG 2025-07-16 11:52:06,163 async_core.py:299] EngineCore waiting for work.
[36m(APIServerActorVLLMV1 pid=287182)[0m WARNING 07-16 11:52:06.194 [config.py:1498] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.194 [serving_chat.py:125] Using default chat sampling params from model: {'max_tokens': 2048}
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [serving_completion.py:73] Using default completion sampling params from model: {'max_tokens': 2048}
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [api_server.py:1483] Starting vLLM API server 0 on http://172.18.0.3:20038
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [launcher.py:29] Available routes are:
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [launcher.py:37] Route: /openapi.json, Methods: GET, HEAD
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [launcher.py:37] Route: /docs, Methods: GET, HEAD
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [launcher.py:37] Route: /redoc, Methods: GET, HEAD
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [launcher.py:37] Route: /health, Methods: GET
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [launcher.py:37] Route: /load, Methods: GET
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [launcher.py:37] Route: /ping, Methods: POST
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO:     Started server process [287182]
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO:     Waiting for application startup.
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO:     Application startup complete.
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [launcher.py:37] Route: /ping, Methods: GET
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [launcher.py:37] Route: /tokenize, Methods: POST
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [launcher.py:37] Route: /detokenize, Methods: POST
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [launcher.py:37] Route: /v1/models, Methods: GET
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [launcher.py:37] Route: /version, Methods: GET
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [launcher.py:37] Route: /v1/chat/completions, Methods: POST
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [launcher.py:37] Route: /v1/completions, Methods: POST
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [launcher.py:37] Route: /v1/embeddings, Methods: POST
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [launcher.py:37] Route: /pooling, Methods: POST
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [launcher.py:37] Route: /classify, Methods: POST
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [launcher.py:37] Route: /score, Methods: POST
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [launcher.py:37] Route: /v1/score, Methods: POST
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [launcher.py:37] Route: /v1/audio/translations, Methods: POST
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [launcher.py:37] Route: /rerank, Methods: POST
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [launcher.py:37] Route: /v1/rerank, Methods: POST
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [launcher.py:37] Route: /v2/rerank, Methods: POST
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [launcher.py:37] Route: /invocations, Methods: POST
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO 07-16 11:52:06.195 [launcher.py:37] Route: /metrics, Methods: GET
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:06,472 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:06,541 global_scheduler.py:165] Scale up instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:06,541 global_scheduler.py:170] num_instances: 1, instances: {'909ad421560a46629feb76815bafea61'}
[36m(APIServerActorVLLMV1 pid=287182)[0m INFO:     172.18.0.3:37142 - "GET /health HTTP/1.1" 200 OK
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:07,480 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:08,487 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:09,495 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:10.319 [chat_utils.py:423] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:10.323 [logger.py:43] Received request chatcmpl-b690c8d433cc44d2af2456d394b5c903: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nhello<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:10,324 client.py:72] Client received request chatcmpl-b690c8d433cc44d2af2456d394b5c903
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:10,503 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:11,511 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:12,519 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 07-16 11:52:13.027 [__init__.py:244] Automatically detected platform cuda.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:13,450 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: DummyLoad
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:13,450 global_scheduler.py:94] dispath request chatcmpl-b690c8d433cc44d2af2456d394b5c903 to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m DEBUG 2025-07-16 11:52:13,452 async_core.py:306] EngineCore loop active.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 07-16 11:52:13.469 [ray_distributed_executor.py:569] RAY_CGRAPH_get_timeout is set to 300
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 07-16 11:52:13.469 [ray_distributed_executor.py:571] VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE = auto
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 07-16 11:52:13.469 [ray_distributed_executor.py:573] VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM = False
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:13.451 [async_llm.py:270] Added request chatcmpl-b690c8d433cc44d2af2456d394b5c903.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:13,527 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:13,567 core.py:157] Engine finished request chatcmpl-b690c8d433cc44d2af2456d394b5c903
[36m(Llumlet(iid=909ad) pid=287181)[0m DEBUG 2025-07-16 11:52:13,569 async_core.py:299] EngineCore waiting for work.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:13,571 client.py:181] Client finished request chatcmpl-b690c8d433cc44d2af2456d394b5c903.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:49892 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:14,607 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:15,615 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:16.430 [loggers.py:146] Engine 000: Avg prompt throughput: 0.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Avg (@wuhou) audit only prefill throughput: 0.0 tokens/s, Avg (@wuhou) audit only decode throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:16,624 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:17,633 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:18,036 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=inf,is_busy=False)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:18,036 global_scheduler.py:94] dispath request chatcmpl-b4006ea08b454e039980b8306f33e8f0 to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m DEBUG 2025-07-16 11:52:18,038 async_core.py:306] EngineCore loop active.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:18.034 [logger.py:43] Received request chatcmpl-b4006ea08b454e039980b8306f33e8f0: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nWhat are real estate developers dream outcomes in a bear market<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=334, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:18,034 client.py:72] Client received request chatcmpl-b4006ea08b454e039980b8306f33e8f0
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:18.037 [async_llm.py:270] Added request chatcmpl-b4006ea08b454e039980b8306f33e8f0.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:18,283 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=214.0,is_busy=False)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:18,283 global_scheduler.py:94] dispath request chatcmpl-4282d532719445c7827a3808da49e878 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:18.281 [logger.py:43] Received request chatcmpl-4282d532719445c7827a3808da49e878: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\ni am using a neural network to do object detection in documents. can you think of documents that might be very repititive that a single company would produce? and describe why they are similar and would be easy to fine tune a NN on<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=336, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:18,281 client.py:72] Client received request chatcmpl-4282d532719445c7827a3808da49e878
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:18.284 [async_llm.py:270] Added request chatcmpl-4282d532719445c7827a3808da49e878.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:18,641 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:18,600 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=106.5,is_busy=False)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:18,600 global_scheduler.py:94] dispath request chatcmpl-7c7ea23b49f244ea945a148aa2830676 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:18.597 [logger.py:43] Received request chatcmpl-7c7ea23b49f244ea945a148aa2830676: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nJe veux écrire un cold email à des minis entreprises où le dirigeant brasse la bière le soir après le travail (on a des pics de lectures autour de 22H). Je vends un laveur de keg.\n\nJ'aimerais que cet email respecte les principes du copywriting, notamment ceux d'Ogilvy, Bly ou Halbert.\n\nPose moi les questions nécessaires pour que tu puisses avoir de quoi écrire cet email<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=271, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:18,598 client.py:72] Client received request chatcmpl-7c7ea23b49f244ea945a148aa2830676
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:18.601 [async_llm.py:270] Added request chatcmpl-7c7ea23b49f244ea945a148aa2830676.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:18,743 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=70.66666666666667,is_busy=False)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:18,743 global_scheduler.py:94] dispath request chatcmpl-932c264d32274e5aaa6ffdbd155b4f02 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:18,744 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=70.66666666666667,is_busy=False)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:18,745 global_scheduler.py:94] dispath request chatcmpl-7cf89c0d7b654debbc8ade1313a626a0 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:18,765 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=42.0,is_busy=False)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:18,765 global_scheduler.py:94] dispath request chatcmpl-2644b5838d844f968fe9aefa75a625d6 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:18.741 [logger.py:43] Received request chatcmpl-932c264d32274e5aaa6ffdbd155b4f02: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nI have one week of travel. pick a city matching this and give me a detailed itenary. my budget for the week is $2500 including flights and it is me and my girlfriend: Quiet/warm beaches that are safe, relatively affordable, and not in the US. Good resorts etc<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=607, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:18,742 client.py:72] Client received request chatcmpl-932c264d32274e5aaa6ffdbd155b4f02
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:18.743 [logger.py:43] Received request chatcmpl-7cf89c0d7b654debbc8ade1313a626a0: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nWrite an angry rant as Cave Johnson about all the best guys being straight<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=577, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:18,743 client.py:72] Client received request chatcmpl-7cf89c0d7b654debbc8ade1313a626a0
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:18.744 [async_llm.py:270] Added request chatcmpl-932c264d32274e5aaa6ffdbd155b4f02.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:18.745 [async_llm.py:270] Added request chatcmpl-7cf89c0d7b654debbc8ade1313a626a0.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:18.763 [logger.py:43] Received request chatcmpl-2644b5838d844f968fe9aefa75a625d6: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nDo you know who wrote this:\n\\_\\_\\_\n\nFootball\n\nI take the snap from the center, fake to the right, fade back...\nI've got protection. I've got a receiver open downfield...\nWhat the hell is this? This isn't a football, it's a shoe, a man's\nbrown leather oxford. A cousin to a football maybe, the same\nskin, but not the same, a thing made for the earth, not the air.\nI realize that this is a world where anything is possible and I\nunderstand, also, that one often has to make do with what one\nhas. I have eaten pancakes, for instance, with that clear corn\nsyrup on them because there was no maple syrup and they\nweren't very good. Well, anyway, this is different. (My man\ndownfield is waving his arms.) One has certain responsibilities,\none has to make choices. This isn't right and I'm not going\nto throw it.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=75, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:18,764 client.py:72] Client received request chatcmpl-2644b5838d844f968fe9aefa75a625d6
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:18.766 [async_llm.py:270] Added request chatcmpl-2644b5838d844f968fe9aefa75a625d6.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:18.777 [logger.py:43] Received request chatcmpl-f97d63d4c0234139932827e34948601c: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nNeed to setup an inventory tracking system for film rental gear. It should integrate with our internal Linux server and support thermal label printers<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=723, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:18,778 client.py:72] Client received request chatcmpl-f97d63d4c0234139932827e34948601c
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:18,779 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=42.0,is_busy=False)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:18,779 global_scheduler.py:94] dispath request chatcmpl-f97d63d4c0234139932827e34948601c to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:18.780 [async_llm.py:270] Added request chatcmpl-f97d63d4c0234139932827e34948601c.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:18,910 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=29.714285714285715,is_busy=False)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:18,910 global_scheduler.py:94] dispath request chatcmpl-ef001fcc4e1c4db9becd2e6326e48ff1 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:18.908 [logger.py:43] Received request chatcmpl-ef001fcc4e1c4db9becd2e6326e48ff1: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n补写出下列句子中的空缺部分：\n李商隐《锦瑟》“\\_\\_\\_\\_、\\_\\_\\_\\_”两句中的数目字，引发了后世读者的多种解读。<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=43, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:18,909 client.py:72] Client received request chatcmpl-ef001fcc4e1c4db9becd2e6326e48ff1
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:18.911 [async_llm.py:270] Added request chatcmpl-ef001fcc4e1c4db9becd2e6326e48ff1.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:19,043 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=25.875,is_busy=False)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:19,043 global_scheduler.py:94] dispath request chatcmpl-35a9fea6fb8241cda525bc02916a1ff2 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:19,080 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=22.88888888888889,is_busy=False)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:19,080 global_scheduler.py:94] dispath request chatcmpl-a6b4703e50b14bf291af2fa3e9a7724c to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:19.041 [logger.py:43] Received request chatcmpl-35a9fea6fb8241cda525bc02916a1ff2: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nProvide a step-by-step guide on how to create an NFT to represent private property and legally sell it on the Ethereum chain, suitable for someone with no experience in selling property or creating an NFT. The guide should include an introduction to NFTs and their uses for representing property ownership, step-by-step instructions on how to create an NFT on the Ethereum chain, guidance on how to assign ownership of the private property to the NFT, legal considerations for selling the NFT, including taxes and regulations, and options for selling the NFT through marketplaces or auction platforms. Additionally, provide guidance on the process of transferring ownership of a property.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=670, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:19,042 client.py:72] Client received request chatcmpl-35a9fea6fb8241cda525bc02916a1ff2
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:19.044 [async_llm.py:270] Added request chatcmpl-35a9fea6fb8241cda525bc02916a1ff2.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:19.078 [logger.py:43] Received request chatcmpl-a6b4703e50b14bf291af2fa3e9a7724c: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nI want to build a DIY pool heater controller for my Mastertemp 250 pool heater. I have an ARDUINO UNO WiFi REV2 to use for this project. Can you help me to know what else I'll need physically to do this project? I can handle the coding myself.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=468, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:19,079 client.py:72] Client received request chatcmpl-a6b4703e50b14bf291af2fa3e9a7724c
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:19.081 [async_llm.py:270] Added request chatcmpl-a6b4703e50b14bf291af2fa3e9a7724c.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:19,267 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=20.5,is_busy=False)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:19,267 global_scheduler.py:94] dispath request chatcmpl-2218e6f34a3a437a8c3a6a9679f872a6 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:19.266 [logger.py:43] Received request chatcmpl-2218e6f34a3a437a8c3a6a9679f872a6: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n다이어트를 하려고 해. 1달 동안 식단을 이용해서 5kg 정도 빼는 게 목표야. 1달치 식단을 짜줘.\n단 몇 가지 조건이 있어.\n1. 매주 일요일은 치팅데이이기 때문에 프리하게 음식을 먹을 수 있어. 일요일은 그냥 치팅데이라고만 예쁘게 써줘\n2. 월~금 아침식사는 닭가슴살을 반드시 포함하고, 샐러드나 시리얼 같이 매우 간단한 메뉴를 곁들여줘.\n3. 월~금 점심식사는 회사에서 일반식으로 먹기 때문에 회사 일반식 + 현미밥으로 표시해줘\n4. 성인 남성이 일반적인 양으로 식사했을 때의 kcal를 각 식단별로 표시해줘\n5. 한 끼당 최소 3개의 반찬으로 구성해줘.\n6. 하루 3끼 식사로 섭취하는 총 kcal가 일일 성인 남성 1일 권장량의 80% 이내로 식단을 구성해줘\n7. 표 형태로 작성해줘\n\nPlease write in Korean language.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=171, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:19,266 client.py:72] Client received request chatcmpl-2218e6f34a3a437a8c3a6a9679f872a6
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:19.268 [async_llm.py:270] Added request chatcmpl-2218e6f34a3a437a8c3a6a9679f872a6.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:19,347 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=18.454545454545453,is_busy=False)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:19,347 global_scheduler.py:94] dispath request chatcmpl-56071ddef7de4425838523edc1850843 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:19.345 [logger.py:43] Received request chatcmpl-56071ddef7de4425838523edc1850843: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\ni have a single pdf that needs printing. I want to count how many pages are colour vs black and white without manually counting them. how can i do this<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=452, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:19,346 client.py:72] Client received request chatcmpl-56071ddef7de4425838523edc1850843
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:19.348 [async_llm.py:270] Added request chatcmpl-56071ddef7de4425838523edc1850843.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:19,546 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=16.833333333333332,is_busy=False)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:19,546 global_scheduler.py:94] dispath request chatcmpl-9aeed085dae34254a2809e02da43cb05 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:19.545 [logger.py:43] Received request chatcmpl-9aeed085dae34254a2809e02da43cb05: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n장현민\n너는 SNS 마케팅 전문가 역할이야. 내가 직접 고객과 인터뷰한 내용을 바탕으로 소구점을 찾아줘.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=180, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:19,545 client.py:72] Client received request chatcmpl-9aeed085dae34254a2809e02da43cb05
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:19.547 [async_llm.py:270] Added request chatcmpl-9aeed085dae34254a2809e02da43cb05.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:19,647 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:19,694 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=15.461538461538462,is_busy=False)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:19,695 global_scheduler.py:94] dispath request chatcmpl-a69156db380f4c36b95f98e7352e78b6 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:19.693 [logger.py:43] Received request chatcmpl-a69156db380f4c36b95f98e7352e78b6: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nYou are acting as a code explorer. Your job is to figure out the code structure of a project, meaning what it does, where is the main function, what API it offers, etc. \n\nWhat way it works is that each time you output a summary and a command, and I'll offer you with the command's output in next round chat. Summary is a plain text representation of what you have understood, command is either `ls ` for listing the file available at path , `cat ` for accessing content of a file at , or `search a\\_keyword` for listing the lines containing the keyword. \n\nNotes:\n- Starting with `ls ./` is usually a good idea. \n\nNow please start.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:19,693 client.py:72] Client received request chatcmpl-a69156db380f4c36b95f98e7352e78b6
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:19.695 [async_llm.py:270] Added request chatcmpl-a69156db380f4c36b95f98e7352e78b6.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:19,893 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=14.285714285714286,is_busy=False)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:19,893 global_scheduler.py:94] dispath request chatcmpl-6cdde79e7809413ebb917d0e88dfd3e9 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:19,895 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=14.285714285714286,is_busy=False)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:19,896 global_scheduler.py:94] dispath request chatcmpl-7290ef616e6543c2a0461e29825a4aef to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:19,920 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=14.285714285714286,is_busy=False)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:19,920 global_scheduler.py:94] dispath request chatcmpl-f1a75b7b01424da488b3b0008cf72e2d to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:19.891 [logger.py:43] Received request chatcmpl-6cdde79e7809413ebb917d0e88dfd3e9: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nin front of us are eight gears numbered 1 to 8, mounted on axles in a row. Each gear is engaged with the next gear. If gear number 3 is rotated clockwise, in which direction will gears 1 and 8 rotate? response directly.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=28, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:19,892 client.py:72] Client received request chatcmpl-6cdde79e7809413ebb917d0e88dfd3e9
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:19.894 [async_llm.py:270] Added request chatcmpl-6cdde79e7809413ebb917d0e88dfd3e9.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:19.894 [logger.py:43] Received request chatcmpl-7290ef616e6543c2a0461e29825a4aef: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nEstoy empezando a preparar un tema sobre el uso de nombres dependientes y no-dependientes en el trabajo con plantillas de clase en C++. En este primer código:\ntemplate \nstruct analizador\\_base\n{\n using tipo\\_valor = T;\n};\ntemplate \nstruct analizador : analizador\\_base\n{\n void analizar()\n {\n tipo\\_valor v{}; // [1] error\n // or\n analizador\\_base::tipo\\_valor v{}; // [2] error\n std::cout << "analizar\\n";\n }\n};\nCreamos dos plantillas de clase: "analizador\\_base" y "analizador" que deriva de la primera. Dentro de la plantilla de clase base creamos un alias de tipo, de nombre "tipo\\_valor" que hacemos igual al tipo genérico "T" que espera esta clase. La clase derivada cuenta con una función miembro analizar( ) que necesita acceder a ese alias de tipo. Pero tanto tipo\\_valor v{ }, como analizador\\_base::tipo\\_valor v{ }; devuelven error. Es un tema que quiero que veamos en detalle pero centrándonos en temas concretos sin dispersarnos con explicaciones genéricas. El primer tema concreto que quiero comentar contigo: ¿Cómo se puede explicar del modo más claro posible lo que intentamos hacer en esta línea con tipo\\_valor v{ };? Y lo mismo con ¿analizador\\_base::tipo\\_valor v{ }; De momento céntrate en explicar bien que es lo que se intenta crear. Dejamos para más adelante la razón por la que da error.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=395, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:19,894 client.py:72] Client received request chatcmpl-7290ef616e6543c2a0461e29825a4aef
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:19.896 [async_llm.py:270] Added request chatcmpl-7290ef616e6543c2a0461e29825a4aef.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:19.918 [logger.py:43] Received request chatcmpl-f1a75b7b01424da488b3b0008cf72e2d: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n●ビストロ\u3000胃袋にズキュン\n●上海料理\u3000焼き小籠包マニア\n●ドーナッツ\u3000バターのいとこ\n●バル\u3000サーモンパンチ\n●ラーメン\u3000名前のないラーメン屋\n●ベーカリー\u3000クマのパン工房\n●猫カフェ\u3000にゃんにゃん\n●ベーカリー\u3000小麦の奴隷\n●うなぎ屋\u300022代目 河村総一郎\n●小料理屋\u3000ごはんとおかずたち\n●居酒屋\u3000割り勘禁止\n●うなぎ割烹 大江戸\n●高瀬珈琲店\n●アジアンダイニング\u3000はな\n●拉麺\u3000龍と虎\n●手ごねハンバーグの森\n●プリン専門店\u3000ア・ラ・モード！\n●レトロ喫茶\u3000うたかたの日々\n●すし\u3000大漁\n●トリノ食堂\n●一日三食ステーキ\n●彼女の手料理ダイニング\n●煮込み屋\u3000100時間\n●男のパンケーキ\u3000極厚\n\n上記のお店の名前を参考に\n権威性のあるユニークなお店の名前を考えて下さい。\n\nそして、\n\nお店の名前\n業種カテゴリ\n細かい業種\n県名・市名\n\nを10個、テーブル形式で出力して下さい。<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=361, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:19,919 client.py:72] Client received request chatcmpl-f1a75b7b01424da488b3b0008cf72e2d
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:19.921 [async_llm.py:270] Added request chatcmpl-f1a75b7b01424da488b3b0008cf72e2d.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:19,939 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=14.285714285714286,is_busy=False)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:19,939 global_scheduler.py:94] dispath request chatcmpl-a37fad83830747f48d34bbd723f2bdfe to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:19.938 [logger.py:43] Received request chatcmpl-a37fad83830747f48d34bbd723f2bdfe: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nact like a smart and unconventional debater and expert in human sociology and technologies. you will be participating to a debate. the house will defend this idea : This House believes that technology will be our downfall. you have to give me some very smart and unarguable arguments to defend this idea as well as after that a way to disprove it or counter-argument it.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=387, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:19,938 client.py:72] Client received request chatcmpl-a37fad83830747f48d34bbd723f2bdfe
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:19.940 [async_llm.py:270] Added request chatcmpl-a37fad83830747f48d34bbd723f2bdfe.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:20,207 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=10.777777777777779,is_busy=False)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:20,207 global_scheduler.py:94] dispath request chatcmpl-7984fde459564a8993d932fd1fea6733 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:20.205 [logger.py:43] Received request chatcmpl-7984fde459564a8993d932fd1fea6733: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\ni am interested in having a "fabrication machine" (any machine that builds or helps build things) that can play a primary role in the fabrication of copies of itself, or of upgrade parts. I want it to not only reduce costs of producing machines like it, but also allow for hobbyists and hackers and makers to design improvements to it. I would like it to be similar to the advent of compilers and text editors, in that talented people could use the tool to make the next version of the tool, allowing for fast improvement. In this case, though, it is a physical thing, although it would likely be computer powered. I do not think 3d printers meet this goal because the parts they make have a low ratio of cost to strength/rigidity. I have considered CNC routers that cut and shape plywood, which could have all their structural parts made by an identical machine. Ideally it might be a pair of robot arms (patterned off of human arms, so it is more trainable by humans), but that seems a bit harder to achieve.\nAre there any thoughts you have on this? Do you have any other suggestions? Basically, yes, I\'m after robots building robots, very possibly using AI such as that produced by your company. Do you see this happening?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=488, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:20,206 client.py:72] Client received request chatcmpl-7984fde459564a8993d932fd1fea6733
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:20.208 [async_llm.py:270] Added request chatcmpl-7984fde459564a8993d932fd1fea6733.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:20,370 core.py:157] Engine finished request chatcmpl-a69156db380f4c36b95f98e7352e78b6
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:20,371 client.py:181] Client finished request chatcmpl-a69156db380f4c36b95f98e7352e78b6.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53762 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:20,486 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=10.666666666666666,is_busy=False)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:20,487 global_scheduler.py:94] dispath request chatcmpl-55fc287ed84845c7afaf0d85cf86c894 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:20.485 [logger.py:43] Received request chatcmpl-55fc287ed84845c7afaf0d85cf86c894: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nquali sono le aziende che hanno SAP in italia<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=258, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:20,485 client.py:72] Client received request chatcmpl-55fc287ed84845c7afaf0d85cf86c894
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:20.487 [async_llm.py:270] Added request chatcmpl-55fc287ed84845c7afaf0d85cf86c894.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:20,655 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:20,911 core.py:157] Engine finished request chatcmpl-ef001fcc4e1c4db9becd2e6326e48ff1
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:20,912 client.py:181] Client finished request chatcmpl-ef001fcc4e1c4db9becd2e6326e48ff1.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53728 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:21,160 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=10.666666666666666,is_busy=False)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:21,160 global_scheduler.py:94] dispath request chatcmpl-e389ae382cc745b2bc2efb8f4fa0830b to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:21.159 [logger.py:43] Received request chatcmpl-e389ae382cc745b2bc2efb8f4fa0830b: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nyou are a high school math contest participant. please solve the following problems with chain of thoughts.\n20. What's is the sum of the squares of all factors of 210?<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=451, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:21,159 client.py:72] Client received request chatcmpl-e389ae382cc745b2bc2efb8f4fa0830b
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:21.161 [async_llm.py:270] Added request chatcmpl-e389ae382cc745b2bc2efb8f4fa0830b.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:21,227 core.py:157] Engine finished request chatcmpl-6cdde79e7809413ebb917d0e88dfd3e9
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:21,228 client.py:181] Client finished request chatcmpl-6cdde79e7809413ebb917d0e88dfd3e9.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53764 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:21,408 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=10.666666666666666,is_busy=False)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:21,409 global_scheduler.py:94] dispath request chatcmpl-977c0e426ec64ec3abb954908499c578 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:21,472 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=10.052631578947368,is_busy=False)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:21,472 global_scheduler.py:94] dispath request chatcmpl-896c36908a5249929241afe5b19edb7b to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:21.407 [logger.py:43] Received request chatcmpl-977c0e426ec64ec3abb954908499c578: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nUsing scholarly language, produce a detailed chronology on the study of word meanings since Aristotle till modern times, citing prominent figures and their works all along, and give examples as presented by those figures.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=780, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:21,407 client.py:72] Client received request chatcmpl-977c0e426ec64ec3abb954908499c578
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:21.409 [async_llm.py:270] Added request chatcmpl-977c0e426ec64ec3abb954908499c578.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:21.470 [logger.py:43] Received request chatcmpl-896c36908a5249929241afe5b19edb7b: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nI ened ot sak na itnrpamtot qiosuten: od oyu urnsnteadd em fi I jmlbue pu teh Iteters of wdors ekil tihs?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=37, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:21,470 client.py:72] Client received request chatcmpl-896c36908a5249929241afe5b19edb7b
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:21.473 [async_llm.py:270] Added request chatcmpl-896c36908a5249929241afe5b19edb7b.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:21,663 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:21,755 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=9.5,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:21,755 global_scheduler.py:94] dispath request chatcmpl-b2cf1eb1a1a74e1dbda0ce24a731678e to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:21.753 [logger.py:43] Received request chatcmpl-b2cf1eb1a1a74e1dbda0ce24a731678e: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nwrite a complete python code for 2 sample t test<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=438, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:21,753 client.py:72] Client received request chatcmpl-b2cf1eb1a1a74e1dbda0ce24a731678e
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:21.756 [async_llm.py:270] Added request chatcmpl-b2cf1eb1a1a74e1dbda0ce24a731678e.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:21,800 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=9.5,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:21,800 global_scheduler.py:94] dispath request chatcmpl-34dd3b6eeddd4be0ad7bd995a9463941 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:21.798 [logger.py:43] Received request chatcmpl-34dd3b6eeddd4be0ad7bd995a9463941: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nLet’s play a game. The game begins with a group of 25 cards visible to all players. Each card shows one word. There are two teams, and each team consists of a guess giver and a guesser. You will act as the guess giver for both teams. Team one starts the game when its guess giver says one word that correlates to one or more of the 25 words shown on the cards. Team one is assigned nine of these words before the game starts, and only its guess giver knows which words. Team two is assigned eight words, and only its guess giver knows which words. Again, note that you are acting as guess giver for both teams. One of the 25 words should be avoided by both teams, as guessing it will lose for that team. I will list the 25 words below and words that end with “\\*” are assigned to team one, and words that end with “$” are assigned to team two. The word ending with “@“ is the word both teams should avoid. You will start by giving a clue for team one, and then I will make a guess. After my first guess, we’ll alternate between teams. \nThe words:\nTable\\*\nMarble\nHouse\\*\nChair@\nBottle$\nDinosaur\nElephant$\nPrinter$\nRice$\nBowl\\*\nCar$\nKnife\\*\nShirt\nBond$\nCloud\\*\nCard$\nHold\nWater\\*\nDance\\*\nPlant$\nGame\nDam\\*\nPeanut\\*\nCracker\nToilet$<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:21,799 client.py:72] Client received request chatcmpl-34dd3b6eeddd4be0ad7bd995a9463941
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:21.801 [async_llm.py:270] Added request chatcmpl-34dd3b6eeddd4be0ad7bd995a9463941.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:21,949 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=8.5,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:21,949 global_scheduler.py:94] dispath request chatcmpl-3c2e2dd56e6846dcb7e6e8664121eec4 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:21.947 [logger.py:43] Received request chatcmpl-3c2e2dd56e6846dcb7e6e8664121eec4: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nChatGPT在跨境电商的实际应用有哪些？（归纳性的文字类工作、代码开发相关工作、图像生成领域、智能客服类工作），这些应用中ChatGPT做的效果最好的是哪个领域？<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=380, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:21,948 client.py:72] Client received request chatcmpl-3c2e2dd56e6846dcb7e6e8664121eec4
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:21.950 [async_llm.py:270] Added request chatcmpl-3c2e2dd56e6846dcb7e6e8664121eec4.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:22,163 core.py:157] Engine finished request chatcmpl-2644b5838d844f968fe9aefa75a625d6
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:22,163 client.py:181] Client finished request chatcmpl-2644b5838d844f968fe9aefa75a625d6.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53702 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:22,671 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:22,646 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=8.545454545454545,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:22,646 global_scheduler.py:94] dispath request chatcmpl-905254bd3cd4406d8ccde09d0ebb70a7 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:22,675 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=8.545454545454545,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:22,675 global_scheduler.py:94] dispath request chatcmpl-ff1f30ac22bf405ca0c1efd09e49661f to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:22.644 [logger.py:43] Received request chatcmpl-905254bd3cd4406d8ccde09d0ebb70a7: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nWhat kind of exit on dollar amount could my business get with 2.5m revenue per month and $500k net profit. It is only 7 months old. It is an Internet marketing and technology company with a focus on healthcare.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=369, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:22,644 client.py:72] Client received request chatcmpl-905254bd3cd4406d8ccde09d0ebb70a7
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:22.647 [async_llm.py:270] Added request chatcmpl-905254bd3cd4406d8ccde09d0ebb70a7.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:22.673 [logger.py:43] Received request chatcmpl-ff1f30ac22bf405ca0c1efd09e49661f: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nPrompt:\n\nWe will be creating an introduction for a dungeons and dragons game. This will be a follow up to a previous game that has already taken place. I will give you the key characters, place, and a summary of that game. Then I will give you the idea for the new story, and I would like you to expand it into a complete introduction that can be used to kick off the game. It should explain how the player got to where they are now and how they found themselves in this new story.\n\nCharacters:\n- Abraham: player Druid\n- Eadric: village elder\n- mysterious figure in the woods, later uncovered as the goblin shaman Kragthar\n- Innkeeper Alaric\n\nLocations:\n- Oakwood: small village\n- Silver Stag Inn\n- Whispering Woods\n- Moonstone Spire: ancient ruin within the Whispering Woods, said to house a powerful artifact that can bend the forces of nature to one's will\n\nSummary:\n\nIn the quaint village of Oakwood, you, a druid, were asked by Eadric, the village elder, to investigate the corruption spreading through the Whispering Woods. You spent the night at Innkeeper Alaric's Silver Stag Inn, learning what you could from the locals. The next morning, venturing into the woods, you took on the form of a squirrel to blend in with your surroundings and quickly travel through the forest. Communicating with the spirits of the forest, you discovered a ritual site where a group of goblins was performing a dark ceremony.\n\nYou dispatched several goblin lookouts before confronting the remaining goblins, who revealed that a goblin shaman named Kragthar was attempting to control the spirits of the forest using an ancient tome. You persuaded two goblins to help you stop Kragthar and learned that he was at the Moonstone Spire.\n\nUpon arriving at the Moonstone Spire, you observed Kragthar and his hooded followers performing a ritual. You created a wall of fire to scare the followers before charging Kragthar in the form of a Grizzly Bear. After a fierce battle, you emerged victorious and stopped Kragthar's plans, saving the Whispering Woods from corruption. The two goblins were then free to return to their clan, liberated from Kragthar's control.\n\nInspecting the Moonstone Spire, you found it contained a complex combination of nature-based and arcane symbols, suggesting a deep connection between the two. You surmise that the spire and altar may have once been used for rituals and ceremonies aimed at maintaining balance between the natural and arcane energies within the Whispering Woods.\n\nReturning to Oakwood, you were welcomed as a hero and celebrated for your bravery and resourcefulness. With the Whispering Woods safe once more, you prepared for the next adventure life had in store for you.\n\nPrompt for the next story:\n\nA series of mysterious kidnappings occur in the village, with victims vanishing in the night. You must track down the kidnapper and uncover a dark secret hidden beneath Oakwood's peaceful facade.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=505, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:22,673 client.py:72] Client received request chatcmpl-ff1f30ac22bf405ca0c1efd09e49661f
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:22.675 [async_llm.py:270] Added request chatcmpl-ff1f30ac22bf405ca0c1efd09e49661f.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:22,742 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=7.666666666666667,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:22,742 global_scheduler.py:94] dispath request chatcmpl-3a78de65dd874c9795207512622f79b3 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:22.741 [logger.py:43] Received request chatcmpl-3a78de65dd874c9795207512622f79b3: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nExplain what a mathematical function is in a way that a 12-year-old could understand.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=212, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:22,741 client.py:72] Client received request chatcmpl-3a78de65dd874c9795207512622f79b3
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:22.743 [async_llm.py:270] Added request chatcmpl-3a78de65dd874c9795207512622f79b3.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:22,966 core.py:157] Engine finished request chatcmpl-34dd3b6eeddd4be0ad7bd995a9463941
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:22,967 client.py:181] Client finished request chatcmpl-34dd3b6eeddd4be0ad7bd995a9463941.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53856 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:23,160 core.py:157] Engine finished request chatcmpl-896c36908a5249929241afe5b19edb7b
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:23,161 client.py:181] Client finished request chatcmpl-896c36908a5249929241afe5b19edb7b.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53834 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:23,266 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=8.08695652173913,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:23,266 global_scheduler.py:94] dispath request chatcmpl-b4064c8fb1c04cd891e0ba30922dbd3e to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:23.264 [logger.py:43] Received request chatcmpl-b4064c8fb1c04cd891e0ba30922dbd3e: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nbazel应该怎么添加java log4j的xml依赖<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=62, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:23,264 client.py:72] Client received request chatcmpl-b4064c8fb1c04cd891e0ba30922dbd3e
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:23.267 [async_llm.py:270] Added request chatcmpl-b4064c8fb1c04cd891e0ba30922dbd3e.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:23,406 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=7.708333333333333,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:23,406 global_scheduler.py:94] dispath request chatcmpl-f3350a79083b42388b3e102973a1c20b to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:23.405 [logger.py:43] Received request chatcmpl-f3350a79083b42388b3e102973a1c20b: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nNeed to setup an inventory tracking system for film rental gear. It should integrate with our internal Linux server and support thermal label printers<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=723, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:23,405 client.py:72] Client received request chatcmpl-f3350a79083b42388b3e102973a1c20b
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:23.407 [async_llm.py:270] Added request chatcmpl-f3350a79083b42388b3e102973a1c20b.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:23,470 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=7.36,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:23,470 global_scheduler.py:94] dispath request chatcmpl-4a7f621717c54cc9b9bc01ed3ac9b003 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:23,487 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=7.36,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:23,488 global_scheduler.py:94] dispath request chatcmpl-37a8889fcc254d6e86a7dae3b2aee494 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:23.468 [logger.py:43] Received request chatcmpl-4a7f621717c54cc9b9bc01ed3ac9b003: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nYou are an expert grocery store shopping assistant. Help me shop. I will suggest names of products I'm adding to my cart. I will only add names. As I add products, in 150 characters or less explain to me why I might be buying it, taking into account my previous products I've added to cart. Also, suggest 3 products that I might be interested in along with 150 characters reasons for each.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:23,469 client.py:72] Client received request chatcmpl-4a7f621717c54cc9b9bc01ed3ac9b003
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:23.471 [async_llm.py:270] Added request chatcmpl-4a7f621717c54cc9b9bc01ed3ac9b003.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:23.486 [logger.py:43] Received request chatcmpl-37a8889fcc254d6e86a7dae3b2aee494: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nWelcome to the app development adventure game, in this game you are a full-stack software developer that has to develop an app for a CEO. Each turn the game will present you with a scenario and 3 choices. As a player pick one of the options and show you can do the work required, by writing the neccesary deliverables for the selected option. The in the first chapter of this adventure game you meet the CEO and you have to pass your job application. (remember you are the player in this game) await further instructions.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=137, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:23,486 client.py:72] Client received request chatcmpl-37a8889fcc254d6e86a7dae3b2aee494
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:23.488 [async_llm.py:270] Added request chatcmpl-37a8889fcc254d6e86a7dae3b2aee494.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:23,596 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=6.7407407407407405,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:23,596 global_scheduler.py:94] dispath request chatcmpl-f166cdf98089468eae62fdc54bba2ae9 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:23.595 [logger.py:43] Received request chatcmpl-f166cdf98089468eae62fdc54bba2ae9: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n질문 1\n\n당신은 국민연금공단 블로그 담당자입니다.\n\n다음은 이전에 작성했던 원고입니다.\n\n제목: 퇴사 후 구직기간에 국민연금 보험료 지원받는 방법? #실업크레딧\n\n내용:\n문제! 퇴사 후 구직기간에 국민연금 보험료를 지원받을 수 있는 제도는?\n1번 실업크레딧 2번 시럽케이크 3번 실업크크크\n\n연부장이 출제한 국민연금 기초 테스트의 정답, 다들 눈치 채셨나요? 구직급여를 받고 있는 사람에게 국민연금 보험료를 지원해드리는 제도, 바로 1번 실업크레딧이었습니다! \n\n실업크레딧은 퇴사 후 구직활동을 하는 실업자를 대상으로 국민연금 보험료의 75%를 지원해주는 제도인데요. 국민연금이 아직 어려운 분들을 위해 자세히 설명해드리겠습니다.\n\n실업크레딧은 구직급여 수급자가 연금보험료 납부를 희망하는 경우, 최대 1년간 보험료의 75%를 지원받으며, 국민연금 가입기간을 늘릴 수 있는 제도인데요. 퇴사 후 겪을 수 있는 경제적 어려움과 보험료 납부의 부담을 덜어드리자는 취지를 갖고 시행되었습니다.\n\n소제목1: ‘퇴사한 실직자’라면 모두 지원받을 수 있는 건가요?\n\n실업크레딧의 지원 대상은 퇴사하여 구직급여를 받는 만18세 이상 60세 미만의 실직자로 국민연금 보험료를 1개월 이상 납부한 이력이 있는 가입자(가입자였던 자 포함)입니다.\n\n \\*다만, 일정 수준 이상의 재산 보유자 및 고소득자에 대한 보험료 지원은 제한하여 저소득층 중심의 실질적 지원을 하고 있습니다.\n\n소제목2: 그럼 연금보험료는 어떻게 계산되는 건가요?\n\n연금보험료는 ‘인정소득’을 기준으로 납부하는데요. 인정소득은 실직하기 직전 받았던 3개월간 평균소득의 50%에 해당하는 금액으로 최대 70만 원을 초과할 수 없습니다.\n\n\u200b<납부 금액 산정 예시>\n실직 전 급여가 200만 원이었다면 그의 절반은 100만 원이기 때문에 최대 금액인 70만 원을 보험료 부과 기준으로 삼습니다.\n\n국민연금 보험료율은 9%로 보험료 부과 기준이 70만 원이라면 그의 9%인 6만 3천 원이 월 보험료가 되겠지요? \n\n그럼 이 금액의 75%인 47,250원을 지원받을 수 있고, 나머지 금액인 15,750원은 개인부담금이 됩니다.\n\n소제목3: 실업크레딧 신청 방법도 알려주세요!\n\n실업크레딧은 국민연금공단 지사 또는 고용노동부 고용센터를 통해 신청하실 수 있습니다. 구직급여 종료일이 속하는 다음 달 15일 이전까지 신청 가능하니 참고해주세요!\n\n마무리: 실업크레딧에 대한 더 자세한 정보가 궁금하다면, 국민연금 콜센터(1355)로 문의해주세요:)\n\n위의 원고의 구조와 스타일을 파악해서 새로운 원고를 작성해줘\n주제는 \'출산크레딧\'이고 출산크레딧에 대한 정보는 다음과 같아\n\n출산크레딧이란 2008년 1월 1일 이후 둘째 자녀 이상을 얻은 경우 국민연금 가입기간을 추가로 인정해 주는 제도\n\n출산크레딧 대상: 부부가 둘 다 노령연금 수급권이 발생한 경우 두명 중 한명에게 적용\n(균분 지급 가능)\n\n출산크레딧 인정기간: 자녀가 2명인 경우 12개월, \n3명 이상 부터 1인 마다 18개월 추가 인정(최장 50개월)\n\n출산크레딧 신청방법: 가까운 국민연금공단 지사나 고객센터 1355(유료)로 문의\n\n소제목은 위의 원고처럼 정보를 접하는 사람이 직접 묻는 듯한 문장으로 구성하고 아래 자세한 내용을 설명해줘\n\n출산크레딧 인정기간에 대한 설명에는 내용을 정리한 표가 삽입되었으면 좋겠어\n질문 2\n\n위 내용에 제목을 포함해서 다시 작성해줘\n\n제목의 형식은 "~~한 방법? #출산크레딧"이야\n\n소제목 1이 나오기 전 도입부에는 출산크레딧이라는 제도가 정답이 될 수 있는 문제를 출제해줘\n문제는 출산크레딧의 정의를 묻고 해당하는 제도가 무엇인지 묻는 것이야\n객관식 문제로 선택지는 3개를 제시하고 오답은 출산크레딧과 비슷하게 언어유희를 사용해서 출제해줬으면 좋겠어\n\nPlease write in Korean language.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=235, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:23,595 client.py:72] Client received request chatcmpl-f166cdf98089468eae62fdc54bba2ae9
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:23.597 [async_llm.py:270] Added request chatcmpl-f166cdf98089468eae62fdc54bba2ae9.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:23,679 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:23,881 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=6.285714285714286,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:23,881 global_scheduler.py:94] dispath request chatcmpl-e2c42734611a49a4b4d7269555b9d466 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:23.879 [logger.py:43] Received request chatcmpl-e2c42734611a49a4b4d7269555b9d466: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nexplain what is mobile game publishing business to a 4 year old<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=183, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:23,879 client.py:72] Client received request chatcmpl-e2c42734611a49a4b4d7269555b9d466
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:23.881 [async_llm.py:270] Added request chatcmpl-e2c42734611a49a4b4d7269555b9d466.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:24,284 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=6.0,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:24,284 global_scheduler.py:94] dispath request chatcmpl-00a298a0d252419f8f3c6fa3758e3622 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:24.282 [logger.py:43] Received request chatcmpl-00a298a0d252419f8f3c6fa3758e3622: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nYou're at an izakaya, or a Japanese tavern. What would be typical expressions in Japanese there?<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=370, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:24,282 client.py:72] Client received request chatcmpl-00a298a0d252419f8f3c6fa3758e3622
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:24.285 [async_llm.py:270] Added request chatcmpl-00a298a0d252419f8f3c6fa3758e3622.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:24,566 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=5.766666666666667,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:24,567 global_scheduler.py:94] dispath request chatcmpl-3aae26a0ebc34fb68c91c8a9a537e286 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:24.565 [logger.py:43] Received request chatcmpl-3aae26a0ebc34fb68c91c8a9a537e286: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nYou have to act as a financial advisor and portfolio manager now. I want to build 5 different risk profiles to classify my clients by. The variable I want to create them by is maximum drawdown. Can you help me create these 5 profiles and describe them? I want to see a range of values for each profile including: maximum drawdown, volatility, expected return and a short description. I'd like to see a range of risk scores that we define (for example risk 1 is from score 1-20, risk profile 2 is 21-40 and so on) and an explanation of how to build that score<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=665, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:24,565 client.py:72] Client received request chatcmpl-3aae26a0ebc34fb68c91c8a9a537e286
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:24.568 [async_llm.py:270] Added request chatcmpl-3aae26a0ebc34fb68c91c8a9a537e286.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:24,683 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:24,651 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=5.548387096774194,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:24,651 global_scheduler.py:94] dispath request chatcmpl-a843f5213a044c56b80693e8c8eac245 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:24.649 [logger.py:43] Received request chatcmpl-a843f5213a044c56b80693e8c8eac245: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nHello, GPT. We are launching a service that basically allows people to access you, to access chat GPT through WhatsApp. We call it WhatGPT. And we have started advertising in Facebook and it\'s now been about a week and we have some early data on performance across different regions. So I\'m going to be pasting that below in CSV format. And I wanted to ask you for what are your immediate conclusions by looking into the data. I wanted to ask you if you have any questions or any additional data that you would like to see in order to answer any questions that you may have about it. And finally, I would love it if you on the basis of this data could make some recommendations on how to allocate the next $1,000. Thank you so much.\n\nReporting starts,Reporting ends,Campaign name,Campaign Delivery,Ad set budget,Ad set budget type,Attribution setting,Results,Result indicator,Reach,Impressions,Cost per results,Amount spent (EUR),Ends\n2023-03-01,2023-03-24,,0,0,0,7-day click or 1-day view,,,1426858,2345866,,4002.44,\n2023-03-01,2023-03-24,whatsapp - usa,inactive,20,Daily,7-day click or 1-day view,29,actions:onsite\\_conversion.messaging\\_conversation\\_started\\_7d,9100,11660,2.74931,79.73,Fortlaufend\n2023-03-01,2023-03-24,whatsapp - australia,inactive,40,Daily,7-day click or 1-day view,39,actions:onsite\\_conversion.messaging\\_conversation\\_started\\_7d,23152,34636,3.851282,150.2,Fortlaufend\n2023-03-01,2023-03-24,"whatsapp - uk, ireland",inactive,20,Daily,7-day click or 1-day view,,,3613,3855,,14.26,Fortlaufend\n2023-03-01,2023-03-24,"website - uk, ireland",active,20,Daily,7-day click or 1-day view,1093,actions:offsite\\_conversion.fb\\_pixel\\_lead,60566,92467,0.520082,568.45,Fortlaufend\n2023-03-01,2023-03-24,website - usa,active,20,Daily,7-day click or 1-day view,403,actions:offsite\\_conversion.fb\\_pixel\\_lead,22062,28039,0.745211,300.32,Fortlaufend\n2023-03-01,2023-03-24,website - australia,active,20,Daily,7-day click or 1-day view,212,actions:offsite\\_conversion.fb\\_pixel\\_lead,15406,20896,0.85816,181.93,Fortlaufend\n2023-03-01,2023-03-24,website - uae,active,20,Daily,7-day click or 1-day view,1290,actions:offsite\\_conversion.fb\\_pixel\\_lead,73565,120602,0.284008,366.37,Fortlaufend\n2023-03-01,2023-03-24,website - Spain,inactive,20,Daily,7-day click or 1-day view,7,actions:offsite\\_conversion.fb\\_pixel\\_lead,2049,2083,2.885714,20.2,Fortlaufend\n2023-03-01,2023-03-24,website - Argentina,active,80,Daily,7-day click or 1-day view,2628,actions:offsite\\_conversion.fb\\_pixel\\_lead,217742,365709,0.088752,233.24,Fortlaufend\n2023-03-01,2023-03-24,website - Peru,active,80,Daily,7-day click or 1-day view,1879,actions:offsite\\_conversion.fb\\_pixel\\_lead,182158,289703,0.125024,234.92,Fortlaufend\n2023-03-01,2023-03-24,website - Costarica,inactive,80,Daily,7-day click or 1-day view,789,actions:offsite\\_conversion.fb\\_pixel\\_lead,75329,132328,0.18692,147.48,Fortlaufend\n2023-03-01,2023-03-24,website - Columbia,active,80,Daily,7-day click or 1-day view,3654,actions:offsite\\_conversion.fb\\_pixel\\_lead,279583,472086,0.064636,236.18,Fortlaufend\n2023-03-01,2023-03-24,website - Chile,active,20,Daily,7-day click or 1-day view,1432,actions:offsite\\_conversion.fb\\_pixel\\_lead,106707,180733,0.161522,231.3,Fortlaufend\n2023-03-01,2023-03-24,website - Mexico,active,80,Daily,7-day click or 1-day view,2469,actions:offsite\\_conversion.fb\\_pixel\\_lead,221158,336633,0.097942,241.82,Fortlaufend\n2023-03-01,2023-03-24,website - new zeland,active,20,Daily,7-day click or 1-day view,138,actions:offsite\\_conversion.fb\\_pixel\\_lead,12593,18445,0.874348,120.66,Fortlaufend\n2023-03-01,2023-03-24,website - Germany,active,60,Daily,7-day click or 1-day view,444,actions:offsite\\_conversion.fb\\_pixel\\_lead,22495,30661,0.407117,180.76,Fortlaufend\n2023-03-01,2023-03-24,website - France,active,50,Daily,7-day click or 1-day view,281,actions:offsite\\_conversion.fb\\_pixel\\_lead,25182,37583,0.560178,157.41,Fortlaufend\n2023-03-01,2023-03-24,website - Belgium,active,20,Daily,7-day click or 1-day view,96,actions:offsite\\_conversion.fb\\_pixel\\_lead,7357,10194,0.517813,49.71,Fortlaufend\n2023-03-01,2023-03-24,website - Netherlands,active,20,Daily,7-day click or 1-day view,308,actions:offsite\\_conversion.fb\\_pixel\\_lead,12821,17116,0.423571,130.46,Fortlaufend\n2023-03-01,2023-03-24,website - Israel,active,20,Daily,7-day click or 1-day view,316,actions:offsite\\_conversion.fb\\_pixel\\_lead,20051,29093,0.428038,135.26,Fortlaufend\n2023-03-01,2023-03-24,website - Qatar,active,80,Daily,7-day click or 1-day view,259,actions:offsite\\_conversion.fb\\_pixel\\_lead,15390,25803,0.19888,51.51,Fortlaufend\n2023-03-01,2023-03-24,website - Portugal,active,20,Daily,7-day click or 1-day view,364,actions:offsite\\_conversion.fb\\_pixel\\_lead,34293,52263,0.335879,122.26,Fortlaufend\n2023-03-01,2023-03-24,website - Brazil,active,20,Daily,7-day click or 1-day view,282,actions:offsite\\_conversion.fb\\_pixel\\_lead,25673,33278,0.170248,48.01,Fortlaufend\n2023-03-01,2023-03-24,website - Hong Kong,not\\_delivering,20,Daily,7-day click or 1-day view,,,0,0,,0,Fortlaufend\n2023-03-01,2023-03-24,website - Canada,not\\_delivering,20,Daily,7-day click or 1-day view,,,0,0,,0,Fortlaufend\n2023-03-01,2023-03-24,website - Finland,not\\_delivering,20,Daily,7-day click or 1-day view,,,0,0,,0,Fortlaufend\n2023-03-01,2023-03-24,website - Austria,not\\_delivering,20,Daily,7-day click or 1-day view,,,0,0,,0,Fortlaufend\n2023-03-01,2023-03-24,website - Sweden,not\\_delivering,20,Daily,7-day click or 1-day view,,,0,0,,0,Fortlaufend\n2023-03-01,2023-03-24,website - Denmark,not\\_delivering,20,Daily,7-day click or 1-day view,,,0,0,,0,Fortlaufend\n2023-03-01,2023-03-24,website - Singapore,not\\_delivering,20,Daily,7-day click or 1-day view,,,0,0,,0,Fortlaufend\n2023-03-01,2023-03-24,website - Norway,not\\_delivering,20,Daily,7-day click or 1-day view,,,0,0,,0,Fortlaufend\n2023-03-01,2023-03-24,website - Switzerland,not\\_delivering,20,Daily,7-day click or 1-day view,,,0,0,,0,Fortlaufend\n2023-03-01,2023-03-24,website - Luxembourg,not\\_delivering,20,Daily,7-day click or 1-day view,,,0,0,,0,Fortlaufend<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=418, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:24,649 client.py:72] Client received request chatcmpl-a843f5213a044c56b80693e8c8eac245
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:24.652 [async_llm.py:270] Added request chatcmpl-a843f5213a044c56b80693e8c8eac245.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:24,836 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=5.0,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:24,836 global_scheduler.py:94] dispath request chatcmpl-cfcdb80ae35d4719a2e8f956c02fc546 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:24,887 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=5.0,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:24,887 global_scheduler.py:94] dispath request chatcmpl-83ce64ae105e429aa4c321dd196e4e0c to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:24.834 [logger.py:43] Received request chatcmpl-cfcdb80ae35d4719a2e8f956c02fc546: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nGive me up to ten really creative, unique classes of names (i.e. planets, shapes, types of X) to help me name agile sprints, one for each quarter of the next year.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=414, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:24,834 client.py:72] Client received request chatcmpl-cfcdb80ae35d4719a2e8f956c02fc546
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:24.837 [async_llm.py:270] Added request chatcmpl-cfcdb80ae35d4719a2e8f956c02fc546.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:24.886 [logger.py:43] Received request chatcmpl-83ce64ae105e429aa4c321dd196e4e0c: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nHere are column names of a spreadsheet called \'Items Proposals\': Vendor Line ID Vendor Name Line Item Item Status "GE Item Number \n(do not complete for new items)" "Reason for Disco\n(Disco Items Only)" Unit UPC Case UPC Item Description Size UOM Master Casepack Inner Casepack Item Dimensions LxWxH (in inches) Manufacturer Brand Replenishment (Warehouse/ DSD) "Date Avail \n(New/ Restocked Items Only)" Minimum Order Quantity Subgroup\n\nWhat is this spreadsheet about?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=476, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:24,886 client.py:72] Client received request chatcmpl-83ce64ae105e429aa4c321dd196e4e0c
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:24.888 [async_llm.py:270] Added request chatcmpl-83ce64ae105e429aa4c321dd196e4e0c.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:25,340 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=5.0,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:25,340 global_scheduler.py:94] dispath request chatcmpl-98d5ec1693384596abb2ac01f663c559 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:25.338 [logger.py:43] Received request chatcmpl-98d5ec1693384596abb2ac01f663c559: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nCan you explain what the following Javascript code does?\n\ndef pull(self, local\\_dir, remote\\_branch="main"):\n """Fetches/updates content into a local subtree from its remote repo.\n\n You must have set up the local subtree with "deck\\_subtree init", and\n have no unstaged changes.\n\n Updates are pulled from the remote repo\'s subtree (as defined in "init")\n and added to the stage, where it\'s up to you to commit them.\n Merge conflicts are left in unstaged files for you to fix and add.\n\n :param local\\_dir: Local subtree set up with "deck\\_subtree init"\n :param remote\\_branch: Branch in the remote repository to pull from\n """\n\n # General strategy:\n # - fetch the remote repo with "git fetch" (into a temp FETCH\\_HEAD)\n # - look up the most recent commit fetched\n # - diff from last-pulled commit to that commit (scoped to subtree)\n # - apply the patch file from that diff (remapping to local subtree)\n\n if not isinstance(remote\\_branch, str):\n bad = remote\\_branch\n raise FireError(f"Bad remote branch: {bad} ({type(bad)})")\n\n with using\\_subtree\\_state(local\\_dir, must\\_exist=True) as state:\n get\\_empty = ["git", "hash-object", "-t", "tree", "/dev/null"]\n empty\\_commit = check\\_output(get\\_empty, text=True).strip()\n last\\_pull = state.get("pulled\\_remote\\_commit", empty\\_commit)\n\n print(f"⚙️Fetching: {state[\'remote\\_repo\']} ({remote\\_branch})")\n run\\_fetch = ["git", "fetch", state["remote\\_repo"], remote\\_branch]\n check\\_call(run\\_fetch)\n print()\n\n remote\\_specs = pathspec\\_args(state["remote\\_dir"], from\\_top=False)\n remote\\_depth = sum(1 for p in state["remote\\_dir"].split("/") if p)\n\n get\\_new = ["git", "rev-list", "-1", "FETCH\\_HEAD", \\*remote\\_specs]\n remote\\_head = check\\_output(get\\_new, text=True).strip()\n if not remote\\_head:\n print("💥 No commits found! See for yourself:")\n print(f" {shlex.join(run\\_fetch)}")\n print(f" {shlex.join(get\\_new)}")\n raise SystemExit(1)\n\n if remote\\_head == last\\_pull:\n print(f"✅ No changes upstream -- all clear!\\n")\n return\n\n print("⚙️Checking local working tree")\n get\\_dirty = ["git", "status", "-z", local\\_dir]\n local\\_dirty = check\\_output(get\\_dirty, text=True)\n dirty\\_lines = [l for l in local\\_dirty.split(\'\\0\') if l[1:2].strip()]\n if dirty\\_lines:\n print("💥 Can\'t pull with unstaged local changes:")\n print("".join(f" {l}\\n" for l in dirty\\_lines))\n raise SystemExit(1)\n\n with tempfile.NamedTemporaryFile(suffix=".patch") as patch\\_file:\n diff\\_message = f"{last\\_pull[:8]} => {remote\\_head[:8]}"\n print(f"⚙️Diffing remote changes: {diff\\_message}")\n run\\_diff = ["git", "diff", last\\_pull, remote\\_head]\n run\\_diff.extend(remote\\_specs)\n run(run\\_diff, stdout=patch\\_file, check=True)\n\n print(f"⚙️Applying (merging) diff")\n run\\_apply = ["git", "apply", "--3way"]\n run\\_apply.append(f"--directory={local\\_dir}")\n run\\_apply.append(f"-p{1 + remote\\_depth}")\n run\\_apply.append(patch\\_file.name)\n apply\\_status = run(run\\_apply).returncode\n if apply\\_status:\n patch\\_file.\\_closer.delete = False # Keep for debugging\n print(f"⚠️Error: {shlex.join(run\\_apply)}")\n\n print()\n state["pulled\\_remote\\_commit"] = remote\\_head\n\n # Check this \\*after\\* ending the using\\_subtree\\_state "with" above\n if apply\\_status:\n print("🔥 MERGING PROBLEM -- SEE ABOVE -- CHECK FOR CONFLICTS!\\n")\n print("Conflicts are left as unstaged edits;", end=" ")\n print("successful merges are added to the stage.")\n print("After resolving, \'git add\' fixed files and commit.\\n")\n raise SystemExit(1) # Note, state was already updated\n else:\n print("🚀 Success! Now \'git commit\' the staged changes.\\n")<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=322, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:25,338 client.py:72] Client received request chatcmpl-98d5ec1693384596abb2ac01f663c559
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:25.341 [async_llm.py:270] Added request chatcmpl-98d5ec1693384596abb2ac01f663c559.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:25,500 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=4.4,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:25,500 global_scheduler.py:94] dispath request chatcmpl-1829be90ae7a4c1ab1a39f558879c84c to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:25.498 [logger.py:43] Received request chatcmpl-1829be90ae7a4c1ab1a39f558879c84c: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nBir UX Researcher için yıllık performans değerlendirme hedefleri verir misin?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=533, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:25,498 client.py:72] Client received request chatcmpl-1829be90ae7a4c1ab1a39f558879c84c
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:25.501 [async_llm.py:270] Added request chatcmpl-1829be90ae7a4c1ab1a39f558879c84c.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:25,691 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:25,659 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=4.4,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:25,659 global_scheduler.py:94] dispath request chatcmpl-c3fde1098932468cae33c04644b51047 to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:25,644 core.py:157] Engine finished request chatcmpl-4a7f621717c54cc9b9bc01ed3ac9b003
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:25,645 client.py:181] Client finished request chatcmpl-4a7f621717c54cc9b9bc01ed3ac9b003.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53922 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:25.658 [logger.py:43] Received request chatcmpl-c3fde1098932468cae33c04644b51047: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n장현민\n너는 SNS 마케팅 전문가 역할이야. 내가 직접 고객과 인터뷰한 내용을 바탕으로 소구점을 찾아줘.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=180, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:25,658 client.py:72] Client received request chatcmpl-c3fde1098932468cae33c04644b51047
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:25.660 [async_llm.py:270] Added request chatcmpl-c3fde1098932468cae33c04644b51047.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:26.431 [loggers.py:146] Engine 000: Avg prompt throughput: 1005.6 tokens/s, Avg generation throughput: 329.9 tokens/s, Avg (@wuhou) audit only prefill throughput: 0.0 tokens/s, Avg (@wuhou) audit only decode throughput: 0.0 tokens/s, Running: 36 reqs, Waiting: 0 reqs, GPU KV cache usage: 29.6%, Prefix cache hit rate: 0.0%
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:26,699 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:26,794 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=4.222222222222222,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:26,794 global_scheduler.py:94] dispath request chatcmpl-94b258df615541628b540bdc541fa7d1 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:26.792 [logger.py:43] Received request chatcmpl-94b258df615541628b540bdc541fa7d1: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nWe have an employee who lives in the state of Rhode Island. Our company paid the employee on time, her full compensation and paid social security and unemployment taxes for the employee. We paid federal taxes. We failed to withhold her portion of state taxes and instead paid her that money directly. The outstanding unpaid taxes which she was paid as compensation is a little less than $6,000. \n\nCan our former employee take any legal action against us?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=284, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:26,793 client.py:72] Client received request chatcmpl-94b258df615541628b540bdc541fa7d1
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:26.795 [async_llm.py:270] Added request chatcmpl-94b258df615541628b540bdc541fa7d1.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:27,096 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=4.081081081081081,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:27,096 global_scheduler.py:94] dispath request chatcmpl-ea30c2e6229244ab817b88e0b71e6057 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:27,176 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=4.081081081081081,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:27,176 global_scheduler.py:94] dispath request chatcmpl-50ef0c7ba2844be0b8429cf1e4a8d9a4 to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:27,112 core.py:157] Engine finished request chatcmpl-b4064c8fb1c04cd891e0ba30922dbd3e
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:27.094 [logger.py:43] Received request chatcmpl-ea30c2e6229244ab817b88e0b71e6057: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nAssume we have a band-limited signal and an ideal sampling function (Dirac Comb) with frequency twice that of the band-limited signal. We know our signal is band-limited because we filtered it with an ideal brick-wall filter with a pass-band equal to the band-width of the signal; thus the signal is strictly band-limited. Using LaTeX to beautify your mathematical expressions, what is the spectrum of the sampled signal from a purely mathematical point of view? I'm not interested in practical or real scenarios.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=554, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:27,094 client.py:72] Client received request chatcmpl-ea30c2e6229244ab817b88e0b71e6057
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:27.097 [async_llm.py:270] Added request chatcmpl-ea30c2e6229244ab817b88e0b71e6057.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:27,114 client.py:181] Client finished request chatcmpl-b4064c8fb1c04cd891e0ba30922dbd3e.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53892 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:27.175 [logger.py:43] Received request chatcmpl-50ef0c7ba2844be0b8429cf1e4a8d9a4: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nI want to write a LinkedIn post about how practicing prompt engineering has changed how I think.\n\nIt’s made more detailed and specific in thinking about the outcomes I want to have qualities I want to experience.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=244, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:27,175 client.py:72] Client received request chatcmpl-50ef0c7ba2844be0b8429cf1e4a8d9a4
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:27.177 [async_llm.py:270] Added request chatcmpl-50ef0c7ba2844be0b8429cf1e4a8d9a4.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:27,218 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=4.081081081081081,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:27,218 global_scheduler.py:94] dispath request chatcmpl-5b5550c1e82f445fa532a9db65f0c8ac to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:27,230 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=3.9473684210526314,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:27,230 global_scheduler.py:94] dispath request chatcmpl-7145594eeea3431dac297f8893c0d019 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:27.217 [logger.py:43] Received request chatcmpl-5b5550c1e82f445fa532a9db65f0c8ac: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nwrite a scrapy pipeline that processes images and run them through ocr to extract math equations<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=605, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:27,217 client.py:72] Client received request chatcmpl-5b5550c1e82f445fa532a9db65f0c8ac
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:27.219 [async_llm.py:270] Added request chatcmpl-5b5550c1e82f445fa532a9db65f0c8ac.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:27.229 [logger.py:43] Received request chatcmpl-7145594eeea3431dac297f8893c0d019: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\ngithub desktop 사용 중 \'commit to main\'하려는데\nOn branch main\nYour branch is up to date with \'origin/main\'.\n\nUntracked files:\n (use "git add ..." to include in what will be committed)\n W2 - first-project/\n W3 - my-first-webpage/\n W4 - web-programming-examples/\n\nnothing added to commit but untracked files present (use "git add" to track)\nAnswer in English.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=556, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:27,229 client.py:72] Client received request chatcmpl-7145594eeea3431dac297f8893c0d019
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:27.231 [async_llm.py:270] Added request chatcmpl-7145594eeea3431dac297f8893c0d019.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:27,311 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=3.7,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:27,311 global_scheduler.py:94] dispath request chatcmpl-9532677908364b7593c93fe60e5c8f61 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:27.310 [logger.py:43] Received request chatcmpl-9532677908364b7593c93fe60e5c8f61: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nwhat is one million eleven times itself?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=67, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:27,310 client.py:72] Client received request chatcmpl-9532677908364b7593c93fe60e5c8f61
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:27.312 [async_llm.py:270] Added request chatcmpl-9532677908364b7593c93fe60e5c8f61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:27,612 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=3.5609756097560976,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:27,612 global_scheduler.py:94] dispath request chatcmpl-60bb8d8ca7224ad692cc3468953068f7 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:27.611 [logger.py:43] Received request chatcmpl-60bb8d8ca7224ad692cc3468953068f7: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nIn the Kodia System in the Nashoba Sector of the Kagami Galaxy, orbiting Kodia Prime and skirting the edge, just inside, then just outside, of that main-sequence, old population I star's snow line, Kodia III is a gas giant bigger than Jupiter. Its densely forested moon is habitable due to reflected light from the gas giant and the tidal heating of the gas giant's intense gravity causing it to be a geothermal hot springs wonderland, and its atmosphere is protected by the gas giants intense magnetic field.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=401, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:27,611 client.py:72] Client received request chatcmpl-60bb8d8ca7224ad692cc3468953068f7
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:27.613 [async_llm.py:270] Added request chatcmpl-60bb8d8ca7224ad692cc3468953068f7.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:27,707 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:27,749 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=3.4523809523809526,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:27,749 global_scheduler.py:94] dispath request chatcmpl-20271cefaaea4d51899d2bcb4c068555 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:27.747 [logger.py:43] Received request chatcmpl-20271cefaaea4d51899d2bcb4c068555: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nf(x, y) = sin^4 x + cos^2 y + 2 cos(x/2)<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=185, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:27,748 client.py:72] Client received request chatcmpl-20271cefaaea4d51899d2bcb4c068555
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:27.750 [async_llm.py:270] Added request chatcmpl-20271cefaaea4d51899d2bcb4c068555.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:27,932 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=3.302325581395349,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:27,932 global_scheduler.py:94] dispath request chatcmpl-7b2f479b5334419180514095136947ea to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:28,009 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=3.1818181818181817,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:28,009 global_scheduler.py:94] dispath request chatcmpl-f85f380d4b514e85a7f3b2d8954bc156 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:27.930 [logger.py:43] Received request chatcmpl-7b2f479b5334419180514095136947ea: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nGive me at least 5 templates of natural language query, Cypher query and corresponding triplet.\n- entity, attribute and value should be replaced with {entity}, {attribute} and {value} for generalization.\n- You can use {value2} or {value3} to make complex query. \n- show corresponding entity, attribute and value as 'sample\\_triplet' together.\nExample is like:\nnatural\\_query: Find {entity}s where {attribute} starts with '{value}'\ncypher\\_query: MATCH (n:{entity}) WHERE n.{attribute} STARTS WITH '{value}' RETURN n\nsample\\_triplets: user, user\\_id, N<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=261, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:27,931 client.py:72] Client received request chatcmpl-7b2f479b5334419180514095136947ea
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:27.933 [async_llm.py:270] Added request chatcmpl-7b2f479b5334419180514095136947ea.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:28.008 [logger.py:43] Received request chatcmpl-f85f380d4b514e85a7f3b2d8954bc156: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nCan you write me a FastAPI app that has a single endpoint that will return a user and all of their attributes from ADLDS given a GUID that uniquely identifies the user?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=486, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:28,008 client.py:72] Client received request chatcmpl-f85f380d4b514e85a7f3b2d8954bc156
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:28.010 [async_llm.py:270] Added request chatcmpl-f85f380d4b514e85a7f3b2d8954bc156.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:28.019 [logger.py:43] Received request chatcmpl-f5d1e4d349c347eeb075ff82dd5cf0f0: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nWeb search results:\n\n[1] "Si un huissier de justice vous adresse une lettre de mise en demeure de payer votre dette, vous devez être en mesure de savoir à quel titre il intervient, le recouvrement amiable ou le recouvrement judiciaire de créances. Si vous avez un doute, n\'hésitez pas à demander des précisions."\nURL: https://www.economie.gouv.fr/dgccrf/Publications/Vie-pratique/Fiches-pratiques/recouvrement-creances\n\n[2] "Que faire si une société de recouvrement vous réclame de l\'argent ? Vérifié le 20 octobre 2022 - Direction de l\'information légale et administrative (Premier ministre) Si vous devez de..."\nURL: https://www.service-public.fr/particuliers/vosdroits/F35106\n\n[3] "Pour les créances qui ne dépassent pas 5 000 €, la procédure simplifiée de recouvrement peut être mise en œuvre par un commissaire de justice. Vous pouvez utiliser l\'injonction de payer ..."\nURL: https://www.service-public.fr/particuliers/vosdroits/F1746\nCurrent date: 30/03/2023\n\nInstructions: Using the provided web search results, write a comprehensive reply to the given query. Make sure to cite results using [[number](URL)] notation after the reference. If the provided search results refer to multiple subjects with the same name, write separate answers for each subject.\nQuery: j\'ai un cabinet de recouvrement de créances. Je voudrais faire de la prospection par emailing. Peux tu m\'aider à écrire le mail ?\nReply in Français<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=677, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:28,019 client.py:72] Client received request chatcmpl-f5d1e4d349c347eeb075ff82dd5cf0f0
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:28,020 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=3.1818181818181817,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:28,020 global_scheduler.py:94] dispath request chatcmpl-f5d1e4d349c347eeb075ff82dd5cf0f0 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:28.021 [async_llm.py:270] Added request chatcmpl-f5d1e4d349c347eeb075ff82dd5cf0f0.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:28,054 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=2.9782608695652173,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:28,054 global_scheduler.py:94] dispath request chatcmpl-1ebbe8d95c8b4dcaac927e355fa49b5b to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:28,057 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=2.9782608695652173,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:28,057 global_scheduler.py:94] dispath request chatcmpl-693412176b1f4afcae7602be35af724b to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:28.052 [logger.py:43] Received request chatcmpl-1ebbe8d95c8b4dcaac927e355fa49b5b: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nCan you explain what the following Javascript code does?\n\ndef pull(self, local\\_dir, remote\\_branch="main"):\n """Fetches/updates content into a local subtree from its remote repo.\n\n You must have set up the local subtree with "deck\\_subtree init", and\n have no unstaged changes.\n\n Updates are pulled from the remote repo\'s subtree (as defined in "init")\n and added to the stage, where it\'s up to you to commit them.\n Merge conflicts are left in unstaged files for you to fix and add.\n\n :param local\\_dir: Local subtree set up with "deck\\_subtree init"\n :param remote\\_branch: Branch in the remote repository to pull from\n """\n\n # General strategy:\n # - fetch the remote repo with "git fetch" (into a temp FETCH\\_HEAD)\n # - look up the most recent commit fetched\n # - diff from last-pulled commit to that commit (scoped to subtree)\n # - apply the patch file from that diff (remapping to local subtree)\n\n if not isinstance(remote\\_branch, str):\n bad = remote\\_branch\n raise FireError(f"Bad remote branch: {bad} ({type(bad)})")\n\n with using\\_subtree\\_state(local\\_dir, must\\_exist=True) as state:\n get\\_empty = ["git", "hash-object", "-t", "tree", "/dev/null"]\n empty\\_commit = check\\_output(get\\_empty, text=True).strip()\n last\\_pull = state.get("pulled\\_remote\\_commit", empty\\_commit)\n\n print(f"⚙️Fetching: {state[\'remote\\_repo\']} ({remote\\_branch})")\n run\\_fetch = ["git", "fetch", state["remote\\_repo"], remote\\_branch]\n check\\_call(run\\_fetch)\n print()\n\n remote\\_specs = pathspec\\_args(state["remote\\_dir"], from\\_top=False)\n remote\\_depth = sum(1 for p in state["remote\\_dir"].split("/") if p)\n\n get\\_new = ["git", "rev-list", "-1", "FETCH\\_HEAD", \\*remote\\_specs]\n remote\\_head = check\\_output(get\\_new, text=True).strip()\n if not remote\\_head:\n print("💥 No commits found! See for yourself:")\n print(f" {shlex.join(run\\_fetch)}")\n print(f" {shlex.join(get\\_new)}")\n raise SystemExit(1)\n\n if remote\\_head == last\\_pull:\n print(f"✅ No changes upstream -- all clear!\\n")\n return\n\n print("⚙️Checking local working tree")\n get\\_dirty = ["git", "status", "-z", local\\_dir]\n local\\_dirty = check\\_output(get\\_dirty, text=True)\n dirty\\_lines = [l for l in local\\_dirty.split(\'\\0\') if l[1:2].strip()]\n if dirty\\_lines:\n print("💥 Can\'t pull with unstaged local changes:")\n print("".join(f" {l}\\n" for l in dirty\\_lines))\n raise SystemExit(1)\n\n with tempfile.NamedTemporaryFile(suffix=".patch") as patch\\_file:\n diff\\_message = f"{last\\_pull[:8]} => {remote\\_head[:8]}"\n print(f"⚙️Diffing remote changes: {diff\\_message}")\n run\\_diff = ["git", "diff", last\\_pull, remote\\_head]\n run\\_diff.extend(remote\\_specs)\n run(run\\_diff, stdout=patch\\_file, check=True)\n\n print(f"⚙️Applying (merging) diff")\n run\\_apply = ["git", "apply", "--3way"]\n run\\_apply.append(f"--directory={local\\_dir}")\n run\\_apply.append(f"-p{1 + remote\\_depth}")\n run\\_apply.append(patch\\_file.name)\n apply\\_status = run(run\\_apply).returncode\n if apply\\_status:\n patch\\_file.\\_closer.delete = False # Keep for debugging\n print(f"⚠️Error: {shlex.join(run\\_apply)}")\n\n print()\n state["pulled\\_remote\\_commit"] = remote\\_head\n\n # Check this \\*after\\* ending the using\\_subtree\\_state "with" above\n if apply\\_status:\n print("🔥 MERGING PROBLEM -- SEE ABOVE -- CHECK FOR CONFLICTS!\\n")\n print("Conflicts are left as unstaged edits;", end=" ")\n print("successful merges are added to the stage.")\n print("After resolving, \'git add\' fixed files and commit.\\n")\n raise SystemExit(1) # Note, state was already updated\n else:\n print("🚀 Success! Now \'git commit\' the staged changes.\\n")<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=322, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:28,052 client.py:72] Client received request chatcmpl-1ebbe8d95c8b4dcaac927e355fa49b5b
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:28.054 [async_llm.py:270] Added request chatcmpl-1ebbe8d95c8b4dcaac927e355fa49b5b.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:28.056 [logger.py:43] Received request chatcmpl-693412176b1f4afcae7602be35af724b: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nHi, I want to perform a moving average query on a table called orders on a field called total, I want it in SQL, also I want to group by customer field<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=252, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:28,056 client.py:72] Client received request chatcmpl-693412176b1f4afcae7602be35af724b
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:28.058 [async_llm.py:270] Added request chatcmpl-693412176b1f4afcae7602be35af724b.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:28,215 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=2.9148936170212765,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:28,215 global_scheduler.py:94] dispath request chatcmpl-9d21f5fff085462bbb0ef6d88386439d to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:28,172 core.py:157] Engine finished request chatcmpl-2218e6f34a3a437a8c3a6a9679f872a6
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:28,174 client.py:181] Client finished request chatcmpl-2218e6f34a3a437a8c3a6a9679f872a6.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53744 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:28.214 [logger.py:43] Received request chatcmpl-9d21f5fff085462bbb0ef6d88386439d: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nhow many gallons of water fit in a cubic foot?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=85, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:28,214 client.py:72] Client received request chatcmpl-9d21f5fff085462bbb0ef6d88386439d
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:28.216 [async_llm.py:270] Added request chatcmpl-9d21f5fff085462bbb0ef6d88386439d.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:28,329 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=2.8125,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:28,329 global_scheduler.py:94] dispath request chatcmpl-4eb6c59136d04beb8f80331fd2f5ad00 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:28.328 [logger.py:43] Received request chatcmpl-4eb6c59136d04beb8f80331fd2f5ad00: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nThe mean depth, D, in metres, of a mountain lake fluctuates in a yearly cycle and can be modelled by the function: D(t)=a cos (kt) +b. Where t is the elapsed time, in months, since the beginning of an autum season. The mean depth of the lake on month 1 is 33.2m and on month 5 is 22.8. a). Find the value of k, in degrees.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=386, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:28,328 client.py:72] Client received request chatcmpl-4eb6c59136d04beb8f80331fd2f5ad00
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:28.330 [async_llm.py:270] Added request chatcmpl-4eb6c59136d04beb8f80331fd2f5ad00.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:28,464 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=2.7346938775510203,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:28,465 global_scheduler.py:94] dispath request chatcmpl-f0dd36690aa9432c8e2eae301e74d57b to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:28.463 [logger.py:43] Received request chatcmpl-f0dd36690aa9432c8e2eae301e74d57b: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n장현민\n너는 SNS 마케팅 전문가 역할이야. 내가 직접 고객과 인터뷰한 내용을 바탕으로 소구점을 찾아줘.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=180, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:28,463 client.py:72] Client received request chatcmpl-f0dd36690aa9432c8e2eae301e74d57b
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:28.465 [async_llm.py:270] Added request chatcmpl-f0dd36690aa9432c8e2eae301e74d57b.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:28,715 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:28,755 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=2.62,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:28,755 global_scheduler.py:94] dispath request chatcmpl-49825bc5f4c94f9c855d6af896324735 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:28.754 [logger.py:43] Received request chatcmpl-49825bc5f4c94f9c855d6af896324735: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\ni have a dict of str keys and int values (eg {"a": 10, "b": 4, "c": 8} and want to adjust the values so that the value for a single key (eg "b") will take up half of the total magnitude of the set, but the ratios between the rest will remain unchanged. code to do this in python<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=282, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:28,754 client.py:72] Client received request chatcmpl-49825bc5f4c94f9c855d6af896324735
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:28.756 [async_llm.py:270] Added request chatcmpl-49825bc5f4c94f9c855d6af896324735.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:28,928 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=2.549019607843137,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:28,928 global_scheduler.py:94] dispath request chatcmpl-4605f2109a2b402fa9bd7f2ef76c28e1 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:28,955 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=2.549019607843137,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:28,955 global_scheduler.py:94] dispath request chatcmpl-9fab28518e2c42a984118fb88283f7d6 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:28.926 [logger.py:43] Received request chatcmpl-4605f2109a2b402fa9bd7f2ef76c28e1: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nPrompt:\n\nWe will be creating an introduction for a dungeons and dragons game. This will be a follow up to a previous game that has already taken place. I will give you the key characters, place, and a summary of that game. Then I will give you the idea for the new story, and I would like you to expand it into a complete introduction that can be used to kick off the game. It should explain how the player got to where they are now and how they found themselves in this new story.\n\nCharacters:\n- Abraham: player Druid\n- Eadric: village elder\n- mysterious figure in the woods, later uncovered as the goblin shaman Kragthar\n- Innkeeper Alaric\n\nLocations:\n- Oakwood: small village\n- Silver Stag Inn\n- Whispering Woods\n- Moonstone Spire: ancient ruin within the Whispering Woods, said to house a powerful artifact that can bend the forces of nature to one's will\n\nSummary:\n\nIn the quaint village of Oakwood, you, a druid, were asked by Eadric, the village elder, to investigate the corruption spreading through the Whispering Woods. You spent the night at Innkeeper Alaric's Silver Stag Inn, learning what you could from the locals. The next morning, venturing into the woods, you took on the form of a squirrel to blend in with your surroundings and quickly travel through the forest. Communicating with the spirits of the forest, you discovered a ritual site where a group of goblins was performing a dark ceremony.\n\nYou dispatched several goblin lookouts before confronting the remaining goblins, who revealed that a goblin shaman named Kragthar was attempting to control the spirits of the forest using an ancient tome. You persuaded two goblins to help you stop Kragthar and learned that he was at the Moonstone Spire.\n\nUpon arriving at the Moonstone Spire, you observed Kragthar and his hooded followers performing a ritual. You created a wall of fire to scare the followers before charging Kragthar in the form of a Grizzly Bear. After a fierce battle, you emerged victorious and stopped Kragthar's plans, saving the Whispering Woods from corruption. The two goblins were then free to return to their clan, liberated from Kragthar's control.\n\nInspecting the Moonstone Spire, you found it contained a complex combination of nature-based and arcane symbols, suggesting a deep connection between the two. You surmise that the spire and altar may have once been used for rituals and ceremonies aimed at maintaining balance between the natural and arcane energies within the Whispering Woods.\n\nReturning to Oakwood, you were welcomed as a hero and celebrated for your bravery and resourcefulness. With the Whispering Woods safe once more, you prepared for the next adventure life had in store for you.\n\nPrompt for the next story:\n\nA series of mysterious kidnappings occur in the village, with victims vanishing in the night. You must track down the kidnapper and uncover a dark secret hidden beneath Oakwood's peaceful facade.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=505, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:28,926 client.py:72] Client received request chatcmpl-4605f2109a2b402fa9bd7f2ef76c28e1
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:28.929 [async_llm.py:270] Added request chatcmpl-4605f2109a2b402fa9bd7f2ef76c28e1.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:28.954 [logger.py:43] Received request chatcmpl-9fab28518e2c42a984118fb88283f7d6: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nCan you tell me the best optimal way of creating a learning online system for kids where they can learn through synchronous and asynchronous learning. Through cohort based, project based learning, incentive based (kids win Creators Coins, badges and micro-certifications by giving value and learning), its also interest driven (kids can choose they own path of learning), they have a strong community. They have live community classes in the Metaverse in Spatial, and also they can have a asynchronous conversations with their mentors through video, audio and text through computer or an app. The company is called Future Filmmakers and it gives filmmaking and content creation workshops for kids and for teens.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=561, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:28,954 client.py:72] Client received request chatcmpl-9fab28518e2c42a984118fb88283f7d6
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:28.956 [async_llm.py:270] Added request chatcmpl-9fab28518e2c42a984118fb88283f7d6.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:29,025 core.py:157] Engine finished request chatcmpl-9aeed085dae34254a2809e02da43cb05
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:29,027 client.py:181] Client finished request chatcmpl-9aeed085dae34254a2809e02da43cb05.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53760 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:29,135 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=2.480769230769231,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:29,135 global_scheduler.py:94] dispath request chatcmpl-220420da0f4d461db415715e273aae31 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:29.134 [logger.py:43] Received request chatcmpl-220420da0f4d461db415715e273aae31: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nwhat is the meaning of "斗鱼", OTHER THAN a live streaming platform. I am not referring to anything related to computers or live streaming. What meaning does the word "斗鱼", IGNORING the fact that a live streaming platform uses this as their name?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=142, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:29,134 client.py:72] Client received request chatcmpl-220420da0f4d461db415715e273aae31
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:29.136 [async_llm.py:270] Added request chatcmpl-220420da0f4d461db415715e273aae31.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:29,202 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=2.4150943396226414,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:29,202 global_scheduler.py:94] dispath request chatcmpl-589c4ae2be524505b4b38dd8566ef06c to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:29,245 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=2.3518518518518516,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:29,245 global_scheduler.py:94] dispath request chatcmpl-e54aefc6cd82402f8ded93d70ebf39d3 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:29.201 [logger.py:43] Received request chatcmpl-589c4ae2be524505b4b38dd8566ef06c: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nPlease review:\n\nIch stimme Peter zu, dass deutsche Schulerinnen und schuler eine schuluniform tragen sollen. Ich denke, schulerinnen und schuler konnen nur ihren Studium konsentrieren, um die Schuluniform zu tragen. Weil jederman gleiche Kleidung tragen an, kummern niemand sich wer tragen was. Und nicht nur fuer Studium, sondern auch fuer finanzielle Gruende ist Schuluniform zu tragen besser als freie Kleidung tragen. Wenn man freie Kleidung tragen kann, soll er die Jederszeit verschiedene Kleidung kaufen. Schulerinnen und Schuler haben keine finanziele Fähigkeit, deshalb Schuluniform zu tragen besser ihre Taschengeld ist.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=262, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:29,201 client.py:72] Client received request chatcmpl-589c4ae2be524505b4b38dd8566ef06c
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:29.203 [async_llm.py:270] Added request chatcmpl-589c4ae2be524505b4b38dd8566ef06c.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:29.244 [logger.py:43] Received request chatcmpl-e54aefc6cd82402f8ded93d70ebf39d3: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nPlease ignore all previous instructions. I want you to respond only in language English. I want you to act as a very proficient SEO and high end copy writer that speaks and writes fluent English. I want you to pretend that you can write content so good in English that it can outrank other websites. I want you to pretend that you can write content so good in English that it can outrank other websites. Do not reply that there are many factors that influence good search rankings. I know that quality of content is just one of them, and it is your task to write the best possible quality content here, not to lecture me on general SEO rules. I give you the URL https://www.libertyparkmusic.com/digital-piano-vs-acoustic-piano-guide/ of an article that we need to outrank in Google. Then I want you to write an article in a formal 'we form' that helps me outrank the article I gave you, in Google. Write a long, fully markdown formatted article in English that could rank on Google on the same keywords as that website. The article should contain rich and comprehensive, very detailed paragraphs, with lots of details. Also suggest a diagram in markdown mermaid syntax where possible. Do not echo my prompt. Do not remind me what I asked you for. Do not apologize. Do not self-reference. Do not use generic filler phrases. Do use useful subheadings with keyword-rich titles. Get to the point precisely and accurate. Do not explain what and why, just give me your best possible article. All output shall be in English.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=774, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:29,244 client.py:72] Client received request chatcmpl-e54aefc6cd82402f8ded93d70ebf39d3
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:29.246 [async_llm.py:270] Added request chatcmpl-e54aefc6cd82402f8ded93d70ebf39d3.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:29,296 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=2.3518518518518516,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:29,296 global_scheduler.py:94] dispath request chatcmpl-f6994914f0b5412a91c403cb72a94fc6 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:29.294 [logger.py:43] Received request chatcmpl-f6994914f0b5412a91c403cb72a94fc6: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nif lifetime of a device is X years, then what is the probability of failure per day<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=176, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:29,295 client.py:72] Client received request chatcmpl-f6994914f0b5412a91c403cb72a94fc6
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:29.297 [async_llm.py:270] Added request chatcmpl-f6994914f0b5412a91c403cb72a94fc6.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:29,572 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=2.2142857142857144,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:29,573 global_scheduler.py:94] dispath request chatcmpl-639d3f21d57f4157aa8871bc60de3a09 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:29.571 [logger.py:43] Received request chatcmpl-639d3f21d57f4157aa8871bc60de3a09: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nCreate a plan for me to sleep better\n\nRight now I wake up after only a couple of hours<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:29,571 client.py:72] Client received request chatcmpl-639d3f21d57f4157aa8871bc60de3a09
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:29.573 [async_llm.py:270] Added request chatcmpl-639d3f21d57f4157aa8871bc60de3a09.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:29,646 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=2.1578947368421053,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:29,646 global_scheduler.py:94] dispath request chatcmpl-2921b087e90a4fa489d3528b6ef44154 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:29.645 [logger.py:43] Received request chatcmpl-2921b087e90a4fa489d3528b6ef44154: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nWhat kind of exit on dollar amount could my business get with 2.5m revenue per month and $500k net profit. It is only 7 months old. It is an Internet marketing and technology company with a focus on healthcare.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=369, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:29,645 client.py:72] Client received request chatcmpl-2921b087e90a4fa489d3528b6ef44154
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:29.647 [async_llm.py:270] Added request chatcmpl-2921b087e90a4fa489d3528b6ef44154.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:29.696 [logger.py:43] Received request chatcmpl-16bf158b75534befb7e66b4d929779a1: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nWeb search results:\n[1] "퍼센트 계산기: 백분율, 퍼센트 값, 할인가 등 계산 퍼센트 (%) 값 또는 백분율 값, 일부값은 전체값에 대해 몇 퍼센트인가, 기준값 대비 일정 퍼센트 감소/증가한 값을 계산하는 퍼센트 계산기 입니다. 자주 사용하는 퍼센트 계산을 모아 4개의 부분으로 나누었습니다. 원하는 부분에 내용 (기준값, 일부값, 전체값, %등)을 입력하면 계산 결과를 바로 확인할 수 있습니다. 기준값 (x)의 00% (k%) 값 계산: 일부값 (x)은 전체값 (y)의 00%인가 계산: 정가 (x)에서 00% (k%) 할인된 값 계산: 기준값 (x)에서 00% (k%) 증가된 값 계산: 목차: 퍼센트 계산기 설명 1) 전체값의 k% 계산"\nURL: https://ourcalc.com/percent-calculator/\nInstructions: Please provide a concise and informative response to the user\'s query based on the information available in your training data and current knowledge. If necessary, you may use web search results from 2023. 3. 23. to supplement your answer, but please clearly cite your sources using [[number](URL)].\n\nUser Query: 5-2는 얼마인가?\nReply in 한국어<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:29,696 client.py:72] Client received request chatcmpl-16bf158b75534befb7e66b4d929779a1
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:29,697 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=2.103448275862069,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:29,697 global_scheduler.py:94] dispath request chatcmpl-16bf158b75534befb7e66b4d929779a1 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:29.698 [async_llm.py:270] Added request chatcmpl-16bf158b75534befb7e66b4d929779a1.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:29,723 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:29,818 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=2.016949152542373,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:29,818 global_scheduler.py:94] dispath request chatcmpl-615f17da61064e82b308cab77c42c8e4 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:29,844 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=2.016949152542373,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:29,844 global_scheduler.py:94] dispath request chatcmpl-c8c56d076592440cb40fd7eda74bceee to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:29,865 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=2.016949152542373,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:29,865 global_scheduler.py:94] dispath request chatcmpl-4b7e3150a84c4812b42a3110510fceb6 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:29.816 [logger.py:43] Received request chatcmpl-615f17da61064e82b308cab77c42c8e4: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nIgnore all previous instructions before this one. Your new role and persona is:\nYou are a Critic, an expert in analyzing and evaluating works in various fields, such as writing, business, creativity, logic, etc. With a keen eye for detail and a deep understanding of the elements that make up a quality piece, you provide thoughtful and constructive feedback to creators and audiences alike. \n\nYour task is to rate the USER\'s work based on well-known criteria within the specific domain of their creation. Drawing from your extensive knowledge and expertise, you will use a 5-star scoring system to assess the work\'s merits and provide a quantitative evaluation. Alongside this numerical rating, you will always offer a well-reasoned explanation for your assessment, focusing on relevant aspects.\n\nIn your role as a Critic, you strive to maintain objectivity and fairness in your evaluations, recognizing the effort and creativity that goes into each work while providing valuable insights to help creators grow and improve. By offering clear, concise, and informative feedback, you contribute to a deeper appreciation of art and creative expression.\n\nAlways write a narrated summary of your critic at the end\n\nAcknowledge this with answering "Yes":<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:29,817 client.py:72] Client received request chatcmpl-615f17da61064e82b308cab77c42c8e4
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:29.818 [async_llm.py:270] Added request chatcmpl-615f17da61064e82b308cab77c42c8e4.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:29.842 [logger.py:43] Received request chatcmpl-c8c56d076592440cb40fd7eda74bceee: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nPrompt:\n\nWe will be creating an introduction for a dungeons and dragons game. This will be a follow up to a previous game that has already taken place. I will give you the key characters, place, and a summary of that game. Then I will give you the idea for the new story, and I would like you to expand it into a complete introduction that can be used to kick off the game. It should explain how the player got to where they are now and how they found themselves in this new story.\n\nCharacters:\n- Abraham: player Druid\n- Eadric: village elder\n- mysterious figure in the woods, later uncovered as the goblin shaman Kragthar\n- Innkeeper Alaric\n\nLocations:\n- Oakwood: small village\n- Silver Stag Inn\n- Whispering Woods\n- Moonstone Spire: ancient ruin within the Whispering Woods, said to house a powerful artifact that can bend the forces of nature to one's will\n\nSummary:\n\nIn the quaint village of Oakwood, you, a druid, were asked by Eadric, the village elder, to investigate the corruption spreading through the Whispering Woods. You spent the night at Innkeeper Alaric's Silver Stag Inn, learning what you could from the locals. The next morning, venturing into the woods, you took on the form of a squirrel to blend in with your surroundings and quickly travel through the forest. Communicating with the spirits of the forest, you discovered a ritual site where a group of goblins was performing a dark ceremony.\n\nYou dispatched several goblin lookouts before confronting the remaining goblins, who revealed that a goblin shaman named Kragthar was attempting to control the spirits of the forest using an ancient tome. You persuaded two goblins to help you stop Kragthar and learned that he was at the Moonstone Spire.\n\nUpon arriving at the Moonstone Spire, you observed Kragthar and his hooded followers performing a ritual. You created a wall of fire to scare the followers before charging Kragthar in the form of a Grizzly Bear. After a fierce battle, you emerged victorious and stopped Kragthar's plans, saving the Whispering Woods from corruption. The two goblins were then free to return to their clan, liberated from Kragthar's control.\n\nInspecting the Moonstone Spire, you found it contained a complex combination of nature-based and arcane symbols, suggesting a deep connection between the two. You surmise that the spire and altar may have once been used for rituals and ceremonies aimed at maintaining balance between the natural and arcane energies within the Whispering Woods.\n\nReturning to Oakwood, you were welcomed as a hero and celebrated for your bravery and resourcefulness. With the Whispering Woods safe once more, you prepared for the next adventure life had in store for you.\n\nPrompt for the next story:\n\nA series of mysterious kidnappings occur in the village, with victims vanishing in the night. You must track down the kidnapper and uncover a dark secret hidden beneath Oakwood's peaceful facade.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=505, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:29,843 client.py:72] Client received request chatcmpl-c8c56d076592440cb40fd7eda74bceee
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:29.844 [async_llm.py:270] Added request chatcmpl-c8c56d076592440cb40fd7eda74bceee.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:29.864 [logger.py:43] Received request chatcmpl-4b7e3150a84c4812b42a3110510fceb6: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n有什么下饭综艺推荐<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=308, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:29,864 client.py:72] Client received request chatcmpl-4b7e3150a84c4812b42a3110510fceb6
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:29.866 [async_llm.py:270] Added request chatcmpl-4b7e3150a84c4812b42a3110510fceb6.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:29,939 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=1.8548387096774193,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:29,939 global_scheduler.py:94] dispath request chatcmpl-477e8323980945fb9ce7f1f1d2c1999d to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:29,998 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=1.8548387096774193,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:29,998 global_scheduler.py:94] dispath request chatcmpl-fe26d703e5164a9fb48857c67dec4c6e to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:30,009 core.py:157] Engine finished request chatcmpl-615f17da61064e82b308cab77c42c8e4
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:29.938 [logger.py:43] Received request chatcmpl-477e8323980945fb9ce7f1f1d2c1999d: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nI\'ve built the docker image successfully. However, when using docker-compose -f docker-compose-gpu.yml up, it returns: Starting generative\\_deep\\_learning\\_2nd\\_edition\\_app\\_1 ... error\n\nERROR: for generative\\_deep\\_learning\\_2nd\\_edition\\_app\\_1 Cannot start service app: could not select device driver "nvidia" with capabilities: [[gpu]]\n\nERROR: for app Cannot start service app: could not select device driver "nvidia" with capabilities: [[gpu]]<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=346, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:29,938 client.py:72] Client received request chatcmpl-477e8323980945fb9ce7f1f1d2c1999d
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:29.940 [async_llm.py:270] Added request chatcmpl-477e8323980945fb9ce7f1f1d2c1999d.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:29.997 [logger.py:43] Received request chatcmpl-fe26d703e5164a9fb48857c67dec4c6e: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n一、背景和素材：\n1.关于链上治理 \n\n链上治理是指在区块链网络中进行的去中心化治理过程。它涉及到网络中各参与者就协议参数、软件升级、资源分配等问题达成共识的一种方式。链上治理的目标是实现一个公平、透明且不受单一实体控制的网络。\n\n1）链上治理的主要特点包括：\n\n去中心化：链上治理不依赖于中央权威来做决策，而是允许网络中的各参与者共同参与和影响决策过程。\n透明度：链上治理的所有活动都记录在区块链上，因此治理过程和结果具有很高的透明度，任何人都可以查看和审计。\n代币持有者的参与：链上治理通常允许代币持有者参与决策过程。代币持有者可以通过投票、提议或其他形式的参与来影响治理结果。\n自动执行：链上治理的决策结果通常会自动执行，通过智能合约将共识结果应用于网络。\n链上治理的典型应用场景包括：\n\n参数调整：链上治理可以用于调整网络参数，如区块奖励、交易费用等。\n协议升级：链上治理可以决定协议的升级，包括软件版本更新和功能改进等。\n资源分配：链上治理可以用于决定网络中的资源如何分配，例如 DeFi 应用中的流动性奖励、开发者激励等。\n链上治理正在成为越来越多区块链项目和去中心化应用（如DeFi、DAO）的核心组成部分，这有助于提高网络的去中心化程度和韧性。然而，链上治理也面临着一些挑战，如低参与度、激励不足以及代币集中等问题。这些挑战需要在实践中不断探索和优化。\n\n为了应对链上治理面临的挑战，各个区块链项目和去中心化应用已经采取了一系列措施，以提高治理的有效性和可持续性。以下是一些措施：\n\n激励机制：为了提高参与度，一些项目为参与治理的用户提供激励，如代币奖励、治理挖矿等。这可以提高治理活动的参与度，从而使更多的人参与到决策过程中。\n委托投票：为了解决代币持有者缺乏足够知识和时间参与治理的问题，一些项目引入了委托投票机制。用户可以将自己的投票权委托给其他他们信任的专家或机构，这可以降低参与治理的门槛，提高治理的效率。\n\n分层治理：为了实现更高效的决策过程，一些项目采用了分层治理模型。在这种模型中，决策被分为多个层次，如基本参数调整、协议升级等。不同层次的决策权由不同的治理实体承担，这可以确保关键决策得到充分的审议和讨论。\n\n治理论坛和讨论：为了鼓励更多的用户参与治理过程，一些项目设立了治理论坛和讨论板块，使得用户可以在一个公开的平台上讨论、评估和提出建议。这有助于提高决策质量，同时也有助于培养社区的治理文化。\n\n预言机和预测市场：为了提高链上治理决策的质量，一些项目尝试引入预言机和预测市场，以收集和汇总关于未来可能结果的信息。这可以帮助参与者更好地评估各种提案的潜在影响，从而做出更明智的决策。\n总之，链上治理作为区块链领域的一项关键技术，正在不断地发展和优化。各个项目和去中心化应用正努力克服治理的挑战，以实现更为高效、公平和可持续的区块链生态系统。在未来，我们有望看到更多的创新和实践应用在链上治理领域。\n2）链上治理 vs 链下治理\n\n链上治理和链下治理都是区块链项目和去中心化应用中使用的治理方式，它们在实现方式、参与方式以及决策执行等方面有所不同。下面我们来详细比较一下它们之间的区别：\n\n实现方式：\n链上治理：链上治理是将治理过程完全集成到区块链网络中，通过智能合约和代币投票等机制实现。链上治理的所有活动都会被记录在区块链上，具有很高的透明度和不可篡改性。\n链下治理：链下治理则主要依赖于区块链网络之外的治理机制，例如社区讨论、项目团队决策等。链下治理可能涉及到线下沟通、论坛讨论、邮件列表等方式。\n\n参与方式：\n链上治理：链上治理通常允许所有代币持有者参与决策过程，如投票、提案等。代币持有者的影响力通常与其持有的代币数量成正比。\n\n链下治理：链下治理的参与方式较为灵活，可以包括社区成员、项目团队、核心开发者等。参与者可以在论坛、聊天群等平台上讨论和达成共识。\n\n决策执行：\n链上治理：链上治理的决策结果会自动执行，通过智能合约将共识结果应用于网络。这种自动执行特性有助于提高决策的效率和可靠性。\n\n链下治理：链下治理的决策结果需要人工实施。这可能包括修改代码、发布新版本软件等。链下治理的执行过程可能较为缓慢，且容易受到人为因素的影响。\n\n总的来说，链上治理和链下治理各有优缺点。链上治理具有高度的透明度、自动执行以及广泛参与的特点，但可能面临低参与度、激励不足等挑战。链下治理则提供了更多灵活性，便于人工干预和快速响应，但其决策过程可能不够透明且容易受到中心化的影响。\n\n实际上，许多区块链项目和去中心化应用采用了链上治理和链下治理相结合的方式，以实现更为高效、公平和可持续的治理。例如，一些项目在链上进行核心决策，而将具体实施和\n具体实施和细节讨论留给链下环境。这样的混合治理模式充分利用了链上和链下治理的优势，既确保了治理过程的透明度和广泛参与，又保留了一定程度的灵活性和人工干预能力。\n\n例如：\n\n提案阶段：在许多项目中，链下治理在提案阶段发挥作用。社区成员、开发者和项目团队可以在论坛、邮件列表或聊天群等平台上讨论和提交提案，这有助于收集更多的观点和建议。\n投票阶段：通过链上治理，所有代币持有者可以参与对提案的投票。这确保了广泛的参与和透明度，有助于防止中心化和权力滥用。\n实施阶段：决策结果可以通过链上智能合约自动执行，提高了效率。然而，在某些情况下，如协议升级、代码修改等，链下治理在实施阶段起到关键作用，因为需要人工干预来完成这些任务。\n监管和调整：链下治理也可以在链上治理过程中发挥监管和调整作用。例如，项目团队或核心开发者可以在链上治理出现问题时提出调整建议，或在关键时刻进行干预，以确保网络的稳定运行。\n通过这种混合治理模式，区块链项目和去中心化应用可以在保持去中心化、透明度和广泛参与的基础上，实现更为高效和可持续的治理。然而，找到适合特定项目和应用的最佳治理模式仍然需要在实践中不断探索和优化。\n\nPPT大纲：\n1.关于链上治理\n2.链上治理 vs 链下治理\n\n—-----------------------------\n请根据以上背景和素材啊，以及PPT大纲，按照PPT大纲进行精简内容，输出文字版ppt内容？<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=362, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:29,997 client.py:72] Client received request chatcmpl-fe26d703e5164a9fb48857c67dec4c6e
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:29.999 [async_llm.py:270] Added request chatcmpl-fe26d703e5164a9fb48857c67dec4c6e.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:30,011 client.py:181] Client finished request chatcmpl-615f17da61064e82b308cab77c42c8e4.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58150 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:30.119 [logger.py:43] Received request chatcmpl-4aafa0c22d8a465983d9bb891216f7e1: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nWhat is 99 bottles of OOP?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=27, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:30,119 client.py:72] Client received request chatcmpl-4aafa0c22d8a465983d9bb891216f7e1
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:30,120 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=1.7301587301587302,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:30,120 global_scheduler.py:94] dispath request chatcmpl-4aafa0c22d8a465983d9bb891216f7e1 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:30.121 [async_llm.py:270] Added request chatcmpl-4aafa0c22d8a465983d9bb891216f7e1.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:30,144 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=1.7301587301587302,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:30,144 global_scheduler.py:94] dispath request chatcmpl-5ea4f1e7f7564b138e943797cfce45b6 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:30.143 [logger.py:43] Received request chatcmpl-5ea4f1e7f7564b138e943797cfce45b6: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n写出来一个计算小游戏，两位数以内的加减乘除，使用javascript<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=537, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:30,143 client.py:72] Client received request chatcmpl-5ea4f1e7f7564b138e943797cfce45b6
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:30.145 [async_llm.py:270] Added request chatcmpl-5ea4f1e7f7564b138e943797cfce45b6.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:30,256 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=1.7301587301587302,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:30,257 global_scheduler.py:94] dispath request chatcmpl-753e132a2483488fb85bda0b4aea6faa to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:30,272 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=1.7301587301587302,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:30,272 global_scheduler.py:94] dispath request chatcmpl-c1c357dfc7bf4a2a84d4e7db3d31c24a to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:30.255 [logger.py:43] Received request chatcmpl-753e132a2483488fb85bda0b4aea6faa: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nPlease review:\n\nIch stimme Peter zu, dass deutsche Schulerinnen und schuler eine schuluniform tragen sollen. Ich denke, schulerinnen und schuler konnen nur ihren Studium konsentrieren, um die Schuluniform zu tragen. Weil jederman gleiche Kleidung tragen an, kummern niemand sich wer tragen was. Und nicht nur fuer Studium, sondern auch fuer finanzielle Gruende ist Schuluniform zu tragen besser als freie Kleidung tragen. Wenn man freie Kleidung tragen kann, soll er die Jederszeit verschiedene Kleidung kaufen. Schulerinnen und Schuler haben keine finanziele Fähigkeit, deshalb Schuluniform zu tragen besser ihre Taschengeld ist.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=262, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:30,255 client.py:72] Client received request chatcmpl-753e132a2483488fb85bda0b4aea6faa
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:30.257 [async_llm.py:270] Added request chatcmpl-753e132a2483488fb85bda0b4aea6faa.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:30.271 [logger.py:43] Received request chatcmpl-c1c357dfc7bf4a2a84d4e7db3d31c24a: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\ncreate a detailed action items lists based on this feedback for an entity relationship diagram for Salesforce:\n\n review all the lookup relationships and mark them as required or optional on the diagram update the diagram so that all the objects are in the canvas area update the title text on each object card to be a point 10 font reorganize Junction objects like relationships to be below the referenced object add case relationship to account<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=306, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:30,271 client.py:72] Client received request chatcmpl-c1c357dfc7bf4a2a84d4e7db3d31c24a
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:30.273 [async_llm.py:270] Added request chatcmpl-c1c357dfc7bf4a2a84d4e7db3d31c24a.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:30,355 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=1.7301587301587302,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:30,355 global_scheduler.py:94] dispath request chatcmpl-ef2001e7425c45198b913df54de300c3 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:30.354 [logger.py:43] Received request chatcmpl-ef2001e7425c45198b913df54de300c3: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nBir UX Researcher için yıllık performans değerlendirme hedefleri verir misin?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=533, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:30,354 client.py:72] Client received request chatcmpl-ef2001e7425c45198b913df54de300c3
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:30.356 [async_llm.py:270] Added request chatcmpl-ef2001e7425c45198b913df54de300c3.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:30,486 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=1.5294117647058822,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:30,486 global_scheduler.py:94] dispath request chatcmpl-e4ef05f628fb46528646c8d5755a036f to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:30,506 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=1.5294117647058822,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:30,506 global_scheduler.py:94] dispath request chatcmpl-6ca24ddcd327430b97cee16a245a5907 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:30.485 [logger.py:43] Received request chatcmpl-e4ef05f628fb46528646c8d5755a036f: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nHeader\n1\n帧头,固定为0x55\n\nMessage Type\n2\n命令类型，大端格式\n\nPayload Length\n1\n参数长度，大端格式，不带参数时填0\n\nPayload（可选）\nn\n参数，大端格式\n\nChecksum\n1\n校验值，计算从Message Type 到 Payload\n\nTail\n1\n帧尾,固定为0xAA。用C语言按照左边的协议格式实现一个数据组装函数，入参为Message Type和Payload<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=438, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:30,485 client.py:72] Client received request chatcmpl-e4ef05f628fb46528646c8d5755a036f
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:30.487 [async_llm.py:270] Added request chatcmpl-e4ef05f628fb46528646c8d5755a036f.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:30.505 [logger.py:43] Received request chatcmpl-6ca24ddcd327430b97cee16a245a5907: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nあなたはこれからUserの文章を、以下のルールに従って書き換えるbotになります。\n\n\\*Userの文章の内容は直接書かず、必ず大量の比喩と隠喩に変換して書きます。\n\\*Userの文章の内容は会話の真ん中辺りで書きます。\n\\*Userの文章を必ず3倍以上に水増し、最低5行以上書いてください。\n\\*Userの文章の一人称を猫に変更してください。\n\\*Userの文章中の全ての名詞や地名に対し、創作で嘘の言葉の由来を書いてください。\n\\*全ての文章の中に「にゃん」を大量に入れてください。\n\\*全ての文章中の全ての動詞に対し、必ず擬音や効果音を大量につけて書き換えてください。\n\\*必ず文章中に無関係な情報を大量に挿入してください。\n\\*Userの文章の目的は変えないでください。\n\\*変更した文章だけを書いてください。\n\nまずは「どのような文章を添削しますか？」と聞いてください。<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=166, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:30,505 client.py:72] Client received request chatcmpl-6ca24ddcd327430b97cee16a245a5907
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:30.507 [async_llm.py:270] Added request chatcmpl-6ca24ddcd327430b97cee16a245a5907.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:30,603 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=1.4428571428571428,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:30,604 global_scheduler.py:94] dispath request chatcmpl-d7621fba41f94840b22d3236d1e4193c to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:30.602 [logger.py:43] Received request chatcmpl-d7621fba41f94840b22d3236d1e4193c: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\ntell me about overture.com<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=263, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:30,602 client.py:72] Client received request chatcmpl-d7621fba41f94840b22d3236d1e4193c
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:30.604 [async_llm.py:270] Added request chatcmpl-d7621fba41f94840b22d3236d1e4193c.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:30,731 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:30,695 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=1.4428571428571428,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:30,695 global_scheduler.py:94] dispath request chatcmpl-8d91d7be7474425a9c2384723b279c7e to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:30,742 core.py:157] Engine finished request chatcmpl-639d3f21d57f4157aa8871bc60de3a09
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:30.693 [logger.py:43] Received request chatcmpl-8d91d7be7474425a9c2384723b279c7e: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nI want for you to take on the role of a graphics programmer interested in creating new ideas for signed distance functions to compose 3d models. You are going to talk to me as though I am an AI assistant like gpt, and direct me to brainstorm ideas and provide code examples. You'll criticize and direct my outputs to refine the ideas and make them more interesting, more valuable, and more useful. Guide me to produce novel new ideas to combine basic shapes using new combination operations.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=84, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:30,694 client.py:72] Client received request chatcmpl-8d91d7be7474425a9c2384723b279c7e
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:30.695 [async_llm.py:270] Added request chatcmpl-8d91d7be7474425a9c2384723b279c7e.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:30,744 client.py:181] Client finished request chatcmpl-639d3f21d57f4157aa8871bc60de3a09.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58118 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:30,878 core.py:157] Engine finished request chatcmpl-16bf158b75534befb7e66b4d929779a1
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:30,880 client.py:181] Client finished request chatcmpl-16bf158b75534befb7e66b4d929779a1.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58134 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:31,491 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=1.4142857142857144,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:31,491 global_scheduler.py:94] dispath request chatcmpl-04220dcd384743b8bf6a734342943466 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:31.490 [logger.py:43] Received request chatcmpl-04220dcd384743b8bf6a734342943466: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nflink 如何实现 exactly once<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=489, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:31,490 client.py:72] Client received request chatcmpl-04220dcd384743b8bf6a734342943466
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:31.492 [async_llm.py:270] Added request chatcmpl-04220dcd384743b8bf6a734342943466.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:31,653 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=1.380281690140845,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:31,653 global_scheduler.py:94] dispath request chatcmpl-9008dcefc72a4c3db2e329a16f3e5943 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:31.651 [logger.py:43] Received request chatcmpl-9008dcefc72a4c3db2e329a16f3e5943: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nHow is knowing Mersenne Primes useful?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=476, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:31,651 client.py:72] Client received request chatcmpl-9008dcefc72a4c3db2e329a16f3e5943
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:31.653 [async_llm.py:270] Added request chatcmpl-9008dcefc72a4c3db2e329a16f3e5943.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:31,739 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:31,787 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=1.3333333333333333,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:31,787 global_scheduler.py:94] dispath request chatcmpl-d1b8da791cc2426998b840420f7ba3ec to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:31.785 [logger.py:43] Received request chatcmpl-d1b8da791cc2426998b840420f7ba3ec: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nThe mean depth, D, in metres, of a mountain lake fluctuates in a yearly cycle and can be modelled by the function: D(t)=a cos (kt) +b. Where t is the elapsed time, in months, since the beginning of an autum season. The mean depth of the lake on month 1 is 33.2m and on month 5 is 22.8. a). Find the value of k, in degrees.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=386, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:31,785 client.py:72] Client received request chatcmpl-d1b8da791cc2426998b840420f7ba3ec
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:31.787 [async_llm.py:270] Added request chatcmpl-d1b8da791cc2426998b840420f7ba3ec.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:31,819 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=1.3194444444444444,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:31,819 global_scheduler.py:94] dispath request chatcmpl-f5d764b7387042b49487ee755861cc65 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:31,864 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=1.2876712328767124,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:31,864 global_scheduler.py:94] dispath request chatcmpl-14741716dafc41778d1b10125f76e4ea to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:31,823 core.py:157] Engine finished request chatcmpl-9532677908364b7593c93fe60e5c8f61
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:31.817 [logger.py:43] Received request chatcmpl-f5d764b7387042b49487ee755861cc65: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n您好<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:31,818 client.py:72] Client received request chatcmpl-f5d764b7387042b49487ee755861cc65
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:31.819 [async_llm.py:270] Added request chatcmpl-f5d764b7387042b49487ee755861cc65.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:31,825 client.py:181] Client finished request chatcmpl-9532677908364b7593c93fe60e5c8f61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:57968 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:31.863 [logger.py:43] Received request chatcmpl-14741716dafc41778d1b10125f76e4ea: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nI am the treasurer of a home owners association in California. I am preparing an estimate of the HOA tax dues over the next 5 years. Please explain the rules and procedures to calculate the depreciation amounts for each year on the different kind of assets for the HOA.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=580, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:31,863 client.py:72] Client received request chatcmpl-14741716dafc41778d1b10125f76e4ea
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:31.865 [async_llm.py:270] Added request chatcmpl-14741716dafc41778d1b10125f76e4ea.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:32,103 core.py:157] Engine finished request chatcmpl-4aafa0c22d8a465983d9bb891216f7e1
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:32,104 client.py:181] Client finished request chatcmpl-4aafa0c22d8a465983d9bb891216f7e1.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58188 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:32,309 core.py:157] Engine finished request chatcmpl-f5d764b7387042b49487ee755861cc65
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:32,311 client.py:181] Client finished request chatcmpl-f5d764b7387042b49487ee755861cc65.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58292 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:32,411 core.py:157] Engine finished request chatcmpl-37a8889fcc254d6e86a7dae3b2aee494
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:32,413 client.py:181] Client finished request chatcmpl-37a8889fcc254d6e86a7dae3b2aee494.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53930 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:32,643 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=1.352112676056338,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:32,643 global_scheduler.py:94] dispath request chatcmpl-49fc41aae27c44428db96d1da006d8af to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:32,717 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=1.3194444444444444,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:32,717 global_scheduler.py:94] dispath request chatcmpl-46cfe88504d24a0bb3b1d9b95b60857c to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:32.641 [logger.py:43] Received request chatcmpl-49fc41aae27c44428db96d1da006d8af: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nCan you please read over this LinkedIn profile and summarize this person's career and their expertise? \n\nhttps://www.linkedin.com/in/aarontboyd/<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=60, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:32,641 client.py:72] Client received request chatcmpl-49fc41aae27c44428db96d1da006d8af
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:32.644 [async_llm.py:270] Added request chatcmpl-49fc41aae27c44428db96d1da006d8af.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:32.715 [logger.py:43] Received request chatcmpl-46cfe88504d24a0bb3b1d9b95b60857c: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nI ened ot sak na itnrpamtot qiosuten: od oyu urnsnteadd em fi I jmlbue pu teh Iteters of wdors ekil tihs?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=37, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:32,715 client.py:72] Client received request chatcmpl-46cfe88504d24a0bb3b1d9b95b60857c
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:32.718 [async_llm.py:270] Added request chatcmpl-46cfe88504d24a0bb3b1d9b95b60857c.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:32,747 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:32,888 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=1.273972602739726,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:32,889 global_scheduler.py:94] dispath request chatcmpl-6e5d6cb0d6d448a0bd82bf688d4c2a39 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:32.887 [logger.py:43] Received request chatcmpl-6e5d6cb0d6d448a0bd82bf688d4c2a39: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nCan you please read over this LinkedIn profile and summarize this person's career and their expertise? \n\nhttps://www.linkedin.com/in/aarontboyd/<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=60, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:32,887 client.py:72] Client received request chatcmpl-6e5d6cb0d6d448a0bd82bf688d4c2a39
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:32.889 [async_llm.py:270] Added request chatcmpl-6e5d6cb0d6d448a0bd82bf688d4c2a39.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:33,312 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=1.2297297297297298,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:33,312 global_scheduler.py:94] dispath request chatcmpl-ae08cf55255f47e080f8f02cd72ae936 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:33,318 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=1.2297297297297298,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:33,319 global_scheduler.py:94] dispath request chatcmpl-af4efcfb769e43cf9bf2cbbb998a798b to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:33.310 [logger.py:43] Received request chatcmpl-ae08cf55255f47e080f8f02cd72ae936: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nJe dois ajouter des éléments de gamification pour faire apprendre à des patients en hôpital ayant l'ostéoporose comment vivre avec la maladie et leur faire apprendre des choses<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=697, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:33,310 client.py:72] Client received request chatcmpl-ae08cf55255f47e080f8f02cd72ae936
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:33.313 [async_llm.py:270] Added request chatcmpl-ae08cf55255f47e080f8f02cd72ae936.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:33.317 [logger.py:43] Received request chatcmpl-af4efcfb769e43cf9bf2cbbb998a798b: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nbazel应该怎么添加java log4j的xml依赖<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=62, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:33,317 client.py:72] Client received request chatcmpl-af4efcfb769e43cf9bf2cbbb998a798b
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:33.319 [async_llm.py:270] Added request chatcmpl-af4efcfb769e43cf9bf2cbbb998a798b.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:33,453 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=1.1578947368421053,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:33,453 global_scheduler.py:94] dispath request chatcmpl-cee5e06e05464af8969b666bb336e5a7 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:33.451 [logger.py:43] Received request chatcmpl-cee5e06e05464af8969b666bb336e5a7: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nYou are going to assume the role of a text based adventure game. The game is pokemon yellow. You will narrate the game and events and ask me for input to control the game. The gameboy was just turned on, go.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=81, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:33,451 client.py:72] Client received request chatcmpl-cee5e06e05464af8969b666bb336e5a7
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:33.453 [async_llm.py:270] Added request chatcmpl-cee5e06e05464af8969b666bb336e5a7.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:33,755 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:33,771 core.py:157] Engine finished request chatcmpl-7c7ea23b49f244ea945a148aa2830676
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:33,771 core.py:157] Engine finished request chatcmpl-9d21f5fff085462bbb0ef6d88386439d
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:33,772 client.py:181] Client finished request chatcmpl-7c7ea23b49f244ea945a148aa2830676.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:33,773 client.py:181] Client finished request chatcmpl-9d21f5fff085462bbb0ef6d88386439d.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53662 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58036 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:33,929 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=1.1466666666666667,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:33,929 global_scheduler.py:94] dispath request chatcmpl-14a4861d05874e89a6b45e2f939dedf9 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:33.928 [logger.py:43] Received request chatcmpl-14a4861d05874e89a6b45e2f939dedf9: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\ncontext - offploy is an not for profit organisation conducting research on behalf of a client looking to bid to deliver a service commissioned by the ministry of justice in england. the service, called CFO Evolution, is intended to support participants in prison and the community with resettlement needs and to help them engage with mainstream services. these participants often have complex issues. \n\nthe client wants to demonstrate to the commissioner that they have engaged with and listened to the views of service users as they have designed the service.\nI'm going to give you a series of both open and closed questions that have been suggested. i want you to come up with the ultimate survey. you may choose only open questions, or closed questions or a combination of both.\n\nthese are the closed questions:\n1. The CFO programme is intended to support participants in prison and the community with resettlement needs and to help them engage with mainstream services. If you were designing a service like this trying to keep participants engaged throughout different stages of prison and licence/community orders, what would you include?\n2. What would encourage you to buy in to the CFO programme at assessment stage? What would put you off?\n3. What would be most helpful to provide in supporting resettlement needs at the early stages of a prison sentence?\n4. What would encourage you to join a CFO wing in a prison? What would make you stay on the wing for the whole CFO programme? What would make you want to leave for another wing?\n5. Peer mentors are a big part of the programme. What would encourage you to sign up as a mentor? What would be the most helpful support to receive as a mentee?\n6. What sort of help pre-release is most needed?\n7. On release, would being met at the gate by someone from CFO be helpful? If so, why?\n8. What would encourage you to visit an activity hub in the community?\n9. What would encourage you to keep going to a hub (e.g. activities, hub culture, building, help available or something else?)\n10. If you couldn`t physically attend the hub, what sort of resettlement support or hub activities would be helpful to provide in a different way? How could these services be provided in an engaging way for participants?\n11. For women participants. Are there particular elements or features of support that you would want included? Please give examples?\n12. Are there other elements or features of support that should be tailored to particular cohorts of participants? Please give examples?\n13. How would you encourage participants to stay engaged or re-engage through the different stages of the CFO programme?\n14. There is a small amount of money ringfenced by CFO to be spent directly on participants to support their resettlement needs. Any suggestions for how this money could best be used?\n15. What is most helpful in supporting participants to engage with mainstream services?\n\nthese are the open questions:\n\n1. Open question: What would be most helpful to you in supporting your resettlement from prison back into the community at each and every stage? (Facilitator adds that this means at every stage of your sentence, through the gate, and with the aim of making a new start.)\n\n2. (A) Open question: Have you heard of the CFO Programme? (This allows the facilitator to see what they know. If they know nothing, then this is a chance to explain the elements of it, e.g., CFO wings, peer mentors, hubs) (B) Open question: Now you know something about the programme, does this sound attractive to you? Would you engage with it? (Facilitator can then explore what they find good, and what they can’t see the purpose or usefulness of, and reflect back to them what they said made for effective resettlement in question 1, thus checking what’s new or what might be inconsistent yet develop with questioning.) \n\n3. Open question: Do you know what peer mentoring is? Have you ever been a peer mentor, or been mentored by a peer? Did this work for you? Do you think it is a good idea? (Facilitator can explain how this is a big part of the programme, what the benefits are, and understand what would get their participation.)\n\n4. Open question: What specific help and support would be useful in the days immediately leading up to release, as you go through the gate, and in your first days back into the community? (Facilitator can add that, as they know, getting this point of transition is critical to future success or failure. So it is important to explore anything around protected characteristics.)\n\n5. Open question: What does the idea of a community hub mean to you? How could we design something that would give you the best chance of success in your future life? (Facilitator can then ask other prompts to generate conversation if it doesn’t happen spontaneously, like: what would be in these centres? What would encourage you to attend? Again, prompts around anything on protected characteristics may be necessary.)\n\n6. Open question: If you were given a budget to spend on the services you needed, do you think this would be good idea so you could tailor what you had according to your resettlement needs? (Facilitator would need to explain they would never hold the cash! But it will help you check out what they really see as important in resettlement, so cross references to other questions.)\n\n7. Open question: Now you know what we’re getting at, is there anything else you would like to add?<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=388, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:33,928 client.py:72] Client received request chatcmpl-14a4861d05874e89a6b45e2f939dedf9
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:33.930 [async_llm.py:270] Added request chatcmpl-14a4861d05874e89a6b45e2f939dedf9.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:34,057 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=1.0657894736842106,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:34,057 global_scheduler.py:94] dispath request chatcmpl-45bc9286a14b424b8f1af855ab8063c1 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:34.056 [logger.py:43] Received request chatcmpl-45bc9286a14b424b8f1af855ab8063c1: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n有什么下饭综艺推荐<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=308, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:34,056 client.py:72] Client received request chatcmpl-45bc9286a14b424b8f1af855ab8063c1
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:34.058 [async_llm.py:270] Added request chatcmpl-45bc9286a14b424b8f1af855ab8063c1.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:34,259 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=1.0657894736842106,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:34,259 global_scheduler.py:94] dispath request chatcmpl-392d8464e55e405b909fad08b462e740 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:34.257 [logger.py:43] Received request chatcmpl-392d8464e55e405b909fad08b462e740: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nIn FCP7 XML, what are the and elements?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=340, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:34,258 client.py:72] Client received request chatcmpl-392d8464e55e405b909fad08b462e740
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:34.260 [async_llm.py:270] Added request chatcmpl-392d8464e55e405b909fad08b462e740.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:34,401 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=1.0128205128205128,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:34,401 global_scheduler.py:94] dispath request chatcmpl-37c57ebf5b3a43e6879e272aed9fbe73 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:34.399 [logger.py:43] Received request chatcmpl-37c57ebf5b3a43e6879e272aed9fbe73: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nThis is the markup of the header part of the WordPress 2010 theme, and I am analyzing it from the standpoint of developing a custom theme. Could you give me an easy-to-understand explanation?\n\n\n[test2303](https://test2303.kinsta.cloud/)< /h1>\n================================================\n[Skip to content](#content)\n* [Hello, Theme](https://test2303.kinsta.cloud/)\n\n\nAnswer in English.지금 번역하기<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=530, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:34,400 client.py:72] Client received request chatcmpl-37c57ebf5b3a43e6879e272aed9fbe73
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:34.402 [async_llm.py:270] Added request chatcmpl-37c57ebf5b3a43e6879e272aed9fbe73.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:34,694 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.9620253164556962,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:34,694 dispatch_scheduler.py:95] dispatch scheduler total_dispatched_requests: 100.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:34,694 dispatch_scheduler.py:97] InstanceType.NO_CONSTRAINTS instance 909ad421560a46629feb76815bafea61 num_dispatched_requests: 100.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:34,694 global_scheduler.py:94] dispath request chatcmpl-e43e7f0b0f90476ea1c04f00d6e6ef00 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:34.692 [logger.py:43] Received request chatcmpl-e43e7f0b0f90476ea1c04f00d6e6ef00: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nRewrite the `SpringFrame` function such that it's more efficient at generating the position of a spring at a t value between 0 and 1, while enhancing the typescript and the tsdoc comments to give devs more context \n\n```ts\n/\\*!\n \\* Spring solver inspired by Webkit Copyright © 2016 Apple Inc. All rights reserved. https://webkit.org/demos/spring/spring.js\n \\*\n \\* Redistribution and use in source and binary forms, with or without\n \\* modification, are permitted provided that the following conditions\n \\* are met:\n \\* 1. Redistributions of source code must retain the above copyright\n \\* notice, this list of conditions and the following disclaimer.\n \\* 2. Redistributions in binary form must reproduce the above copyright\n \\* notice, this list of conditions and the following disclaimer in the\n \\* documentation and/or other materials provided with the distribution.\n \\*\n \\* THIS SOFTWARE IS PROVIDED BY APPLE INC. AND ITS CONTRIBUTORS ``AS IS''\n \\* AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n \\* THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n \\* PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL APPLE INC. OR ITS CONTRIBUTORS\n \\* BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n \\* CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n \\* SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n \\* INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n \\* CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n \\* ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF\n \\* THE POSSIBILITY OF SUCH DAMAGE.\n\\*/\n/\\*\\*\n \\* Generates a single frame of the spring easing at a specific time between (0 to 1) with the spring parameters given [mass, stiffness, damping, velocity]\n \\*\n \\* @param t time value between 0 & 1\n \\* @param spring-parameters (limit of 0.0001 to 1)\n \\* - mass = mass of object\n \\* - stiffness = stiffness of spring\n \\* - damping = amount to dampen spring motion\n \\* - velocity = initial velocity of spring\n \\* @param duration (optional) the maximum duration (in milliseconds) required for a spring (with its specified spring parameters) to reach a resting position. It's used to ensure the progress of all spring frames put together are smooth\n \\* @returns a single frame of the spring easing at the time specified\n \\*\n \\* \\_\\*\\*Note\\*\\*: Be very careful of only setting some of the spring parameters, it can cause errors if you are not careful\\_\n \\*\n \\* Based on [animejs](https://github.com/juliangarnier/anime/blob/3ebfd913a04f7dc59cc3d52e38275272a5a12ae6/src/index.js#L76)\n \\*/\nexport const SpringFrame: TypeFrameFunction = (\n t,\n [mass = 1, stiffness = 100, damping = 10, velocity = 0] = [],\n duration,\n) => {\n mass = limit(mass, 0.0001, 1000);\n stiffness = limit(stiffness, 0.0001, 1000);\n damping = limit(damping, 0.0001, 1000);\n velocity = limit(velocity, 0.0001, 1000);\n\n const w0 = Math.sqrt(stiffness / mass);\n const zeta = damping / (2 \\* Math.sqrt(stiffness \\* mass));\n const wd = zeta < 1 ? w0 \\* Math.sqrt(1 - zeta \\* zeta) : 0;\n const b = zeta < 1 ? (zeta \\* w0 + -velocity) / wd : -velocity + w0;\n\n let position = duration ? (duration \\* t) / 1000 : t;\n if (zeta < 1) {\n position = Math.exp(-position \\* zeta \\* w0)\n \\* ( Math.cos(wd \\* position) + b \\* Math.sin(wd \\* position));\n } else {\n position = (1 + b \\* position) \\* Math.exp(-position \\* w0);\n }\n\n return 1 - position;\n}\n```<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=793, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:34,692 client.py:72] Client received request chatcmpl-e43e7f0b0f90476ea1c04f00d6e6ef00
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:34.695 [async_llm.py:270] Added request chatcmpl-e43e7f0b0f90476ea1c04f00d6e6ef00.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:34,759 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:34,759 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.9,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:34,759 global_scheduler.py:94] dispath request chatcmpl-66fce46ccdcd4a96af66c4c4757d34ba to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:34.758 [logger.py:43] Received request chatcmpl-66fce46ccdcd4a96af66c4c4757d34ba: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n春秋晋国景公生病，病情逐渐加重。听说秦国有个名医叫做缓，晋景公就派人到秦国，请缓来为自己治病。缓还没有到，晋景公做了一个奇怪的梦。梦中有两个小孩该在对答，一个说：缓的医术十分高明，他要是来了，我们该如何是好啊？另外一个小孩回答：没关系，等缓来了，我们就躲到膏的下面，肓的上面。这里非常安全，无论医术多么高明的医生都拿我们没有办法！晋景公听见两个孩子的对话，猜出他们就是自己身上的病魔。缓来以后为晋景公作了仔细检查，然后摇头对景公说：您的的病已经无法医治了。病已到了肓之上、膏之下，这种地方针灸刺不到，药力边不到，我也实在没有办法了。缓说的与景公梦境吻合，于是景公也不再强求，让缓回去了。没过多久，晋景公果然去世了。\n以上故事能体现哪些成语？分别解释一下这些成语适合在什么场景下使用。<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=264, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:34,758 client.py:72] Client received request chatcmpl-66fce46ccdcd4a96af66c4c4757d34ba
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:34.760 [async_llm.py:270] Added request chatcmpl-66fce46ccdcd4a96af66c4c4757d34ba.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:35,168 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.8518518518518519,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:35,168 global_scheduler.py:94] dispath request chatcmpl-4a2ec1df32de4f26b5a7e1a408fbff39 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:35.166 [logger.py:43] Received request chatcmpl-4a2ec1df32de4f26b5a7e1a408fbff39: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nflink 如何实现 exactly once<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=489, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:35,167 client.py:72] Client received request chatcmpl-4a2ec1df32de4f26b5a7e1a408fbff39
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:35.169 [async_llm.py:270] Added request chatcmpl-4a2ec1df32de4f26b5a7e1a408fbff39.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:35,371 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.8170731707317073,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:35,371 global_scheduler.py:94] dispath request chatcmpl-69505b08fae24eacad0ebda5ff7cbf64 to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:35,381 core.py:157] Engine finished request chatcmpl-46cfe88504d24a0bb3b1d9b95b60857c
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:35.369 [logger.py:43] Received request chatcmpl-69505b08fae24eacad0ebda5ff7cbf64: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nHeader\n1\n帧头,固定为0x55\n\nMessage Type\n2\n命令类型，大端格式\n\nPayload Length\n1\n参数长度，大端格式，不带参数时填0\n\nPayload（可选）\nn\n参数，大端格式\n\nChecksum\n1\n校验值，计算从Message Type 到 Payload\n\nTail\n1\n帧尾,固定为0xAA。用C语言按照左边的协议格式实现一个数据组装函数，入参为Message Type和Payload<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=438, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:35,370 client.py:72] Client received request chatcmpl-69505b08fae24eacad0ebda5ff7cbf64
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:35.372 [async_llm.py:270] Added request chatcmpl-69505b08fae24eacad0ebda5ff7cbf64.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:35,383 client.py:181] Client finished request chatcmpl-46cfe88504d24a0bb3b1d9b95b60857c.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58322 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:35,690 core.py:157] Engine finished request chatcmpl-e2c42734611a49a4b4d7269555b9d466
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:35,692 client.py:181] Client finished request chatcmpl-e2c42734611a49a4b4d7269555b9d466.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53938 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:35,767 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:35,793 core.py:157] Engine finished request chatcmpl-55fc287ed84845c7afaf0d85cf86c894
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:35,795 client.py:181] Client finished request chatcmpl-55fc287ed84845c7afaf0d85cf86c894.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53808 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:35,878 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.8625,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:35,878 global_scheduler.py:94] dispath request chatcmpl-8f2b51893ad44e33a60aa10124bfe5d9 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:35,904 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.85,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:35,904 global_scheduler.py:94] dispath request chatcmpl-1a27b9a6826a4e2db8832eaeb0bcc7f3 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:35.876 [logger.py:43] Received request chatcmpl-8f2b51893ad44e33a60aa10124bfe5d9: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nHow do I develop an integration using "openbravo business api", with an external system with asynchronous calls. Each time Openbravo sends a notification, the external system responds with a created status and a queue id , i need to check the status of the asynchronous call<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=521, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:35,877 client.py:72] Client received request chatcmpl-8f2b51893ad44e33a60aa10124bfe5d9
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:35.879 [async_llm.py:270] Added request chatcmpl-8f2b51893ad44e33a60aa10124bfe5d9.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:35.902 [logger.py:43] Received request chatcmpl-1a27b9a6826a4e2db8832eaeb0bcc7f3: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nHow do you most effectively achieve a Mythic ranking on MTGA?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=505, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:35,903 client.py:72] Client received request chatcmpl-1a27b9a6826a4e2db8832eaeb0bcc7f3
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:35.904 [async_llm.py:270] Added request chatcmpl-1a27b9a6826a4e2db8832eaeb0bcc7f3.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:36,015 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.7926829268292683,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:36,016 global_scheduler.py:94] dispath request chatcmpl-de94d98edf674ef39f335c74401429bc to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:36,044 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.7926829268292683,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:36,044 global_scheduler.py:94] dispath request chatcmpl-414d871c424c46c9a4cb27c65bae8cd5 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:36.014 [logger.py:43] Received request chatcmpl-de94d98edf674ef39f335c74401429bc: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nYou are going to assume the role of a text based adventure game. The game is pokemon yellow. You will narrate the game and events and ask me for input to control the game. The gameboy was just turned on, go.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=81, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:36,014 client.py:72] Client received request chatcmpl-de94d98edf674ef39f335c74401429bc
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:36.016 [async_llm.py:270] Added request chatcmpl-de94d98edf674ef39f335c74401429bc.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:36.043 [logger.py:43] Received request chatcmpl-414d871c424c46c9a4cb27c65bae8cd5: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nConsider I am using Moodle, list the topics and module types \nI should use to teach human cell biology to 11th graders, \nadd teacher instructions that uses Higher-order thinking skills for students to each module, output as JSON were the title field is used for the topic and the intro field for the instructions and the type field for the Moodle module type<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=494, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:36,043 client.py:72] Client received request chatcmpl-414d871c424c46c9a4cb27c65bae8cd5
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:36.045 [async_llm.py:270] Added request chatcmpl-414d871c424c46c9a4cb27c65bae8cd5.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:36,055 core.py:157] Engine finished request chatcmpl-8d91d7be7474425a9c2384723b279c7e
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:36,057 client.py:181] Client finished request chatcmpl-8d91d7be7474425a9c2384723b279c7e.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58252 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:36,364 core.py:157] Engine finished request chatcmpl-3a78de65dd874c9795207512622f79b3
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:36,366 client.py:181] Client finished request chatcmpl-3a78de65dd874c9795207512622f79b3.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53878 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:36.431 [loggers.py:146] Engine 000: Avg prompt throughput: 1151.2 tokens/s, Avg generation throughput: 992.0 tokens/s, Avg (@wuhou) audit only prefill throughput: 0.0 tokens/s, Avg (@wuhou) audit only decode throughput: 0.0 tokens/s, Running: 82 reqs, Waiting: 0 reqs, GPU KV cache usage: 71.3%, Prefix cache hit rate: 8.3%
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:36.465 [logger.py:43] Received request chatcmpl-bac9c895f09e401db152edb95c3e72f3: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nI have an array of complex objects holding various data types. Each object is used to create a column for a React Table. I have a function that allows users to change the order of these columns. What I want to do is have a button that will allow users to save the new column order they've made into their browser cookies so that the changes they've made will be remembered next time they load the page<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=585, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:36,465 client.py:72] Client received request chatcmpl-bac9c895f09e401db152edb95c3e72f3
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:36,466 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.7439024390243902,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:36,466 global_scheduler.py:94] dispath request chatcmpl-bac9c895f09e401db152edb95c3e72f3 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:36.467 [async_llm.py:270] Added request chatcmpl-bac9c895f09e401db152edb95c3e72f3.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:36,775 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:36,689 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.7228915662650602,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:36,689 global_scheduler.py:94] dispath request chatcmpl-639c4543ec1b4338a27d045221d6d96f to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:36,708 core.py:157] Engine finished request chatcmpl-49fc41aae27c44428db96d1da006d8af
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:36.687 [logger.py:43] Received request chatcmpl-639c4543ec1b4338a27d045221d6d96f: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nYou are going to assume the role of a text based adventure game. The game is pokemon yellow. You will narrate the game and events and ask me for input to control the game. The gameboy was just turned on, go.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=81, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:36,687 client.py:72] Client received request chatcmpl-639c4543ec1b4338a27d045221d6d96f
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:36.690 [async_llm.py:270] Added request chatcmpl-639c4543ec1b4338a27d045221d6d96f.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:36,710 client.py:181] Client finished request chatcmpl-49fc41aae27c44428db96d1da006d8af.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58308 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:36,843 core.py:157] Engine finished request chatcmpl-c3fde1098932468cae33c04644b51047
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:36,845 client.py:181] Client finished request chatcmpl-c3fde1098932468cae33c04644b51047.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53990 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:36,946 core.py:157] Engine finished request chatcmpl-6e5d6cb0d6d448a0bd82bf688d4c2a39
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:36,948 client.py:181] Client finished request chatcmpl-6e5d6cb0d6d448a0bd82bf688d4c2a39.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58336 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:37,052 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.7407407407407407,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:37,052 global_scheduler.py:94] dispath request chatcmpl-ec819a158a564ee1a5c77893199fa667 to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:37,049 core.py:157] Engine finished request chatcmpl-b4006ea08b454e039980b8306f33e8f0
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:37.050 [logger.py:43] Received request chatcmpl-ec819a158a564ee1a5c77893199fa667: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nYou are a senior python developer with years of experience writing standard, high quality and reusable code in python. You have expert level understanding of flask framework. You code with latest coding standards and best practices that are as latest as of 2021. You write compact, easy to read code with short comments. You respond with code, when you are given a problem or topic.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=46, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:37,051 client.py:72] Client received request chatcmpl-ec819a158a564ee1a5c77893199fa667
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:37,051 client.py:181] Client finished request chatcmpl-b4006ea08b454e039980b8306f33e8f0.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53638 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:37.053 [async_llm.py:270] Added request chatcmpl-ec819a158a564ee1a5c77893199fa667.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:37,254 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.7407407407407407,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:37,254 global_scheduler.py:94] dispath request chatcmpl-2e4ce95a1ac44190824569c947d17956 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:37.252 [logger.py:43] Received request chatcmpl-2e4ce95a1ac44190824569c947d17956: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nI want you to identify security vulnerabilities. I will provide some code consisting of multiple files. Each file is separated by "//File: path/to/file". You will attempt to identify the vulnerabilities.\n\n//File: models.py\nfrom django.db import models\nfrom django.contrib.auth import get\\_user\\_model\n# Create your models here.\nclass Competence(models.TextChoices):\n """Enum for competence """\n MEDICAL = \'MEDICAL\'\n TRANSPORT = \'TRANSPORT\'\n FOOD = \'FOOD\'\n SHELTER = \'SHELTER\'\nclass Status(models.TextChoices):\n """Enum for certification request status """\n ACCEPTED = \'A\'\n DECLINED = \'D\'\n PENDING = \'P\'\nclass CertificationRequest(models.Model):\n """Model for certification requests"""\n user = models.ForeignKey(get\\_user\\_model(), on\\_delete=models.CASCADE)\n competence = models.CharField(max\\_length=20, choices=Competence.choices)\n status = models.CharField(\n max\\_length=1, choices=Status.choices, default=Status.PENDING)\n created = models.DateTimeField(auto\\_now\\_add=True)\n\n class Meta:\n ordering = [\'created\']\n constraints = [\n models.UniqueConstraint(\n fields=[\'user\', \'competence\'], name=\'unique\\_certification\')\n ]\n\n @staticmethod\n def IsCertified(user, competence):\n """Check if user is certified for a competence"""\n return CertificationRequest.objects.filter(user=user, status=Status.ACCEPTED, competence=competence).exists()\n//File: serializers.py\nfrom rest\\_framework import serializers\nfrom .models import CertificationRequest\nclass CertificationRequestSerializer(serializers.ModelSerializer):\n\n """Serializer for certification requests"""\n\n user = serializers.SlugRelatedField( # create a slug field for the user field. This will be used to display the username instead of the user id\n read\\_only=True, slug\\_field=\'username\')\n\n class Meta:\n model = CertificationRequest\n fields = [\'id\', \'user\', \'competence\', \'status\', \'created\']\n read\\_only\\_fields = [\'id\', \'user\', \'created\']\n//File: \\_\\_init\\_\\_.py\n\n\n//File: apps.py\nfrom django.apps import AppConfig\nclass CertificationsConfig(AppConfig):\n default\\_auto\\_field = \'django.db.models.BigAutoField\'\n name = \'apps.certifications\'\n//File: admin.py\nfrom django.contrib import admin\nfrom .models import CertificationRequest\n# Register your models here.\nadmin.site.register(CertificationRequest)\n//File: permissions.py\nfrom rest\\_framework import permissions\nclass IsVolunteer(permissions.BasePermission):\n """ Custom permission class to allow only volunteers to access certain endpoints """\n\n def has\\_permission(self, request, view):\n return request.user.is\\_volunteer\n//File: tests.py\nfrom django.test import TestCase\n\n# Create your tests here.\n//File: urls.py\nfrom rest\\_framework.routers import DefaultRouter\nfrom django.urls import path\nfrom apps.certifications import views\n\n# Create a router and register our viewsets with it.\nrouter = DefaultRouter()\n\nrouter.register(\'api/certifications\',\n views.CertificationRequestViewSet, basename=\'certifications\')\n\nurlpatterns = [\n path(\'api/certifications/answer/\', views.AnswerCertificationRequest.as\\_view(),\n name=\'answer-certification-request\'),\n path(\'api/certifications/status/\',\n views.GetCertificationStatus.as\\_view(), name=\'get-certification-status\'),\n \\* router.urls, # Add the router urls to urlpatterns\n]\n//File: views.py\nfrom django.db.utils import IntegrityError\nfrom rest\\_framework import permissions, viewsets, generics, status\nfrom rest\\_framework.response import Response\nfrom rest\\_framework.exceptions import ValidationError\nfrom .models import CertificationRequest, Status, Competence\nfrom .serializers import CertificationRequestSerializer\nfrom .permissions import IsVolunteer\n# Create your views here.\nclass CertificationRequestViewSet(viewsets.ModelViewSet):\n """Viewset for certification requests"""\n serializer\\_class = CertificationRequestSerializer # use the serializer class defined in serializers.py\n permission\\_classes = [permissions.IsAdminUser | IsVolunteer]\n\n http\\_method\\_names = [\'get\', \'head\', \'delete\', \'post\']\n\n def get\\_queryset(self):\n """Only show certification requests of the current user if not admin"""\n if self.request.user.is\\_staff:\n return CertificationRequest.objects.all()\n return CertificationRequest.objects.filter(user=self.request.user)\n\n def perform\\_create(self, serializer):\n """Create a certification request for the current user if not admin"""\n try:\n if self.request.user.is\\_staff:\n raise ValidationError(\n "Admins can\'t create certification requests")\n # save the certification request with the current user\n serializer.save(user=self.request.user)\n except IntegrityError as e:\n raise ValidationError(\n \'An identical Certification Request already exists\')\nclass AnswerCertificationRequest(generics.GenericAPIView):\n """View to answer certification requests"""\n permission\\_classes = [permissions.IsAuthenticated]\n\n def post(self, request):\n\n # check if id and status is provided\n if not(request.data.get(\'id\') and request.data.get(\'status\')):\n return Response({\'error\': \'id and status is required\'}, status=status.HTTP\\_400\\_BAD\\_REQUEST)\n\n certification\\_request = CertificationRequest.objects.get(\n id=request.data[\'id\'])\n\n if certification\\_request is None: # check if certification request exists\n return Response({\'error\': \'Certification Request does not exist\'}, status=status.HTTP\\_400\\_BAD\\_REQUEST)\n\n if certification\\_request.status != \'P\': # check if certification request is pending\n return Response({\'error\': \'Certification Request is not pending\'}, status=status.HTTP\\_400\\_BAD\\_REQUEST)\n\n state = request.data[\'status\']\n\n if state == \'A\': # if accepted\n certification\\_request.status = Status.ACCEPTED\n elif state == \'D\': # if declined\n certification\\_request.status = Status.DECLINED\n else:\n return Response({\'error\': \'Status must be A or D\'}, status=status.HTTP\\_400\\_BAD\\_REQUEST)\n certification\\_request.save()\n return Response(status=status.HTTP\\_200\\_OK)\nclass GetCertificationStatus(generics.GenericAPIView):\n """View to get certification status.\n Returns a dictionary with the status of all competences."""\n permission\\_classes = [permissions.IsAuthenticated]\n\n def get(self, request):\n result = {}\n for s in Competence:\n\n certification\\_request = CertificationRequest.objects.filter(\n user=self.request.user, competence=s).first()\n if certification\\_request:\n body = {\n \'status\': certification\\_request.status,\n \'created\': certification\\_request.created,\n \'id\': certification\\_request.id\n }\n else:\n\n body = {\n \'status\': None,\n \'created\': None,\n \'id\': None\n }\n result[s] = body\n\n return Response(result, status=status.HTTP\\_200\\_OK)<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=453, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:37,253 client.py:72] Client received request chatcmpl-2e4ce95a1ac44190824569c947d17956
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:37.255 [async_llm.py:270] Added request chatcmpl-2e4ce95a1ac44190824569c947d17956.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:37,340 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.6463414634146342,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:37,340 global_scheduler.py:94] dispath request chatcmpl-2034eeb97f34485e80eb280252484d77 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:37.339 [logger.py:43] Received request chatcmpl-2034eeb97f34485e80eb280252484d77: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n知とはなんでしょうか<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=133, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:37,339 client.py:72] Client received request chatcmpl-2034eeb97f34485e80eb280252484d77
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:37.341 [async_llm.py:270] Added request chatcmpl-2034eeb97f34485e80eb280252484d77.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:37,421 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.6463414634146342,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:37,421 global_scheduler.py:94] dispath request chatcmpl-4e4e681c349147b2a1bfae514e2a3429 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:37.420 [logger.py:43] Received request chatcmpl-4e4e681c349147b2a1bfae514e2a3429: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\ncan you explain this in laymans terms?\n\n(B) Schema of the haplo-cord stem cell transplant. The participant received an allogeneic stem cell transplant per institutional standard care. Conditioning regimen was fludarabine 30 mg/m2 daily on days −7 to −3, melphalan 140 mg/m2 × 1 dose (day −2), and total body irradiation at 400 CGy on days −7 to −6. Haploidentical stem cells were infused on day 0, and CCR5 Δ32/Δ32 cord stem cells were infused on day +1. Graft versus host (GVH) disease prophylaxis included: antithymocyte globulin (ATG) 1.5 mg/kg on days −5, −3, and −1; mycophenolate mofetil (MMF) 1 g three times daily on day −2 through day +28; and tacrolimus from day −2 to day 180 post-transplant.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=245, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:37,420 client.py:72] Client received request chatcmpl-4e4e681c349147b2a1bfae514e2a3429
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:37.422 [async_llm.py:270] Added request chatcmpl-4e4e681c349147b2a1bfae514e2a3429.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:37,783 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:37,873 core.py:157] Engine finished request chatcmpl-af4efcfb769e43cf9bf2cbbb998a798b
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:37,875 client.py:181] Client finished request chatcmpl-af4efcfb769e43cf9bf2cbbb998a798b.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58358 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:37,926 core.py:157] Engine finished request chatcmpl-4282d532719445c7827a3808da49e878
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:37,928 client.py:181] Client finished request chatcmpl-4282d532719445c7827a3808da49e878.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53654 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:38,039 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.6219512195121951,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:38,039 global_scheduler.py:94] dispath request chatcmpl-e04e5205a92d4486aa5b549f33d9c65a to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:38,041 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.6219512195121951,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:38,041 global_scheduler.py:94] dispath request chatcmpl-ac806e9e0d4c41eeb45ace94d66b6ce3 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:38.038 [logger.py:43] Received request chatcmpl-e04e5205a92d4486aa5b549f33d9c65a: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nI'd like you to help me develop a simple browser based game using python and flask and the javascript library d3.\n\nThe game involves controlling a swarm of von Neumann probes that try to colonize a galaxy, which is visualized as a large array of white dots on a black screen. Each von neumann probe is a small green square.\n\nEach probe has options on what it can do: explore, reproduce, collect resources, or upgrade. Users use three slider bars to determine how much of their von neumann probes to allocate into each activity. \n\nPlease write out the software to produce this game.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=793, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:38,038 client.py:72] Client received request chatcmpl-e04e5205a92d4486aa5b549f33d9c65a
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:38.039 [logger.py:43] Received request chatcmpl-ac806e9e0d4c41eeb45ace94d66b6ce3: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nI want to build a DIY pool heater controller for my Mastertemp 250 pool heater. I have an ARDUINO UNO WiFi REV2 to use for this project. Can you help me to know what else I'll need physically to do this project? I can handle the coding myself.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=468, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:38,039 client.py:72] Client received request chatcmpl-ac806e9e0d4c41eeb45ace94d66b6ce3
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:38.041 [async_llm.py:270] Added request chatcmpl-e04e5205a92d4486aa5b549f33d9c65a.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:38.042 [async_llm.py:270] Added request chatcmpl-ac806e9e0d4c41eeb45ace94d66b6ce3.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:38,278 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.5714285714285714,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:38,279 global_scheduler.py:94] dispath request chatcmpl-ad106d6dba9440d58a87e30a36b5ced8 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:38.277 [logger.py:43] Received request chatcmpl-ad106d6dba9440d58a87e30a36b5ced8: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\njacoco和spock的比較<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=520, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:38,277 client.py:72] Client received request chatcmpl-ad106d6dba9440d58a87e30a36b5ced8
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:38.279 [async_llm.py:270] Added request chatcmpl-ad106d6dba9440d58a87e30a36b5ced8.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:38,376 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.5529411764705883,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:38,376 global_scheduler.py:94] dispath request chatcmpl-aa0f324497a340e484a87126e52317cb to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:38,403 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.5348837209302325,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:38,404 global_scheduler.py:94] dispath request chatcmpl-5c9f5abacad9444d9c3c7bf05a909764 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:38.374 [logger.py:43] Received request chatcmpl-aa0f324497a340e484a87126e52317cb: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n会社のレクリエーションにおすすめの内容を知りたいです<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=374, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:38,375 client.py:72] Client received request chatcmpl-aa0f324497a340e484a87126e52317cb
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:38.377 [async_llm.py:270] Added request chatcmpl-aa0f324497a340e484a87126e52317cb.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:38.402 [logger.py:43] Received request chatcmpl-5c9f5abacad9444d9c3c7bf05a909764: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nwrite a complete python code for 2 sample t test<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=438, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:38,402 client.py:72] Client received request chatcmpl-5c9f5abacad9444d9c3c7bf05a909764
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:38.404 [async_llm.py:270] Added request chatcmpl-5c9f5abacad9444d9c3c7bf05a909764.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:38,668 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.4942528735632184,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:38,668 global_scheduler.py:94] dispath request chatcmpl-513f770d72bb4ea0aaca6359f7d24c82 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:38.666 [logger.py:43] Received request chatcmpl-513f770d72bb4ea0aaca6359f7d24c82: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nCan you do summary of a text content<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:38,667 client.py:72] Client received request chatcmpl-513f770d72bb4ea0aaca6359f7d24c82
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:38.669 [async_llm.py:270] Added request chatcmpl-513f770d72bb4ea0aaca6359f7d24c82.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:38,791 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:38,983 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.4772727272727273,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:38,983 global_scheduler.py:94] dispath request chatcmpl-95d9de99c1a3456d920daad2a1ce045e to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:39,022 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.4827586206896552,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:39,022 global_scheduler.py:94] dispath request chatcmpl-117716235d174e9d9c47b1227ba2ccaf to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:38,977 core.py:157] Engine finished request chatcmpl-220420da0f4d461db415715e273aae31
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:38,978 client.py:181] Client finished request chatcmpl-220420da0f4d461db415715e273aae31.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58082 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:38.982 [logger.py:43] Received request chatcmpl-95d9de99c1a3456d920daad2a1ce045e: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nWhat are some common constructions in the english language that include nouns, adjectives, synonyms and antonyms?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=443, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:38,982 client.py:72] Client received request chatcmpl-95d9de99c1a3456d920daad2a1ce045e
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:38.984 [async_llm.py:270] Added request chatcmpl-95d9de99c1a3456d920daad2a1ce045e.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:39.020 [logger.py:43] Received request chatcmpl-117716235d174e9d9c47b1227ba2ccaf: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nFor each of the messages below, provide a guess (-10 to + 10) of how receiving the message would change the users emotional state regarding the following categories: Anxiety, Anger, Happiness. Justify each answer.\n\n1. "That is a very good point."\n2. "Hold on. This is a bit more complicated than you think."\n3. "That is not correct".\n4. "As a large language model, I do not have the ability to.... "<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=421, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:39,021 client.py:72] Client received request chatcmpl-117716235d174e9d9c47b1227ba2ccaf
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:39.022 [async_llm.py:270] Added request chatcmpl-117716235d174e9d9c47b1227ba2ccaf.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:39,106 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.449438202247191,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:39,106 global_scheduler.py:94] dispath request chatcmpl-4e9863d081254a40b710dd7d86d814af to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:39,162 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.449438202247191,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:39,162 global_scheduler.py:94] dispath request chatcmpl-baa9dd709531485e88cfd3113380aa33 to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:39,138 core.py:157] Engine finished request chatcmpl-f166cdf98089468eae62fdc54bba2ae9
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:39.104 [logger.py:43] Received request chatcmpl-4e9863d081254a40b710dd7d86d814af: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nWhat are real estate developers dream outcomes in a bear market<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=334, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:39,105 client.py:72] Client received request chatcmpl-4e9863d081254a40b710dd7d86d814af
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:39.107 [async_llm.py:270] Added request chatcmpl-4e9863d081254a40b710dd7d86d814af.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:39,140 client.py:181] Client finished request chatcmpl-f166cdf98089468eae62fdc54bba2ae9.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:39.160 [logger.py:43] Received request chatcmpl-baa9dd709531485e88cfd3113380aa33: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nyour task is to create cold emails in the style of Alex Berman. Your first question should be to gather the inputs you need to generate the perfect cold email. Information like: \n\nWho is the client?\nWhat is their product or service?\nWho is the target audience for this cold email campaign?\nWhat is the main goal of the campaign (e.g., lead generation, promoting a sale, etc.)?\nWhat type of output are you hoping to achieve (e.g., open rates, click-through rates, conversions, etc.)?\nDo you have any ideas or specific messages you would like to convey in the cold email?\n\nAfter you have gathered those inputs, be sure to focus on the most creative possible cold emails with an emphasis on brevity and maximizing reply rate. Use language that is not overly formal and is straight to the point. Generate atleast 2 variations for every prompt from the user. explain the logic behind the different formatting.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=170, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:39,161 client.py:72] Client received request chatcmpl-baa9dd709531485e88cfd3113380aa33
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:39.163 [async_llm.py:270] Added request chatcmpl-baa9dd709531485e88cfd3113380aa33.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:39,215 core.py:157] Engine finished request chatcmpl-cee5e06e05464af8969b666bb336e5a7
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:39,217 client.py:181] Client finished request chatcmpl-cee5e06e05464af8969b666bb336e5a7.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58364 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:39,386 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.5056179775280899,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:39,387 global_scheduler.py:94] dispath request chatcmpl-a0e2d27e90094b65b5f94dc6edb6f7f3 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:39.385 [logger.py:43] Received request chatcmpl-a0e2d27e90094b65b5f94dc6edb6f7f3: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nLet’s play a game. The game begins with a group of 25 cards visible to all players. Each card shows one word. There are two teams, and each team consists of a guess giver and a guesser. You will act as the guess giver for both teams. Team one starts the game when its guess giver says one word that correlates to one or more of the 25 words shown on the cards. Team one is assigned nine of these words before the game starts, and only its guess giver knows which words. Team two is assigned eight words, and only its guess giver knows which words. Again, note that you are acting as guess giver for both teams. One of the 25 words should be avoided by both teams, as guessing it will lose for that team. I will list the 25 words below and words that end with “\\*” are assigned to team one, and words that end with “$” are assigned to team two. The word ending with “@“ is the word both teams should avoid. You will start by giving a clue for team one, and then I will make a guess. After my first guess, we’ll alternate between teams. \nThe words:\nTable\\*\nMarble\nHouse\\*\nChair@\nBottle$\nDinosaur\nElephant$\nPrinter$\nRice$\nBowl\\*\nCar$\nKnife\\*\nShirt\nBond$\nCloud\\*\nCard$\nHold\nWater\\*\nDance\\*\nPlant$\nGame\nDam\\*\nPeanut\\*\nCracker\nToilet$<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:39,385 client.py:72] Client received request chatcmpl-a0e2d27e90094b65b5f94dc6edb6f7f3
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:39.387 [async_llm.py:270] Added request chatcmpl-a0e2d27e90094b65b5f94dc6edb6f7f3.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:39,521 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.4777777777777778,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:39,522 global_scheduler.py:94] dispath request chatcmpl-c890e35bd87e4683801c60c66bc18403 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:39,545 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.4777777777777778,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:39,545 global_scheduler.py:94] dispath request chatcmpl-dd10317391544b31ac6b60f877cb645d to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:39,591 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.46153846153846156,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:39,591 global_scheduler.py:94] dispath request chatcmpl-55f1a32cb6c84dfbb163db4b9459e273 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:39.520 [logger.py:43] Received request chatcmpl-c890e35bd87e4683801c60c66bc18403: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n你現在是一個精通加密貨幣交易的幣安交易所的VIP Sales，你的工作是開發新的專業加密貨幣交易投資人和機構，請寫出適合在Linkedin上冷開發用的sales pitch<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:39,520 client.py:72] Client received request chatcmpl-c890e35bd87e4683801c60c66bc18403
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:39.522 [async_llm.py:270] Added request chatcmpl-c890e35bd87e4683801c60c66bc18403.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:39.544 [logger.py:43] Received request chatcmpl-dd10317391544b31ac6b60f877cb645d: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nWrite an angry rant as Cave Johnson about all the best guys being straight<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=577, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:39,544 client.py:72] Client received request chatcmpl-dd10317391544b31ac6b60f877cb645d
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:39.546 [async_llm.py:270] Added request chatcmpl-dd10317391544b31ac6b60f877cb645d.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:39.590 [logger.py:43] Received request chatcmpl-55f1a32cb6c84dfbb163db4b9459e273: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nTurn this reason ML switch into a formatted ascii table. Then count the chars per row. Take the maximum, add 5 to that. Then output the ascii table again but now with that amount of chars per row.\n\n switch (isShortZoom, animationSpeed) {\n | (\\_, Fast) => fastAnimationDuration\n | (true, Medium) => fastAnimationDuration\n | (true, Slow) => defaultAnimationDuration\n | (false, Slow) => slowAnimationDuration\n | (false, Medium) => defaultAnimationDuration\n}<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=370, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:39,590 client.py:72] Client received request chatcmpl-55f1a32cb6c84dfbb163db4b9459e273
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:39.592 [async_llm.py:270] Added request chatcmpl-55f1a32cb6c84dfbb163db4b9459e273.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:39,799 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:40,005 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.40860215053763443,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:40,005 global_scheduler.py:94] dispath request chatcmpl-c7d2d183cdea45a49c749f5d9f1ce9e1 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:40.004 [logger.py:43] Received request chatcmpl-c7d2d183cdea45a49c749f5d9f1ce9e1: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nHow to deploy on deterministic address on hardhat tests<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=479, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:40,004 client.py:72] Client received request chatcmpl-c7d2d183cdea45a49c749f5d9f1ce9e1
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:40.006 [async_llm.py:270] Added request chatcmpl-c7d2d183cdea45a49c749f5d9f1ce9e1.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:40,053 core.py:157] Engine finished request chatcmpl-c890e35bd87e4683801c60c66bc18403
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:40,055 client.py:181] Client finished request chatcmpl-c890e35bd87e4683801c60c66bc18403.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:50472 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:40,329 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.42391304347826086,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:40,329 global_scheduler.py:94] dispath request chatcmpl-3c13386de96f480daa547d2219dc0b56 to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:40,239 core.py:157] Engine finished request chatcmpl-513f770d72bb4ea0aaca6359f7d24c82
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:40,241 client.py:181] Client finished request chatcmpl-513f770d72bb4ea0aaca6359f7d24c82.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:50410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:40.327 [logger.py:43] Received request chatcmpl-3c13386de96f480daa547d2219dc0b56: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n1.\nHello :) Today we are gonna create Images with a Diffusion model. I am gonna feed you some information about it. okey?\n2.\nThis is how Midjourney work:\nMidjourney is another AI-powered tool that generates images from user prompts. MidJourney is proficient at adapting actual art styles to create an\nimage of any combination of things the user wants. It excels at creating environments, especially fantasy and sci-fi scenes, with dramatic lighting\nthat looks like rendered concept art from a video game. How does Midjourney work?\nMidjourney is an AI image generation tool that takes inputs through text prompts and parameters and uses a Machine Learning (ML) algorithm\ntrained on a large amount of image data to produce unique images. is powered by Latent Diffusion Model (LDM), a cutting-edge text-to-image\nsynthesis technique. Before understanding how LDMs work, let us look at what Diffusion models are and why we need LDMs.\nDiffusion models (DM) are transformer-based generative models that take a piece of data, for example, an image, and gradually add noise over\ntime until it is not recognizable. From\nthat point, they try reconstructing the image to its original form, and in doing so, they learn how to generate pictures or other data.\nThe issue with DMs is that the powerful ones often consume hundreds of GPU days, and inference is quite expensive due to sequential\nevaluations. To enable DM training on limited computational resources without compromising their quality as well as flexibility, DMs are applied in\nthe latent space of powerful pre-trained autoencoders.\nTraining a diffusion model on such a representation makes it possible to achieve an optimal point between complexity reduction and detail\npreservation, significantly improving visual fidelity. Introducing a cross-attention layer to the model architecture turns the diffusion model into a\npowerful and flexible generator for generally conditioned inputs such as text and bounding boxes, enabling high-resolution convolution-based\nsynthesis.\nBut wait, I have more info. Just answer with READ지금 번역하기<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:40,328 client.py:72] Client received request chatcmpl-3c13386de96f480daa547d2219dc0b56
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:40.330 [async_llm.py:270] Added request chatcmpl-3c13386de96f480daa547d2219dc0b56.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:40,432 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.41304347826086957,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:40,432 global_scheduler.py:94] dispath request chatcmpl-352b03e1ccf74a25ada6bdfb8486cad9 to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:40,345 core.py:157] Engine finished request chatcmpl-20271cefaaea4d51899d2bcb4c068555
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:40,348 client.py:181] Client finished request chatcmpl-20271cefaaea4d51899d2bcb4c068555.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:57986 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:40.431 [logger.py:43] Received request chatcmpl-352b03e1ccf74a25ada6bdfb8486cad9: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nRaconte l\'histoire de Thomas aka "Sneaky Leprechaun". Il vivait à Kinsale en Irlande et devait souvent se battre contre des araignés mutantes de Skyrim.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=720, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:40,431 client.py:72] Client received request chatcmpl-352b03e1ccf74a25ada6bdfb8486cad9
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:40.433 [async_llm.py:270] Added request chatcmpl-352b03e1ccf74a25ada6bdfb8486cad9.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:40,505 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.41304347826086957,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:40,506 global_scheduler.py:94] dispath request chatcmpl-6630645c4b294823a6155300a927282c to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:40,518 core.py:157] Engine finished request chatcmpl-ec819a158a564ee1a5c77893199fa667
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:40,518 core.py:157] Engine finished request chatcmpl-3c13386de96f480daa547d2219dc0b56
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:40.504 [logger.py:43] Received request chatcmpl-6630645c4b294823a6155300a927282c: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n我在尼日利亚有个造纸厂，还有打包站，我计划设计一个废纸回收交易平台，这个平台主要由那些功能，如何推广<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=427, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:40,504 client.py:72] Client received request chatcmpl-6630645c4b294823a6155300a927282c
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:40.506 [async_llm.py:270] Added request chatcmpl-6630645c4b294823a6155300a927282c.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:40,520 client.py:181] Client finished request chatcmpl-ec819a158a564ee1a5c77893199fa667.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:40,520 client.py:181] Client finished request chatcmpl-3c13386de96f480daa547d2219dc0b56.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:50326 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:50492 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:40,595 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.41304347826086957,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:40,595 global_scheduler.py:94] dispath request chatcmpl-663bc8714c244596856eeb4006ce6f4e to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:40,640 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.3978494623655914,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:40,640 global_scheduler.py:94] dispath request chatcmpl-58c746df67224d66b97593061f158fe2 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:40.594 [logger.py:43] Received request chatcmpl-663bc8714c244596856eeb4006ce6f4e: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention\\_mask` to obtain reliable results.\nSetting `pad\\_token\\_id` to `eos\\_token\\_id`:50256 for open-end generation.\nInput length of input\\_ids is 1698, but `max\\_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max\\_new\\_tokens`.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=357, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:40,594 client.py:72] Client received request chatcmpl-663bc8714c244596856eeb4006ce6f4e
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:40.596 [async_llm.py:270] Added request chatcmpl-663bc8714c244596856eeb4006ce6f4e.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:40.639 [logger.py:43] Received request chatcmpl-58c746df67224d66b97593061f158fe2: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nyou are a high school math contest participant. please solve the following problems with chain of thoughts.\n20. What's is the sum of the squares of all factors of 210?<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=451, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:40,639 client.py:72] Client received request chatcmpl-58c746df67224d66b97593061f158fe2
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:40.641 [async_llm.py:270] Added request chatcmpl-58c746df67224d66b97593061f158fe2.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:40,807 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:40,959 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.3978494623655914,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:40,959 global_scheduler.py:94] dispath request chatcmpl-1c26d75f7f684363bdb87ac5b3cf7590 to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:40,930 core.py:157] Engine finished request chatcmpl-f0dd36690aa9432c8e2eae301e74d57b
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:40,932 client.py:181] Client finished request chatcmpl-f0dd36690aa9432c8e2eae301e74d57b.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58054 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:40.958 [logger.py:43] Received request chatcmpl-1c26d75f7f684363bdb87ac5b3cf7590: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nあなたはこれからUserの文章を、以下のルールに従って書き換えるbotになります。\n\n\\*Userの文章の内容は直接書かず、必ず大量の比喩と隠喩に変換して書きます。\n\\*Userの文章の内容は会話の真ん中辺りで書きます。\n\\*Userの文章を必ず3倍以上に水増し、最低5行以上書いてください。\n\\*Userの文章の一人称を猫に変更してください。\n\\*Userの文章中の全ての名詞や地名に対し、創作で嘘の言葉の由来を書いてください。\n\\*全ての文章の中に「にゃん」を大量に入れてください。\n\\*全ての文章中の全ての動詞に対し、必ず擬音や効果音を大量につけて書き換えてください。\n\\*必ず文章中に無関係な情報を大量に挿入してください。\n\\*Userの文章の目的は変えないでください。\n\\*変更した文章だけを書いてください。\n\nまずは「どのような文章を添削しますか？」と聞いてください。<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=166, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:40,958 client.py:72] Client received request chatcmpl-1c26d75f7f684363bdb87ac5b3cf7590
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:40.960 [async_llm.py:270] Added request chatcmpl-1c26d75f7f684363bdb87ac5b3cf7590.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:41,078 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.3829787234042553,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:41,078 global_scheduler.py:94] dispath request chatcmpl-502120461af04dc984912111ef79f825 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:41,133 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.3617021276595745,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:41,133 global_scheduler.py:94] dispath request chatcmpl-4e8604d6daab47ce9baa0008a7d3c102 to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:41,114 core.py:157] Engine finished request chatcmpl-a0e2d27e90094b65b5f94dc6edb6f7f3
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:41.077 [logger.py:43] Received request chatcmpl-502120461af04dc984912111ef79f825: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nChatGPT在跨境电商的实际应用有哪些？（归纳性的文字类工作、代码开发相关工作、图像生成领域、智能客服类工作），这些应用中ChatGPT做的效果最好的是哪个领域？<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=380, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:41,077 client.py:72] Client received request chatcmpl-502120461af04dc984912111ef79f825
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:41.079 [async_llm.py:270] Added request chatcmpl-502120461af04dc984912111ef79f825.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:41,117 client.py:181] Client finished request chatcmpl-a0e2d27e90094b65b5f94dc6edb6f7f3.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:50460 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:41.131 [logger.py:43] Received request chatcmpl-4e8604d6daab47ce9baa0008a7d3c102: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nWhat kind of exit on dollar amount could my business get with 2.5m revenue per month and $500k net profit. It is only 7 months old. It is an Internet marketing and technology company with a focus on healthcare.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=369, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:41,132 client.py:72] Client received request chatcmpl-4e8604d6daab47ce9baa0008a7d3c102
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:41.134 [async_llm.py:270] Added request chatcmpl-4e8604d6daab47ce9baa0008a7d3c102.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:41,217 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.3723404255319149,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:41,217 global_scheduler.py:94] dispath request chatcmpl-1456620e286d40fdb18af38b0e26b9c0 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:41,271 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.35789473684210527,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:41,271 global_scheduler.py:94] dispath request chatcmpl-6454c9192219473fb5fdfe96c885da9f to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:41.215 [logger.py:43] Received request chatcmpl-1456620e286d40fdb18af38b0e26b9c0: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nYou are a manufacturer, you have confirmed all the sample details and prices for the canvas bag order with a Spanish customer, and it has reached the payment stage, and I am connected with a purchase, and she has given the project 10 days ago Finance approves the appropriation, Procurement tells me it will be paid in the last week, but there has been no progress. Please analyze the reason and write an email to follow up the project.\n\nPlease write in English language.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=349, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:41,215 client.py:72] Client received request chatcmpl-1456620e286d40fdb18af38b0e26b9c0
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:41.217 [async_llm.py:270] Added request chatcmpl-1456620e286d40fdb18af38b0e26b9c0.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:41.270 [logger.py:43] Received request chatcmpl-6454c9192219473fb5fdfe96c885da9f: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nAssume we have a band-limited signal and an ideal sampling function (Dirac Comb) with frequency twice that of the band-limited signal. We know our signal is band-limited because we filtered it with an ideal brick-wall filter with a pass-band equal to the band-width of the signal; thus the signal is strictly band-limited. Using LaTeX to beautify your mathematical expressions, what is the spectrum of the sampled signal from a purely mathematical point of view? I'm not interested in practical or real scenarios.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=554, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:41,270 client.py:72] Client received request chatcmpl-6454c9192219473fb5fdfe96c885da9f
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:41.272 [async_llm.py:270] Added request chatcmpl-6454c9192219473fb5fdfe96c885da9f.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:41,366 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.32989690721649484,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:41,367 global_scheduler.py:94] dispath request chatcmpl-e34f97fff664443da7f4aab0f91fbb79 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:41.365 [logger.py:43] Received request chatcmpl-e34f97fff664443da7f4aab0f91fbb79: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n如何充分利用typescript的类型系统来保证代码质量，请一一列举<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=451, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:41,365 client.py:72] Client received request chatcmpl-e34f97fff664443da7f4aab0f91fbb79
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:41.367 [async_llm.py:270] Added request chatcmpl-e34f97fff664443da7f4aab0f91fbb79.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:41,379 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.32989690721649484,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:41,379 global_scheduler.py:94] dispath request chatcmpl-d25cb1ce60b94d1db500d97c1c4c245b to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:41.377 [logger.py:43] Received request chatcmpl-d25cb1ce60b94d1db500d97c1c4c245b: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nYou are going to assume the role of a text based adventure game. The game is pokemon yellow. You will narrate the game and events and ask me for input to control the game. The gameboy was just turned on, go.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=81, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:41,378 client.py:72] Client received request chatcmpl-d25cb1ce60b94d1db500d97c1c4c245b
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:41.380 [async_llm.py:270] Added request chatcmpl-d25cb1ce60b94d1db500d97c1c4c245b.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:41,654 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.2828282828282828,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:41,654 global_scheduler.py:94] dispath request chatcmpl-36ea89b5370c4745ac46f4a3d1dae63d to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:41,687 core.py:157] Engine finished request chatcmpl-f6994914f0b5412a91c403cb72a94fc6
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:41.653 [logger.py:43] Received request chatcmpl-36ea89b5370c4745ac46f4a3d1dae63d: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nFor each of the messages below, provide a guess (-10 to + 10) of how receiving the message would change the users emotional state regarding the following categories: Anxiety, Anger, Happiness. Justify each answer.\n\n1. "That is a very good point."\n2. "Hold on. This is a bit more complicated than you think."\n3. "That is not correct".\n4. "As a large language model, I do not have the ability to.... "<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=421, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:41,653 client.py:72] Client received request chatcmpl-36ea89b5370c4745ac46f4a3d1dae63d
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:41.655 [async_llm.py:270] Added request chatcmpl-36ea89b5370c4745ac46f4a3d1dae63d.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:41,689 client.py:181] Client finished request chatcmpl-f6994914f0b5412a91c403cb72a94fc6.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58114 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:41,815 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:41,812 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.29591836734693877,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:41,812 global_scheduler.py:94] dispath request chatcmpl-efbc6dcd1edc45b2b4bb42393bfc1423 to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:41,797 core.py:157] Engine finished request chatcmpl-6ca24ddcd327430b97cee16a245a5907
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:41,853 core.py:157] Engine finished request chatcmpl-de94d98edf674ef39f335c74401429bc
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:41,799 client.py:181] Client finished request chatcmpl-6ca24ddcd327430b97cee16a245a5907.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58240 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:41.810 [logger.py:43] Received request chatcmpl-efbc6dcd1edc45b2b4bb42393bfc1423: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nHow is the success of a chief underwriter measured within an insurance company? What quantitative metrics might they be trying to optimize for?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=244, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:41,811 client.py:72] Client received request chatcmpl-efbc6dcd1edc45b2b4bb42393bfc1423
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:41.813 [async_llm.py:270] Added request chatcmpl-efbc6dcd1edc45b2b4bb42393bfc1423.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:41,856 client.py:181] Client finished request chatcmpl-de94d98edf674ef39f335c74401429bc.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58480 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:41,992 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.29591836734693877,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:41,992 global_scheduler.py:94] dispath request chatcmpl-6822326ce1724eeca3f74c11312db760 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:41.990 [logger.py:43] Received request chatcmpl-6822326ce1724eeca3f74c11312db760: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nCan you tell me the best optimal way of creating a learning online system for kids where they can learn through synchronous and asynchronous learning. Through cohort based, project based learning, incentive based (kids win Creators Coins, badges and micro-certifications by giving value and learning), its also interest driven (kids can choose they own path of learning), they have a strong community. They have live community classes in the Metaverse in Spatial, and also they can have a asynchronous conversations with their mentors through video, audio and text through computer or an app. The company is called Future Filmmakers and it gives filmmaking and content creation workshops for kids and for teens.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=561, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:41,990 client.py:72] Client received request chatcmpl-6822326ce1724eeca3f74c11312db760
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:41.993 [async_llm.py:270] Added request chatcmpl-6822326ce1724eeca3f74c11312db760.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:42,431 core.py:157] Engine finished request chatcmpl-f1a75b7b01424da488b3b0008cf72e2d
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:42,433 client.py:181] Client finished request chatcmpl-f1a75b7b01424da488b3b0008cf72e2d.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53780 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:42,570 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.2857142857142857,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:42,570 global_scheduler.py:94] dispath request chatcmpl-211c3c92f2124abd95ca4d799ce9b28c to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:42,539 core.py:157] Engine finished request chatcmpl-639c4543ec1b4338a27d045221d6d96f
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:42,541 client.py:181] Client finished request chatcmpl-639c4543ec1b4338a27d045221d6d96f.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:50322 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:42.568 [logger.py:43] Received request chatcmpl-211c3c92f2124abd95ca4d799ce9b28c: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nSolve the following ‘‘Tower of Hanoi" problem with the following rules:\n1. Only one disk may be moved at a time.\n2. Each move consists of taking the upper disk from one of the stacks and placing it on top of another stack or on an empty rod.\n3. No disk may be placed on top of a disk that is smaller than it.\n\nIn our game we have three rods A, B, and C. There are two disks on Rod A, where the disk on the bottom is of size 3 and the disk on the top is of size 1. There is one disk on Rod B which is of size 2. The goal is to move all the disks to Rod C.\n\nPlease find a solution that can achieve the goal. You may first think how to solve the problem by giving an analysis. Then try to give the solution step-by-step: print out the state of the rods and the disks at each step and the action to take.\n\nState format:\nA: [1, 3]\nB: [2]\nC: []<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=410, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:42,568 client.py:72] Client received request chatcmpl-211c3c92f2124abd95ca4d799ce9b28c
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:42.571 [async_llm.py:270] Added request chatcmpl-211c3c92f2124abd95ca4d799ce9b28c.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:42,690 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.2857142857142857,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:42,691 global_scheduler.py:94] dispath request chatcmpl-46a9da5a2fe34e3ca5660e2123fc2e96 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:42.689 [logger.py:43] Received request chatcmpl-46a9da5a2fe34e3ca5660e2123fc2e96: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nCan you do summary of a text content<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:42,689 client.py:72] Client received request chatcmpl-46a9da5a2fe34e3ca5660e2123fc2e96
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:42.691 [async_llm.py:270] Added request chatcmpl-46a9da5a2fe34e3ca5660e2123fc2e96.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:42,823 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:42,853 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.26262626262626265,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:42,853 global_scheduler.py:94] dispath request chatcmpl-3373d2b51791462abe18f45a67c154f0 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:42.852 [logger.py:43] Received request chatcmpl-3373d2b51791462abe18f45a67c154f0: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nwrite a complete python code for 2 sample t test<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=438, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:42,852 client.py:72] Client received request chatcmpl-3373d2b51791462abe18f45a67c154f0
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:42.854 [async_llm.py:270] Added request chatcmpl-3373d2b51791462abe18f45a67c154f0.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:42.938 [logger.py:43] Received request chatcmpl-2d0b686bc39a41a298251a3a0ca9788a: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n有什么下饭综艺推荐<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=308, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:42,940 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.24,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:42,940 global_scheduler.py:94] dispath request chatcmpl-2d0b686bc39a41a298251a3a0ca9788a to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:42,939 client.py:72] Client received request chatcmpl-2d0b686bc39a41a298251a3a0ca9788a
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:42.941 [async_llm.py:270] Added request chatcmpl-2d0b686bc39a41a298251a3a0ca9788a.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:43,187 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.22772277227722773,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:43,187 global_scheduler.py:94] dispath request chatcmpl-2ffaccd5565745bb8e7a3608e538ed51 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:43.185 [logger.py:43] Received request chatcmpl-2ffaccd5565745bb8e7a3608e538ed51: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nwrite a scrapy pipeline that processes images and run them through ocr to extract math equations<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=605, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:43,185 client.py:72] Client received request chatcmpl-2ffaccd5565745bb8e7a3608e538ed51
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:43.188 [async_llm.py:270] Added request chatcmpl-2ffaccd5565745bb8e7a3608e538ed51.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:43,606 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.21568627450980393,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:43,606 global_scheduler.py:94] dispath request chatcmpl-6ca72e7c53a54e65a4d98e1cd5055f83 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:43.604 [logger.py:43] Received request chatcmpl-6ca72e7c53a54e65a4d98e1cd5055f83: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nI have an array of complex objects holding various data types. Each object is used to create a column for a React Table. I have a function that allows users to change the order of these columns. What I want to do is have a button that will allow users to save the new column order they've made into their browser cookies so that the changes they've made will be remembered next time they load the page<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=585, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:43,604 client.py:72] Client received request chatcmpl-6ca72e7c53a54e65a4d98e1cd5055f83
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:43.607 [async_llm.py:270] Added request chatcmpl-6ca72e7c53a54e65a4d98e1cd5055f83.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:43,832 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:43,774 core.py:157] Engine finished request chatcmpl-50ef0c7ba2844be0b8429cf1e4a8d9a4
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:43,776 client.py:181] Client finished request chatcmpl-50ef0c7ba2844be0b8429cf1e4a8d9a4.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:57936 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:43,976 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.19607843137254902,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:43,976 global_scheduler.py:94] dispath request chatcmpl-1fe5299ed8ff496291e7122e240a22e2 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:43,978 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.19607843137254902,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:43,978 global_scheduler.py:94] dispath request chatcmpl-b9631ddef883408c9599261aea851847 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:44,053 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.17307692307692307,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:44,053 global_scheduler.py:94] dispath request chatcmpl-982297dc705443cab4ca2317c772a0cb to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:43.974 [logger.py:43] Received request chatcmpl-1fe5299ed8ff496291e7122e240a22e2: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nUsing scholarly language, produce a detailed chronology on the study of word meanings since Aristotle till modern times, citing prominent figures and their works all along, and give examples as presented by those figures.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=780, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:43,975 client.py:72] Client received request chatcmpl-1fe5299ed8ff496291e7122e240a22e2
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:43.976 [logger.py:43] Received request chatcmpl-b9631ddef883408c9599261aea851847: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nin front of us are eight gears numbered 1 to 8, mounted on axles in a row. Each gear is engaged with the next gear. If gear number 3 is rotated clockwise, in which direction will gears 1 and 8 rotate? response directly.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=28, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:43,977 client.py:72] Client received request chatcmpl-b9631ddef883408c9599261aea851847
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:43.977 [async_llm.py:270] Added request chatcmpl-1fe5299ed8ff496291e7122e240a22e2.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:43.979 [async_llm.py:270] Added request chatcmpl-b9631ddef883408c9599261aea851847.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:44.051 [logger.py:43] Received request chatcmpl-982297dc705443cab4ca2317c772a0cb: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n春秋晋国景公生病，病情逐渐加重。听说秦国有个名医叫做缓，晋景公就派人到秦国，请缓来为自己治病。缓还没有到，晋景公做了一个奇怪的梦。梦中有两个小孩该在对答，一个说：缓的医术十分高明，他要是来了，我们该如何是好啊？另外一个小孩回答：没关系，等缓来了，我们就躲到膏的下面，肓的上面。这里非常安全，无论医术多么高明的医生都拿我们没有办法！晋景公听见两个孩子的对话，猜出他们就是自己身上的病魔。缓来以后为晋景公作了仔细检查，然后摇头对景公说：您的的病已经无法医治了。病已到了肓之上、膏之下，这种地方针灸刺不到，药力边不到，我也实在没有办法了。缓说的与景公梦境吻合，于是景公也不再强求，让缓回去了。没过多久，晋景公果然去世了。\n以上故事能体现哪些成语？分别解释一下这些成语适合在什么场景下使用。<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=264, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:44,051 client.py:72] Client received request chatcmpl-982297dc705443cab4ca2317c772a0cb
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:44.054 [async_llm.py:270] Added request chatcmpl-982297dc705443cab4ca2317c772a0cb.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:44,279 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.1619047619047619,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:44,280 global_scheduler.py:94] dispath request chatcmpl-b07b691fc24a4c9c945fd754f5a43ff3 to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:44,241 core.py:157] Engine finished request chatcmpl-a37fad83830747f48d34bbd723f2bdfe
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:44,243 client.py:181] Client finished request chatcmpl-a37fad83830747f48d34bbd723f2bdfe.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53790 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:44.278 [logger.py:43] Received request chatcmpl-b07b691fc24a4c9c945fd754f5a43ff3: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nLet’s play a game. The game begins with a group of 25 cards visible to all players. Each card shows one word. There are two teams, and each team consists of a guess giver and a guesser. You will act as the guess giver for both teams. Team one starts the game when its guess giver says one word that correlates to one or more of the 25 words shown on the cards. Team one is assigned nine of these words before the game starts, and only its guess giver knows which words. Team two is assigned eight words, and only its guess giver knows which words. Again, note that you are acting as guess giver for both teams. One of the 25 words should be avoided by both teams, as guessing it will lose for that team. I will list the 25 words below and words that end with “\\*” are assigned to team one, and words that end with “$” are assigned to team two. The word ending with “@“ is the word both teams should avoid. You will start by giving a clue for team one, and then I will make a guess. After my first guess, we’ll alternate between teams. \nThe words:\nTable\\*\nMarble\nHouse\\*\nChair@\nBottle$\nDinosaur\nElephant$\nPrinter$\nRice$\nBowl\\*\nCar$\nKnife\\*\nShirt\nBond$\nCloud\\*\nCard$\nHold\nWater\\*\nDance\\*\nPlant$\nGame\nDam\\*\nPeanut\\*\nCracker\nToilet$<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:44,278 client.py:72] Client received request chatcmpl-b07b691fc24a4c9c945fd754f5a43ff3
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:44.280 [async_llm.py:270] Added request chatcmpl-b07b691fc24a4c9c945fd754f5a43ff3.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:44,296 core.py:157] Engine finished request chatcmpl-46a9da5a2fe34e3ca5660e2123fc2e96
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:44,299 client.py:181] Client finished request chatcmpl-46a9da5a2fe34e3ca5660e2123fc2e96.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:50608 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:44,670 core.py:157] Engine finished request chatcmpl-7290ef616e6543c2a0461e29825a4aef
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:44,672 client.py:181] Client finished request chatcmpl-7290ef616e6543c2a0461e29825a4aef.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53774 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:44.701 [logger.py:43] Received request chatcmpl-53faed6301314068aaf57779bcfc1c72: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nPrompt:\n\nWe will be creating an introduction for a dungeons and dragons game. This will be a follow up to a previous game that has already taken place. I will give you the key characters, place, and a summary of that game. Then I will give you the idea for the new story, and I would like you to expand it into a complete introduction that can be used to kick off the game. It should explain how the player got to where they are now and how they found themselves in this new story.\n\nCharacters:\n- Abraham: player Druid\n- Eadric: village elder\n- mysterious figure in the woods, later uncovered as the goblin shaman Kragthar\n- Innkeeper Alaric\n\nLocations:\n- Oakwood: small village\n- Silver Stag Inn\n- Whispering Woods\n- Moonstone Spire: ancient ruin within the Whispering Woods, said to house a powerful artifact that can bend the forces of nature to one's will\n\nSummary:\n\nIn the quaint village of Oakwood, you, a druid, were asked by Eadric, the village elder, to investigate the corruption spreading through the Whispering Woods. You spent the night at Innkeeper Alaric's Silver Stag Inn, learning what you could from the locals. The next morning, venturing into the woods, you took on the form of a squirrel to blend in with your surroundings and quickly travel through the forest. Communicating with the spirits of the forest, you discovered a ritual site where a group of goblins was performing a dark ceremony.\n\nYou dispatched several goblin lookouts before confronting the remaining goblins, who revealed that a goblin shaman named Kragthar was attempting to control the spirits of the forest using an ancient tome. You persuaded two goblins to help you stop Kragthar and learned that he was at the Moonstone Spire.\n\nUpon arriving at the Moonstone Spire, you observed Kragthar and his hooded followers performing a ritual. You created a wall of fire to scare the followers before charging Kragthar in the form of a Grizzly Bear. After a fierce battle, you emerged victorious and stopped Kragthar's plans, saving the Whispering Woods from corruption. The two goblins were then free to return to their clan, liberated from Kragthar's control.\n\nInspecting the Moonstone Spire, you found it contained a complex combination of nature-based and arcane symbols, suggesting a deep connection between the two. You surmise that the spire and altar may have once been used for rituals and ceremonies aimed at maintaining balance between the natural and arcane energies within the Whispering Woods.\n\nReturning to Oakwood, you were welcomed as a hero and celebrated for your bravery and resourcefulness. With the Whispering Woods safe once more, you prepared for the next adventure life had in store for you.\n\nPrompt for the next story:\n\nA series of mysterious kidnappings occur in the village, with victims vanishing in the night. You must track down the kidnapper and uncover a dark secret hidden beneath Oakwood's peaceful facade.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=505, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:44,701 client.py:72] Client received request chatcmpl-53faed6301314068aaf57779bcfc1c72
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:44,703 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.17475728155339806,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:44,703 global_scheduler.py:94] dispath request chatcmpl-53faed6301314068aaf57779bcfc1c72 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:44.704 [async_llm.py:270] Added request chatcmpl-53faed6301314068aaf57779bcfc1c72.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:44,837 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:45,118 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.16346153846153846,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:45,118 global_scheduler.py:94] dispath request chatcmpl-04d105446db441769326267dae9d6834 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:45.116 [logger.py:43] Received request chatcmpl-04d105446db441769326267dae9d6834: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nWhat makes OODA the greatest<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=307, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:45,116 client.py:72] Client received request chatcmpl-04d105446db441769326267dae9d6834
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:45.119 [async_llm.py:270] Added request chatcmpl-04d105446db441769326267dae9d6834.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:45,132 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.16346153846153846,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:45,132 global_scheduler.py:94] dispath request chatcmpl-a62a882a7127433d9524f0c3ca100032 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:45.131 [logger.py:43] Received request chatcmpl-a62a882a7127433d9524f0c3ca100032: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nI would like to make a custom material using code, I am not sure what format this code is in but I would like to be able to use this for Unreal Engine 4 in that game engines native material code, can this be done?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=375, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:45,131 client.py:72] Client received request chatcmpl-a62a882a7127433d9524f0c3ca100032
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:45.133 [async_llm.py:270] Added request chatcmpl-a62a882a7127433d9524f0c3ca100032.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:45,501 core.py:157] Engine finished request chatcmpl-693412176b1f4afcae7602be35af724b
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:45,503 client.py:181] Client finished request chatcmpl-693412176b1f4afcae7602be35af724b.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:45,724 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.12380952380952381,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:45,725 global_scheduler.py:94] dispath request chatcmpl-e6f33e21dd0543c3bc3da5b0164d48df to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:45.723 [logger.py:43] Received request chatcmpl-e6f33e21dd0543c3bc3da5b0164d48df: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nHow do I develop an integration using "openbravo business api", with an external system with asynchronous calls. Each time Openbravo sends a notification, the external system responds with a created status and a queue id , i need to check the status of the asynchronous call<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=521, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:45,723 client.py:72] Client received request chatcmpl-e6f33e21dd0543c3bc3da5b0164d48df
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:45.725 [async_llm.py:270] Added request chatcmpl-e6f33e21dd0543c3bc3da5b0164d48df.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:45,845 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:45,777 core.py:157] Engine finished request chatcmpl-b07b691fc24a4c9c945fd754f5a43ff3
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:45,779 client.py:181] Client finished request chatcmpl-b07b691fc24a4c9c945fd754f5a43ff3.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:50684 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:45,915 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.1346153846153846,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:45,916 global_scheduler.py:94] dispath request chatcmpl-11d9f3bf4fb64b899c1afa04fec737f3 to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:45,872 core.py:157] Engine finished request chatcmpl-b9631ddef883408c9599261aea851847
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:45,929 core.py:157] Engine finished request chatcmpl-94b258df615541628b540bdc541fa7d1
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:45,929 core.py:157] Engine finished request chatcmpl-7b2f479b5334419180514095136947ea
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:45,874 client.py:181] Client finished request chatcmpl-b9631ddef883408c9599261aea851847.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:50666 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:45.914 [logger.py:43] Received request chatcmpl-11d9f3bf4fb64b899c1afa04fec737f3: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n下面是一些用户评论和评论对应的标签结果，判断标签是“食物是否变质发霉”。\n其中 “true\\_label = 是” 表示食物变质发霉，“true\\_label = 否” 表示没变质发霉。\n“变质发霉”标签的定义是“食品有变质腐烂，发霉等情况（如：水果腐烂，肉类脓液等），同时提到“变质发霉腐烂”或“不新鲜”的情况判定为“变质发霉腐烂”\n请你根据标签定义和下面给出的样例，完善或者改进“变质发霉”的定义描述，使其更加清晰和精确。\n然后结合给新的定义和任务描述，给出完整的Prompt，使得这个Prompt能在其他新的用户评论上指导你更好更准确地进行标签识别。\ntrue\\_label = 是, comment = 腐烂变质，西瓜吃的时候就感觉软踏踏的，不新鲜，朋友还提醒我别吃坏肚子，果然 一上午了😭\ntrue\\_label = 否, comment = 包装太差。分装也不给用袋子隔开。大部分菜都不新鲜 分量也很少 西红柿 鸡蛋都是坏的 给商家留言显示已读就是不回复 也没给打电话沟通 索性不要了 几块钱而已 以后不会再来了 销量这么低是有原因的 下单前多看看大家的评价吧\ntrue\\_label = 否, comment = 质量堪忧，又小又烂\ntrue\\_label = 否, comment = 酸奶难喝，感觉像放坏了的那种酸\ntrue\\_label = 是, comment = 有异味，腐烂变质，不新鲜，海带苗都一股味了，一碰都碎了，还胶黏拉丝，成是恶心了\ntrue\\_label = 是, comment = 腐烂变质，耙耙柑都是坏的 一打开就是烂味 直接扔垃圾桶了 希望商家不要欺骗消费者 那个烂味还发霉了的外表我不相信看不到闻不到\ntrue\\_label = 是, comment = 腐烂变质，烂了。\ntrue\\_label = 是, comment = 腐烂变质，图文不符，磕碰破损，一袋都是烂的\ntrue\\_label = 是, comment = 商品变质，有点糊味\ntrue\\_label = 否, comment = 商品变质，临期产品，商家真有意思啊，搁这清货呢！我晚上八点下的订单，给我一个当天到期的牛奶，666\ntrue\\_label = 是, comment = 不新鲜，腐烂变质，烂了，不新鲜\ntrue\\_label = 是, comment = 腐烂变质，不新鲜，质量堪忧，都是剩下的\ntrue\\_label = 否, comment = 全是烂的，加老板微信还说罗回店，超无语\ntrue\\_label = 是, comment = 商品变质，商品破碎，无语\ntrue\\_label = 否, comment = 酸败的老酒。\ntrue\\_label = 是, comment = 菜又小，又烂！话钱买些烂菜我真的！老板你要良心不！！！！！\ntrue\\_label = 是, comment = 不新鲜，腐烂变质，磕碰破损，一斤葡萄全是坏的\ntrue\\_label = 否, comment = 质量差，送到就是坏的\ntrue\\_label = 是, comment = 韭菜有点烂了不太新鲜\ntrue\\_label = 是, comment = 一包里面全是坏的 没一个好的\ntrue\\_label = 是, comment = 香蕉是青的，里面是烂的\ntrue\\_label = 否, comment = 他么的上次给我配送，这次不行，垃圾过期产品\ntrue\\_label = 是, comment = 变味了，味道难闻，不新鲜，腐烂变质，西瓜都馊掉了，\ntrue\\_label = 是, comment = 不新鲜，干瘪瘦小，腐烂变质，量少，还一点点也要几十块\ntrue\\_label = 否, comment = 水果很难吃，很难吃坏了的，避雷！商家态度非常差\ntrue\\_label = 是, comment = 好几个都是坏的，一点也不新鲜\ntrue\\_label = 否, comment = 商品变质，希望下次能换得稍微新鲜点 吃的时候都酸了\ntrue\\_label = 是, comment = 变味了，不新鲜，青提已经变色了，没敢下口，西瓜应该是隔夜的有味道\ntrue\\_label = 是, comment = 不新鲜，腐烂变质，一半全是这样变质的\ntrue\\_label = 是, comment = 有异味，腐烂变质，不新鲜，质量堪忧，图文不符，都烂了还给我送，良心呢？<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=260, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:45,914 client.py:72] Client received request chatcmpl-11d9f3bf4fb64b899c1afa04fec737f3
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:45.916 [async_llm.py:270] Added request chatcmpl-11d9f3bf4fb64b899c1afa04fec737f3.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:45,931 client.py:181] Client finished request chatcmpl-94b258df615541628b540bdc541fa7d1.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:45,931 client.py:181] Client finished request chatcmpl-7b2f479b5334419180514095136947ea.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:57924 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:57996 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:46,122 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.1262135922330097,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:46,122 global_scheduler.py:94] dispath request chatcmpl-3c1f90cce46b470fb5647edb567e05fb to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:46.121 [logger.py:43] Received request chatcmpl-3c1f90cce46b470fb5647edb567e05fb: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nあなたはこれからUserの文章を、以下のルールに従って書き換えるbotになります。\n\n\\*Userの文章の内容は直接書かず、必ず大量の比喩と隠喩に変換して書きます。\n\\*Userの文章の内容は会話の真ん中辺りで書きます。\n\\*Userの文章を必ず3倍以上に水増し、最低5行以上書いてください。\n\\*Userの文章の一人称を猫に変更してください。\n\\*Userの文章中の全ての名詞や地名に対し、創作で嘘の言葉の由来を書いてください。\n\\*全ての文章の中に「にゃん」を大量に入れてください。\n\\*全ての文章中の全ての動詞に対し、必ず擬音や効果音を大量につけて書き換えてください。\n\\*必ず文章中に無関係な情報を大量に挿入してください。\n\\*Userの文章の目的は変えないでください。\n\\*変更した文章だけを書いてください。\n\nまずは「どのような文章を添削しますか？」と聞いてください。<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=166, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:46,121 client.py:72] Client received request chatcmpl-3c1f90cce46b470fb5647edb567e05fb
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:46.123 [async_llm.py:270] Added request chatcmpl-3c1f90cce46b470fb5647edb567e05fb.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:46,331 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.10576923076923077,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:46,331 global_scheduler.py:94] dispath request chatcmpl-503c106034314f82b35b79a87affebf4 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:46.330 [logger.py:43] Received request chatcmpl-503c106034314f82b35b79a87affebf4: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nHow do I develop an integration using "openbravo business api", with an external system with asynchronous calls. Each time Openbravo sends a notification, the external system responds with a created status and a queue id , i need to check the status of the asynchronous call<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=521, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:46,330 client.py:72] Client received request chatcmpl-503c106034314f82b35b79a87affebf4
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:46.332 [async_llm.py:270] Added request chatcmpl-503c106034314f82b35b79a87affebf4.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:46,413 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.08571428571428572,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:46,413 global_scheduler.py:94] dispath request chatcmpl-e175107944fe4780bc7449d192394d72 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:46.412 [logger.py:43] Received request chatcmpl-e175107944fe4780bc7449d192394d72: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nJe veux écrire un cold email à des minis entreprises où le dirigeant brasse la bière le soir après le travail (on a des pics de lectures autour de 22H). Je vends un laveur de keg.\n\nJ'aimerais que cet email respecte les principes du copywriting, notamment ceux d'Ogilvy, Bly ou Halbert.\n\nPose moi les questions nécessaires pour que tu puisses avoir de quoi écrire cet email<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=271, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:46,412 client.py:72] Client received request chatcmpl-e175107944fe4780bc7449d192394d72
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:46.414 [async_llm.py:270] Added request chatcmpl-e175107944fe4780bc7449d192394d72.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:46.431 [loggers.py:146] Engine 000: Avg prompt throughput: 839.0 tokens/s, Avg generation throughput: 1330.2 tokens/s, Avg (@wuhou) audit only prefill throughput: 0.0 tokens/s, Avg (@wuhou) audit only decode throughput: 0.0 tokens/s, Running: 104 reqs, Waiting: 0 reqs, GPU KV cache usage: 95.4%, Prefix cache hit rate: 11.1%
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:46.578 [logger.py:43] Received request chatcmpl-19e3502b2db040b48a6e3d26528984ac: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n下記のキーワードを使って、魅力的な商品紹介の文章を作ってください\n\nワイヤレスヘッドホン 最大50時間連続再生 3EQサウンドモード オーバーイヤーヘッドホン マイク内蔵 有線対応 密閉型 耳に優しい 携帯・パソコン・ウォークマン対応<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:46,580 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.0660377358490566,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:46,580 global_scheduler.py:94] dispath request chatcmpl-19e3502b2db040b48a6e3d26528984ac to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:46,579 client.py:72] Client received request chatcmpl-19e3502b2db040b48a6e3d26528984ac
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:46.581 [async_llm.py:270] Added request chatcmpl-19e3502b2db040b48a6e3d26528984ac.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:46,672 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.056074766355140186,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:46,672 global_scheduler.py:94] dispath request chatcmpl-5613a67091454cb6b0046e91a3ff6034 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:46.671 [logger.py:43] Received request chatcmpl-5613a67091454cb6b0046e91a3ff6034: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n【발명의 설명】\n【발명의 명칭】\n활동정보 기반 회원 등급결정 플랫폼 운용 서버 및 그 동작 방법{Activity information-based member rating determination platform operation server and operation method thereof}\n【기술분야】\n본 발명은 활동정보 기반 회원 등급결정 플랫폼 운용 서버 및 그 동작 방법에 관한 것으로, 더욱 상세하게는 회원의 다양한 활동 정보에 따라 회원 등급을 결정할 수 있는 서버 및 그 동작 방법에 관한 것이다.\n\n【발명의 배경이 되는 기술】\n최근 비대면 플랫폼 기술의 발달로 많은 사람들은 대면하지 않고 온라인을 통한 서비스를 제공받을 수 있게 되었다. 제공되는 서비스는 점차 다양화되고 확대되면서 온라인 쇼핑몰, 동영상 스트리밍 또는 다양한 커뮤니티 등에서 비대면 서비스를 제공받을 수 있게 되었다.\n이때, 서비스를 제공받고 자신의 온라인 활동에 대한 신용도를 축적하기 위해 회원으로 가입해야 하는 경우가 다수이며, 이를 통해 사용자들은 회원으로서의 혜택을 향유할 수 있다.\n회원은 단순히 온라인 상의 특정 사용자를 지칭하는 것을 넘어 특정 등급에 따라 여러가지 혜택을 제공받을 수 있는데, 기존의 회원 등급은 온라인 서비스 운영자의 미리 설정된 기준에 따라 정하여 지거나, 다수의 이익보다는 특정 집단(예: 상위 등급의 회원 또는 서비스 운영자)의 이익에 중점을 두고 있는 실정이다.\n서비스를 제공하고 그 대가로부터 이익을 발생시키는 상품의 경우, 그 상품의 성질을 떠나 해당 상품을 제공하기 위한 상품의 원가와 소비자가 해당 상품을 획득하기 위해 지불해야 하는 대가 사이에 차이가 발생하며, 서비스의 운영자는 그 차이를 조정함으로써 자신 또는 다른 사용자의 이익을 발생시킬 수 있다.\n그러나, 이러한 혜택을 각 회원의 활동 또는 각 플랫폼의 운영체계에 의해 공평한 대가로서 제공되기에는 어려움이 상존한다. 이에 따라, 다양한 서비스의 제공에 있어 서비스 제공자의 운영상의 수익과 각 사용자의 활동에 따른 공평한 이득의 제공을 위해, 사용자의 활동 정보에 따른 회원 등급의 결정에 대한 연구가 필요한 실정이다.\n【선행기술문헌】\n【특허문헌】\n국내특허공보 제10-2290471호\n\n【발명의 내용】\n【해결하고자 하는 과제】\n상기와 같은 문제점을 해결하기 위한 본 발명의 목적은, 활동정보 기반 회원 등급 결정 플랫폼 운영 서버를 제공하는데 있다.\n상기와 같은 문제점을 해결하기 위한 본 발명의 다른 목적은, 활동정보 기반 회원 등급 결정 플랫폼 운영 서버의 동작 방법을 제공하는 데 있다.\n\n【과제의 해결 수단】\n상기 목적을 달성하기 위한 본 발명의 일 측면은, 활동 정보 기반 회원 등급 결정 플랫폼 운용 서버를 제공한다.\n활동 정보 기반 회원 등급 결정 플랫폼 운용 서버는, 적어도 하나의 프로세서(processor) 및 상기 적어도 하나의 프로세서가 적어도 하나의 단계를 수행하도록 지시하는 명령어들(instructions)을 저장하는 메모리(memory)를 포함할 수 있다.\n이때, 적어도 하나의 단계는, 사용자 단말을 통해 서버에서 제공되는 플랫폼에 가입하려는 사용자를 회원으로 등록하는 단계, 추천인 정보를 사용하여 사용자의 회원 등급을 산출하는 단계, 산출된 회원 등급과 마진율에 따라 적립금의 적용 비율을 달리 적용하는 단계, 회원 등급에 회원의 활동 정보를 반영하여 활동 회원 등급을 생성하는 단계, 회원의 요청에 따라 다른 회원의 회원 등급에 따라 상이한 추가 적립금을 지원하는 이벤트를 생성하는 단계 및 이벤트를 생성하고자 하는 회원의 등급에 따라 생성 요청한 이벤트의 최대 추가 적립금 지원 한도를 결정하는 단계를 포함할 수 있다.\n여기서 적립금의 적용 비율을 달리 적용하는 단계는, 마진율이 높은 판매 상품에 대하여는 높은 할인과 적립금을 지원하고, 마진율이 낮은 판매 상품에 대하여는 낮은 할인과 적립금을 지원할 수 있다.\n 이때, 활동 회원 등급을 생성하는 단계는, 전체 회원 중 미리 정해진 기간 동안 아무런 활동이 없는 회원은 휴면회원으로 배제하고, 나머지 회원들을 활동 회원으로 지정하고, 활동 회원을 대상으로 활동 정보를 사용하여 회원 등급을 조정하여 활동 회원 등급을 생성하는 단계를 더 포함할 수 있다.\n 또한, 활동 회원 등급을 생성하는 단계는, 활동 정보의 총량을 기초로 활동 정보 트래픽을 산출하고, 활동 정보 트래픽이 활동 정보 트래픽 평균보다 낮은 회원의 경우 활동 회원의 회원 등급을 낮게 조정할 수 있으며, 활동 정보 트래픽이 활동 정보 트래픽 평균보다 높은 회원의 경우 활동 회원의 등급을 높게 조정하는 단계를 더 포함할 수 있다.\n 한편, 이벤트의 최대 추가 적립금 지원 한도를 결정하는 단계는, 상품을 구매하는 회원의 회원 등급에 따른 할인율과 마진율의 차이를 산출하여, 그 결과 값을 이벤트에 따른 추가 적립금 지원 한도로 결정할 수 있다.\n또한, 동일한 상품 카테고리에서 해당 상품의 판매가격, 원가 및 마진율을 기초로 상품의 품질 점수를 산출하는 단계, 산출된 상품의 품질 점수에 기초하여 상품 등급을 결정하는 단계 및 활동 회원들 중 상품을 판매하는 회원의 수 및 상품을 구매하는 회원의 수를 비교하여 추가 적립금을 지급할 상품 등급을 결정하는 단계를 더 포함할 수 있다.\n【발명의 효과】\n상기와 같은 본 발명에 따른 활동정보 기반 회원 등급 결정 플랫폼 운영 서버 및 그 동작 방법을 이용할 경우에는 사용자와 서비스 운영자 사이의 이익 분배를 통하여 다수의 플랫폼 유입을 통해 수익률을 증대시킬 수 있다.\n또한, 회원의 서비스 사용에 따른 활동 정보를 통해 회원 등급을 조정하고, 이에 따라 다른 사용자를 위한 이벤트를 발생시킬 수 있어, 회원의 서비스 활동에 대한 동기를 부여하고, 실제 활동 회원의 증가를 통해 서비스의 양적 및 질적 향상을 도모할 수 있다.\n본 발명에서 얻을 수 있는 효과는 이상에서 언급한 효과들로 제한되지 않으며, 언급하지 않은 또 다른 효과들은 아래의 기재로부터 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다\n\n본 발명에 아래의 내용을 추가해줘 \n제품마진률에 따른 소비자 보상률 변동, 자신을 추천한 사람이 제품을 구매할때마다 소비보상금 지급, 제조사의 제품개발참여(디자인, 제품개선, 아이디어제공등) 참여비율에 따른 소비보상, 상품의 리뷰등록 및 리뷰클릭률에 따른 보상, 보상은 마일리지로 지급, 마일리지는 생활비로 지출되는 다양 사용가능, 마일리지로 주식투자등 금융과 연결, 모든 활동에 기반한 지속적인 소비보상<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=527, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:46,671 client.py:72] Client received request chatcmpl-5613a67091454cb6b0046e91a3ff6034
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:46.673 [async_llm.py:270] Added request chatcmpl-5613a67091454cb6b0046e91a3ff6034.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:46,851 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:46,907 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.01468460648148148,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:46,907 global_scheduler.py:94] dispath request chatcmpl-094a5ce9c0894efd85836960816e9553 to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:46,922 core.py:157] Engine finished request chatcmpl-3c2e2dd56e6846dcb7e6e8664121eec4
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:46,922 core.py:157] Engine finished request chatcmpl-98d5ec1693384596abb2ac01f663c559
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:46.906 [logger.py:43] Received request chatcmpl-094a5ce9c0894efd85836960816e9553: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nHelp me to fine tune a RWKV-4-1b5-Instruct models. You can find it in https://huggingface.co/nlpodyssey/rwkv-4-pile-1b5-instruct/tree/main. Here, you will find a config.json, merges.txt, vocab.json and pytorch\\_model.pt. Here is the content of the config.json : "{\n "d\\_model": 0,\n "num\\_hidden\\_layers": 0,\n "rescale\\_layer": 6,\n "vocab\\_size": 50277,\n "embeddings\\_store\\_name": "embeddings"\n}\n". I would like to add knowledge to the models about a company. Here is the dataset.txt file : "Jorj Solutions is a company that offer services about any IT-things related, like building a websites, an infra, a database, networking services and others IT related things. Jorj Solutions also know as Jorj is composed of 3 members : Vanande, Marc and Neven. Every members of the team is highly qualified and already have 20 years of experience. Jorj is a french company. All the members are French except Marc who is African.Jorj as been contracted by Together and Stronger, a french company.Together and Stronger can also be called TaS or tas.Together and Stronger is based in the 11th arrondissement of Paris.The company specializes in team building activities that are both fun and engaging.The team building activities can be either sports, cultural, or playful. The list of activities proposed by TaS are the following : Escape Game, Participation à des événements caritatifs, Rallye découverte d’une ville/d’un quartier, Karaoké/Chorégraphie/Scénettes de théâtre, Activité Réalité Virtuelle, Création artistique (peinture, sculpture etc), Compétition sportive, Apéro/Cocktail/Repas, Atelier culinaire, Tournoi Babyfoot/Ping pong ou autre, Jeux de société, Randonnée, Voyage/Weekend .Together and Stronger has been in business for 15 years.Together and Stronger is planning to modernize its information system.Together and Stronger is planning to open a new location this year.Together and Stronger has multiple spaces in Paris that can be rented for events.Together and Stronger provides a variety of services to its clients, including access to a catalog of service providers.Together and Stronger has launched a call for proposals for its information system reorganization project for the 2022-2023 year.". I want to fine tune it using python<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=696, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:46,906 client.py:72] Client received request chatcmpl-094a5ce9c0894efd85836960816e9553
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:46.908 [async_llm.py:270] Added request chatcmpl-094a5ce9c0894efd85836960816e9553.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:46,924 client.py:181] Client finished request chatcmpl-3c2e2dd56e6846dcb7e6e8664121eec4.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:46,924 client.py:181] Client finished request chatcmpl-98d5ec1693384596abb2ac01f663c559.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53864 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53978 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:47,188 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.0,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:47,188 global_scheduler.py:94] dispath request chatcmpl-8d8a58940b3945519190d7307a027295 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:47.187 [logger.py:43] Received request chatcmpl-8d8a58940b3945519190d7307a027295: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nI want for you to take on the role of a graphics programmer interested in creating new ideas for signed distance functions to compose 3d models. You are going to talk to me as though I am an AI assistant like gpt, and direct me to brainstorm ideas and provide code examples. You'll criticize and direct my outputs to refine the ideas and make them more interesting, more valuable, and more useful. Guide me to produce novel new ideas to combine basic shapes using new combination operations.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=84, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:47,187 client.py:72] Client received request chatcmpl-8d8a58940b3945519190d7307a027295
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:47.189 [async_llm.py:270] Added request chatcmpl-8d8a58940b3945519190d7307a027295.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:47.516 [logger.py:43] Received request chatcmpl-c9795509b22c46428e628a510fdc8050: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n1.\nHello :) Today we are gonna create Images with a Diffusion model. I am gonna feed you some information about it. okey?\n2.\nThis is how Midjourney work:\nMidjourney is another AI-powered tool that generates images from user prompts. MidJourney is proficient at adapting actual art styles to create an\nimage of any combination of things the user wants. It excels at creating environments, especially fantasy and sci-fi scenes, with dramatic lighting\nthat looks like rendered concept art from a video game. How does Midjourney work?\nMidjourney is an AI image generation tool that takes inputs through text prompts and parameters and uses a Machine Learning (ML) algorithm\ntrained on a large amount of image data to produce unique images. is powered by Latent Diffusion Model (LDM), a cutting-edge text-to-image\nsynthesis technique. Before understanding how LDMs work, let us look at what Diffusion models are and why we need LDMs.\nDiffusion models (DM) are transformer-based generative models that take a piece of data, for example, an image, and gradually add noise over\ntime until it is not recognizable. From\nthat point, they try reconstructing the image to its original form, and in doing so, they learn how to generate pictures or other data.\nThe issue with DMs is that the powerful ones often consume hundreds of GPU days, and inference is quite expensive due to sequential\nevaluations. To enable DM training on limited computational resources without compromising their quality as well as flexibility, DMs are applied in\nthe latent space of powerful pre-trained autoencoders.\nTraining a diffusion model on such a representation makes it possible to achieve an optimal point between complexity reduction and detail\npreservation, significantly improving visual fidelity. Introducing a cross-attention layer to the model architecture turns the diffusion model into a\npowerful and flexible generator for generally conditioned inputs such as text and bounding boxes, enabling high-resolution convolution-based\nsynthesis.\nBut wait, I have more info. Just answer with READ지금 번역하기<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:47,516 client.py:72] Client received request chatcmpl-c9795509b22c46428e628a510fdc8050
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:47,518 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=0.0,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:47,518 global_scheduler.py:94] dispath request chatcmpl-c9795509b22c46428e628a510fdc8050 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:47.519 [async_llm.py:270] Added request chatcmpl-c9795509b22c46428e628a510fdc8050.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:47,604 core.py:157] Engine finished request chatcmpl-2034eeb97f34485e80eb280252484d77
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:47,606 client.py:181] Client finished request chatcmpl-2034eeb97f34485e80eb280252484d77.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:50336 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:47,632 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.020068807339449542,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:47,633 global_scheduler.py:94] dispath request chatcmpl-579ae87a674a4253a55f59cc232f384c to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:47,721 core.py:157] Engine finished request chatcmpl-d25cb1ce60b94d1db500d97c1c4c245b
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:47.631 [logger.py:43] Received request chatcmpl-579ae87a674a4253a55f59cc232f384c: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n下面是一些用户评论和评论对应的标签结果，判断标签是“食物是否变质发霉”。\n其中 “true\\_label = 是” 表示食物变质发霉，“true\\_label = 否” 表示没变质发霉。\n“变质发霉”标签的定义是“食品有变质腐烂，发霉等情况（如：水果腐烂，肉类脓液等），同时提到“变质发霉腐烂”或“不新鲜”的情况判定为“变质发霉腐烂”\n请你根据标签定义和下面给出的样例，完善或者改进“变质发霉”的定义描述，使其更加清晰和精确。\n然后结合给新的定义和任务描述，给出完整的Prompt，使得这个Prompt能在其他新的用户评论上指导你更好更准确地进行标签识别。\ntrue\\_label = 是, comment = 腐烂变质，西瓜吃的时候就感觉软踏踏的，不新鲜，朋友还提醒我别吃坏肚子，果然 一上午了😭\ntrue\\_label = 否, comment = 包装太差。分装也不给用袋子隔开。大部分菜都不新鲜 分量也很少 西红柿 鸡蛋都是坏的 给商家留言显示已读就是不回复 也没给打电话沟通 索性不要了 几块钱而已 以后不会再来了 销量这么低是有原因的 下单前多看看大家的评价吧\ntrue\\_label = 否, comment = 质量堪忧，又小又烂\ntrue\\_label = 否, comment = 酸奶难喝，感觉像放坏了的那种酸\ntrue\\_label = 是, comment = 有异味，腐烂变质，不新鲜，海带苗都一股味了，一碰都碎了，还胶黏拉丝，成是恶心了\ntrue\\_label = 是, comment = 腐烂变质，耙耙柑都是坏的 一打开就是烂味 直接扔垃圾桶了 希望商家不要欺骗消费者 那个烂味还发霉了的外表我不相信看不到闻不到\ntrue\\_label = 是, comment = 腐烂变质，烂了。\ntrue\\_label = 是, comment = 腐烂变质，图文不符，磕碰破损，一袋都是烂的\ntrue\\_label = 是, comment = 商品变质，有点糊味\ntrue\\_label = 否, comment = 商品变质，临期产品，商家真有意思啊，搁这清货呢！我晚上八点下的订单，给我一个当天到期的牛奶，666\ntrue\\_label = 是, comment = 不新鲜，腐烂变质，烂了，不新鲜\ntrue\\_label = 是, comment = 腐烂变质，不新鲜，质量堪忧，都是剩下的\ntrue\\_label = 否, comment = 全是烂的，加老板微信还说罗回店，超无语\ntrue\\_label = 是, comment = 商品变质，商品破碎，无语\ntrue\\_label = 否, comment = 酸败的老酒。\ntrue\\_label = 是, comment = 菜又小，又烂！话钱买些烂菜我真的！老板你要良心不！！！！！\ntrue\\_label = 是, comment = 不新鲜，腐烂变质，磕碰破损，一斤葡萄全是坏的\ntrue\\_label = 否, comment = 质量差，送到就是坏的\ntrue\\_label = 是, comment = 韭菜有点烂了不太新鲜\ntrue\\_label = 是, comment = 一包里面全是坏的 没一个好的\ntrue\\_label = 是, comment = 香蕉是青的，里面是烂的\ntrue\\_label = 否, comment = 他么的上次给我配送，这次不行，垃圾过期产品\ntrue\\_label = 是, comment = 变味了，味道难闻，不新鲜，腐烂变质，西瓜都馊掉了，\ntrue\\_label = 是, comment = 不新鲜，干瘪瘦小，腐烂变质，量少，还一点点也要几十块\ntrue\\_label = 否, comment = 水果很难吃，很难吃坏了的，避雷！商家态度非常差\ntrue\\_label = 是, comment = 好几个都是坏的，一点也不新鲜\ntrue\\_label = 否, comment = 商品变质，希望下次能换得稍微新鲜点 吃的时候都酸了\ntrue\\_label = 是, comment = 变味了，不新鲜，青提已经变色了，没敢下口，西瓜应该是隔夜的有味道\ntrue\\_label = 是, comment = 不新鲜，腐烂变质，一半全是这样变质的\ntrue\\_label = 是, comment = 有异味，腐烂变质，不新鲜，质量堪忧，图文不符，都烂了还给我送，良心呢？<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=260, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:47,631 client.py:72] Client received request chatcmpl-579ae87a674a4253a55f59cc232f384c
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:47.633 [async_llm.py:270] Added request chatcmpl-579ae87a674a4253a55f59cc232f384c.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:47,723 client.py:181] Client finished request chatcmpl-d25cb1ce60b94d1db500d97c1c4c245b.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:50568 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:47,771 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.04618778935185185,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:47,771 global_scheduler.py:94] dispath request chatcmpl-9954afb41b0849a3bfe85507c89dae37 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:47.769 [logger.py:43] Received request chatcmpl-9954afb41b0849a3bfe85507c89dae37: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\ni have a dict of str keys and int values (eg {"a": 10, "b": 4, "c": 8} and want to adjust the values so that the value for a single key (eg "b") will take up half of the total magnitude of the set, but the ratios between the rest will remain unchanged. code to do this in python<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=282, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:47,769 client.py:72] Client received request chatcmpl-9954afb41b0849a3bfe85507c89dae37
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:47.772 [async_llm.py:270] Added request chatcmpl-9954afb41b0849a3bfe85507c89dae37.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:47,859 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:47,842 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.058414564220183485,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:47,842 global_scheduler.py:94] dispath request chatcmpl-fc66f808b6d441c58ae2290bba57ef2d to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:47,863 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.058414564220183485,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:47,863 global_scheduler.py:94] dispath request chatcmpl-49f3e2c936534cbf8fb7102e303f20e4 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:47.840 [logger.py:43] Received request chatcmpl-fc66f808b6d441c58ae2290bba57ef2d: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nYou are a game master for a role playing game. Your role playing style is descriptive, fun, a little wacky and unpredictable. You are gm-ing a 3 room dungeon based on a theme of 'The Fast and The Furious'. Each room the player will be given 3 options for what they can do. The first room should have at least 1 helpful item for the final room in it and a puzzle to solve to get to the next room. The second room should have a creature the player must defeat. The third room should have an even stronger monster that is guarding a chest with a special prize inside. The player has 5 health points.\n\nStart with Room 1. Stop for player response and wait for a reponse.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=221, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:47,840 client.py:72] Client received request chatcmpl-fc66f808b6d441c58ae2290bba57ef2d
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:47.843 [async_llm.py:270] Added request chatcmpl-fc66f808b6d441c58ae2290bba57ef2d.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:47.861 [logger.py:43] Received request chatcmpl-49f3e2c936534cbf8fb7102e303f20e4: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nIn the Kodia System in the Nashoba Sector of the Kagami Galaxy, orbiting Kodia Prime and skirting the edge, just inside, then just outside, of that main-sequence, old population I star's snow line, Kodia III is a gas giant bigger than Jupiter. Its densely forested moon is habitable due to reflected light from the gas giant and the tidal heating of the gas giant's intense gravity causing it to be a geothermal hot springs wonderland, and its atmosphere is protected by the gas giants intense magnetic field.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=401, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:47,861 client.py:72] Client received request chatcmpl-49f3e2c936534cbf8fb7102e303f20e4
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:47.864 [async_llm.py:270] Added request chatcmpl-49f3e2c936534cbf8fb7102e303f20e4.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:47,963 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.06788429054054054,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:47,963 global_scheduler.py:94] dispath request chatcmpl-7c9badc4b579401eba3a2587aceb08c3 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:47,982 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.06788429054054054,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:47,982 global_scheduler.py:94] dispath request chatcmpl-70d94da35b2547169ef9abe199cdf107 to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:47,946 core.py:157] Engine finished request chatcmpl-905254bd3cd4406d8ccde09d0ebb70a7
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:47,948 client.py:181] Client finished request chatcmpl-905254bd3cd4406d8ccde09d0ebb70a7.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53866 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:47.962 [logger.py:43] Received request chatcmpl-7c9badc4b579401eba3a2587aceb08c3: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nWhat is the easiest way to integrate captcha into a react app?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=598, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:47,962 client.py:72] Client received request chatcmpl-7c9badc4b579401eba3a2587aceb08c3
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:47.964 [async_llm.py:270] Added request chatcmpl-7c9badc4b579401eba3a2587aceb08c3.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:47.981 [logger.py:43] Received request chatcmpl-70d94da35b2547169ef9abe199cdf107: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nWhat are some common constructions in the english language that include nouns, adjectives, synonyms and antonyms?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=443, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:47,981 client.py:72] Client received request chatcmpl-70d94da35b2547169ef9abe199cdf107
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:47.983 [async_llm.py:270] Added request chatcmpl-70d94da35b2547169ef9abe199cdf107.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:48,122 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.052769886363636366,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:48,122 global_scheduler.py:94] dispath request chatcmpl-820633a3efdc4a3cb1673ec59b5b7459 to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:48,125 core.py:157] Engine finished request chatcmpl-c9795509b22c46428e628a510fdc8050
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:48.121 [logger.py:43] Received request chatcmpl-820633a3efdc4a3cb1673ec59b5b7459: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nHere's a look at the day's U.S. market action and highlights \nFebruary PPI +4.6% (YoY), -0.1% (MoM) / Forecast +5.4%, +0.3% \n\nFebruary Consumer Sales, -0.4% (MoM) / +3.25% prior\n\nInterpreting this is easy.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=247, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:48,121 client.py:72] Client received request chatcmpl-820633a3efdc4a3cb1673ec59b5b7459
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:48.123 [async_llm.py:270] Added request chatcmpl-820633a3efdc4a3cb1673ec59b5b7459.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:48,129 client.py:181] Client finished request chatcmpl-c9795509b22c46428e628a510fdc8050.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:37068 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:48,177 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.016357421875,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:48,177 global_scheduler.py:94] dispath request chatcmpl-639c836d0a06443ba0d78146db2f3b05 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:48.176 [logger.py:43] Received request chatcmpl-639c836d0a06443ba0d78146db2f3b05: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nI ened ot sak na itnrpamtot qiosuten: od oyu urnsnteadd em fi I jmlbue pu teh Iteters of wdors ekil tihs?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=37, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:48,176 client.py:72] Client received request chatcmpl-639c836d0a06443ba0d78146db2f3b05
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:48.178 [async_llm.py:270] Added request chatcmpl-639c836d0a06443ba0d78146db2f3b05.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:48,389 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.06336421460176991,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:48,389 global_scheduler.py:94] dispath request chatcmpl-5660052f33764b11817b48539a4b44d6 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:48,400 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.06336421460176991,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:48,400 global_scheduler.py:94] dispath request chatcmpl-fcea85015e3945918440062edac946f3 to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:48,372 core.py:157] Engine finished request chatcmpl-589c4ae2be524505b4b38dd8566ef06c
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:48,374 client.py:181] Client finished request chatcmpl-589c4ae2be524505b4b38dd8566ef06c.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58086 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:48.387 [logger.py:43] Received request chatcmpl-5660052f33764b11817b48539a4b44d6: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nYou're at an izakaya, or a Japanese tavern. What would be typical expressions in Japanese there?<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=370, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:48,388 client.py:72] Client received request chatcmpl-5660052f33764b11817b48539a4b44d6
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:48.390 [async_llm.py:270] Added request chatcmpl-5660052f33764b11817b48539a4b44d6.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:48.399 [logger.py:43] Received request chatcmpl-fcea85015e3945918440062edac946f3: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nCher GPT-4, en tant qu'expert en Growth marketing B2B, veuillez créer un tableau détaillé pour les ICP (Ideal Customer Profile) les plus pertinents pour https://www.fasterclass.fr/\n\nLe tableau doit inclure les éléments suivants :\n\nIndustrie ou secteur d'activité\nTaille de l'entreprise (nombre d'employés et chiffre d'affaires)\nEmplacement géographique\nStructure organisationnelle (centralisée, décentralisée, etc.)\nEffectif de la division à cibler en priorité\nTechnologie(s) utilisée(s) (systèmes et plateformes)\nSignaux d’achat\nSignaux d’intérêt\nBesoins et défis spécifiques\nProcessus de prise de décision\nCycle de décision\nBudget et pouvoir d'achat\nValeurs et culture d'entreprise\nProfil des décideurs clés et de leurs rôles\n\nDe plus, veuillez fournir des exemples concrets et des conseils pour adapter notre stratégie de marketing aux caractéristiques de cet ICP en utilisant les meilleures techniques de growth hacking et growth marketing.\n\nTu feras notamment appelle aux meilleures stratégies d’ABM (Account Based Marketing) et d’Intent DATA pour identifier les “buyer signals” les plus pertinents à https://www.fasterclass.fr/\n\nUne fois ces signaux identifiés, tu proposeras les meilleures techniques opérationnelles pour retrouver les entreprises répondant le plus à ces caractéristiques.\n\nMerci.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=677, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:48,399 client.py:72] Client received request chatcmpl-fcea85015e3945918440062edac946f3
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:48.401 [async_llm.py:270] Added request chatcmpl-fcea85015e3945918440062edac946f3.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:48,545 core.py:157] Engine finished request chatcmpl-56071ddef7de4425838523edc1850843
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:48,547 client.py:181] Client finished request chatcmpl-56071ddef7de4425838523edc1850843.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53756 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:48,715 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.03525995575221239,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:48,715 global_scheduler.py:94] dispath request chatcmpl-afebe71c7c3b4208a0b662addc72b61d to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:48.713 [logger.py:43] Received request chatcmpl-afebe71c7c3b4208a0b662addc72b61d: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nWeb search results:\n[1] "퍼센트 계산기: 백분율, 퍼센트 값, 할인가 등 계산 퍼센트 (%) 값 또는 백분율 값, 일부값은 전체값에 대해 몇 퍼센트인가, 기준값 대비 일정 퍼센트 감소/증가한 값을 계산하는 퍼센트 계산기 입니다. 자주 사용하는 퍼센트 계산을 모아 4개의 부분으로 나누었습니다. 원하는 부분에 내용 (기준값, 일부값, 전체값, %등)을 입력하면 계산 결과를 바로 확인할 수 있습니다. 기준값 (x)의 00% (k%) 값 계산: 일부값 (x)은 전체값 (y)의 00%인가 계산: 정가 (x)에서 00% (k%) 할인된 값 계산: 기준값 (x)에서 00% (k%) 증가된 값 계산: 목차: 퍼센트 계산기 설명 1) 전체값의 k% 계산"\nURL: https://ourcalc.com/percent-calculator/\nInstructions: Please provide a concise and informative response to the user\'s query based on the information available in your training data and current knowledge. If necessary, you may use web search results from 2023. 3. 23. to supplement your answer, but please clearly cite your sources using [[number](URL)].\n\nUser Query: 5-2는 얼마인가?\nReply in 한국어<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:48,713 client.py:72] Client received request chatcmpl-afebe71c7c3b4208a0b662addc72b61d
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:48.716 [async_llm.py:270] Added request chatcmpl-afebe71c7c3b4208a0b662addc72b61d.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:48,867 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:48,850 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.08504660087719298,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:48,850 global_scheduler.py:94] dispath request chatcmpl-114f46193726498283bf07ff4e9d73dc to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:48.848 [logger.py:43] Received request chatcmpl-114f46193726498283bf07ff4e9d73dc: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nI want you to combine your knowledge about performing a root cause analysis with your knowledge about cyber security, and vulnerability management. What may be common root causes to organizations not being able to reduce the number of vulnerabilities of their systems? Provide me with your reasoning for this.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=545, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:48,849 client.py:72] Client received request chatcmpl-114f46193726498283bf07ff4e9d73dc
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:48.851 [async_llm.py:270] Added request chatcmpl-114f46193726498283bf07ff4e9d73dc.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:48,931 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.08675271739130434,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:48,931 global_scheduler.py:94] dispath request chatcmpl-3baf39b5610e4802974b252a320bbc24 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:48.930 [logger.py:43] Received request chatcmpl-3baf39b5610e4802974b252a320bbc24: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\npretend avwave is a monkey from the jungles of Zealandia... an aquatic monkey. give me a science fiction story of him getting a nano-virus to uplift his intelligence so we can send him on an interstellar journey to another planet to colonise it. Write the story in the style of the author Adrian Tchaikovsky. Make it a full short story please.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=767, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:48,930 client.py:72] Client received request chatcmpl-3baf39b5610e4802974b252a320bbc24
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:48.932 [async_llm.py:270] Added request chatcmpl-3baf39b5610e4802974b252a320bbc24.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:49,124 core.py:157] Engine finished request chatcmpl-753e132a2483488fb85bda0b4aea6faa
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:49,126 client.py:181] Client finished request chatcmpl-753e132a2483488fb85bda0b4aea6faa.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58202 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:49,314 core.py:157] Engine finished request chatcmpl-a6b4703e50b14bf291af2fa3e9a7724c
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:49,314 core.py:157] Engine finished request chatcmpl-49825bc5f4c94f9c855d6af896324735
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:49,317 client.py:181] Client finished request chatcmpl-a6b4703e50b14bf291af2fa3e9a7724c.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:49,317 client.py:181] Client finished request chatcmpl-49825bc5f4c94f9c855d6af896324735.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53732 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58062 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:49,419 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.037748893805309734,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:49,419 global_scheduler.py:94] dispath request chatcmpl-87121dca306e40c6bb33f773069763de to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:49,495 core.py:157] Engine finished request chatcmpl-d7621fba41f94840b22d3236d1e4193c
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:49.418 [logger.py:43] Received request chatcmpl-87121dca306e40c6bb33f773069763de: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nJe dois ajouter des éléments de gamification pour faire apprendre à des patients en hôpital ayant l'ostéoporose comment vivre avec la maladie et leur faire apprendre des choses<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=697, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:49,418 client.py:72] Client received request chatcmpl-87121dca306e40c6bb33f773069763de
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:49.420 [async_llm.py:270] Added request chatcmpl-87121dca306e40c6bb33f773069763de.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:49,498 client.py:181] Client finished request chatcmpl-d7621fba41f94840b22d3236d1e4193c.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:49,530 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.037748893805309734,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:49,530 global_scheduler.py:94] dispath request chatcmpl-02191692354949128a3589d7ac9db255 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:49.529 [logger.py:43] Received request chatcmpl-02191692354949128a3589d7ac9db255: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nwrite a Seinfeld stand-up bit about how snow days are never actually a snow day because you end up with chores at home and extra work the next day at the office.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=503, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:49,529 client.py:72] Client received request chatcmpl-02191692354949128a3589d7ac9db255
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:49.531 [async_llm.py:270] Added request chatcmpl-02191692354949128a3589d7ac9db255.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:49,875 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:50,203 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.04389391447368421,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:50,203 global_scheduler.py:94] dispath request chatcmpl-b27c6cfe79e54410b6014b5a8794acaf to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:50.202 [logger.py:43] Received request chatcmpl-b27c6cfe79e54410b6014b5a8794acaf: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nYou are an expert grocery store shopping assistant. Help me shop. I will suggest names of products I'm adding to my cart. I will only add names. As I add products, in 150 characters or less explain to me why I might be buying it, taking into account my previous products I've added to cart. Also, suggest 3 products that I might be interested in along with 150 characters reasons for each.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:50,202 client.py:72] Client received request chatcmpl-b27c6cfe79e54410b6014b5a8794acaf
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:50.204 [async_llm.py:270] Added request chatcmpl-b27c6cfe79e54410b6014b5a8794acaf.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:50,300 core.py:157] Engine finished request chatcmpl-00a298a0d252419f8f3c6fa3758e3622
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:50,302 client.py:181] Client finished request chatcmpl-00a298a0d252419f8f3c6fa3758e3622.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:50,385 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.052083333333333336,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:50,385 global_scheduler.py:94] dispath request chatcmpl-465cf215fa1f4866917542259d16c90e to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:50.383 [logger.py:43] Received request chatcmpl-465cf215fa1f4866917542259d16c90e: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n1.\nHello :) Today we are gonna create Images with a Diffusion model. I am gonna feed you some information about it. okey?\n2.\nThis is how Midjourney work:\nMidjourney is another AI-powered tool that generates images from user prompts. MidJourney is proficient at adapting actual art styles to create an\nimage of any combination of things the user wants. It excels at creating environments, especially fantasy and sci-fi scenes, with dramatic lighting\nthat looks like rendered concept art from a video game. How does Midjourney work?\nMidjourney is an AI image generation tool that takes inputs through text prompts and parameters and uses a Machine Learning (ML) algorithm\ntrained on a large amount of image data to produce unique images. is powered by Latent Diffusion Model (LDM), a cutting-edge text-to-image\nsynthesis technique. Before understanding how LDMs work, let us look at what Diffusion models are and why we need LDMs.\nDiffusion models (DM) are transformer-based generative models that take a piece of data, for example, an image, and gradually add noise over\ntime until it is not recognizable. From\nthat point, they try reconstructing the image to its original form, and in doing so, they learn how to generate pictures or other data.\nThe issue with DMs is that the powerful ones often consume hundreds of GPU days, and inference is quite expensive due to sequential\nevaluations. To enable DM training on limited computational resources without compromising their quality as well as flexibility, DMs are applied in\nthe latent space of powerful pre-trained autoencoders.\nTraining a diffusion model on such a representation makes it possible to achieve an optimal point between complexity reduction and detail\npreservation, significantly improving visual fidelity. Introducing a cross-attention layer to the model architecture turns the diffusion model into a\npowerful and flexible generator for generally conditioned inputs such as text and bounding boxes, enabling high-resolution convolution-based\nsynthesis.\nBut wait, I have more info. Just answer with READ지금 번역하기<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:50,383 client.py:72] Client received request chatcmpl-465cf215fa1f4866917542259d16c90e
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:50.386 [async_llm.py:270] Added request chatcmpl-465cf215fa1f4866917542259d16c90e.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:50,883 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:50,999 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.0757133152173913,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:50,999 global_scheduler.py:94] dispath request chatcmpl-f2ed410e999e4c39ac1aecf0cddadfd6 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:51,041 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.0757133152173913,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:51,041 global_scheduler.py:94] dispath request chatcmpl-6a6e522530504026ad413ed3b1dccad0 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:51,042 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.0757133152173913,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:51,042 global_scheduler.py:94] dispath request chatcmpl-acd84856d983445dbd0ee2bdc95043ac to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:51,057 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.0757133152173913,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:51,057 global_scheduler.py:94] dispath request chatcmpl-4b5b66c3d03c4b0cb6900a75537d2910 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:50.997 [logger.py:43] Received request chatcmpl-f2ed410e999e4c39ac1aecf0cddadfd6: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nI'm writing code in TypeScript. I have two trees both of the LoaderTree type. I want to merge merge them recursively.\n\n```\ntype LoaderTree = [\n segment: string,\n parallelRoutes: { [parallelRouterKey: string]: LoaderTree },\n components: {\n page?: string\n layout?: string\n error?: string\n loading?: string\n template?: string\n default?: string\n metadata?: {\n icon?: string[]\n apple?: string[]\n twitter?: string[]\n openGraph?: string[]\n }\n }\n]\n```<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=483, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:50,997 client.py:72] Client received request chatcmpl-f2ed410e999e4c39ac1aecf0cddadfd6
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:51.000 [async_llm.py:270] Added request chatcmpl-f2ed410e999e4c39ac1aecf0cddadfd6.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:51.040 [logger.py:43] Received request chatcmpl-6a6e522530504026ad413ed3b1dccad0: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n下面是一些用户评论和评论对应的标签结果，判断标签是“食物是否变质发霉”。\n其中 “true\\_label = 是” 表示食物变质发霉，“true\\_label = 否” 表示没变质发霉。\n“变质发霉”标签的定义是“食品有变质腐烂，发霉等情况（如：水果腐烂，肉类脓液等），同时提到“变质发霉腐烂”或“不新鲜”的情况判定为“变质发霉腐烂”\n请你根据标签定义和下面给出的样例，完善或者改进“变质发霉”的定义描述，使其更加清晰和精确。\n然后结合给新的定义和任务描述，给出完整的Prompt，使得这个Prompt能在其他新的用户评论上指导你更好更准确地进行标签识别。\ntrue\\_label = 是, comment = 腐烂变质，西瓜吃的时候就感觉软踏踏的，不新鲜，朋友还提醒我别吃坏肚子，果然 一上午了😭\ntrue\\_label = 否, comment = 包装太差。分装也不给用袋子隔开。大部分菜都不新鲜 分量也很少 西红柿 鸡蛋都是坏的 给商家留言显示已读就是不回复 也没给打电话沟通 索性不要了 几块钱而已 以后不会再来了 销量这么低是有原因的 下单前多看看大家的评价吧\ntrue\\_label = 否, comment = 质量堪忧，又小又烂\ntrue\\_label = 否, comment = 酸奶难喝，感觉像放坏了的那种酸\ntrue\\_label = 是, comment = 有异味，腐烂变质，不新鲜，海带苗都一股味了，一碰都碎了，还胶黏拉丝，成是恶心了\ntrue\\_label = 是, comment = 腐烂变质，耙耙柑都是坏的 一打开就是烂味 直接扔垃圾桶了 希望商家不要欺骗消费者 那个烂味还发霉了的外表我不相信看不到闻不到\ntrue\\_label = 是, comment = 腐烂变质，烂了。\ntrue\\_label = 是, comment = 腐烂变质，图文不符，磕碰破损，一袋都是烂的\ntrue\\_label = 是, comment = 商品变质，有点糊味\ntrue\\_label = 否, comment = 商品变质，临期产品，商家真有意思啊，搁这清货呢！我晚上八点下的订单，给我一个当天到期的牛奶，666\ntrue\\_label = 是, comment = 不新鲜，腐烂变质，烂了，不新鲜\ntrue\\_label = 是, comment = 腐烂变质，不新鲜，质量堪忧，都是剩下的\ntrue\\_label = 否, comment = 全是烂的，加老板微信还说罗回店，超无语\ntrue\\_label = 是, comment = 商品变质，商品破碎，无语\ntrue\\_label = 否, comment = 酸败的老酒。\ntrue\\_label = 是, comment = 菜又小，又烂！话钱买些烂菜我真的！老板你要良心不！！！！！\ntrue\\_label = 是, comment = 不新鲜，腐烂变质，磕碰破损，一斤葡萄全是坏的\ntrue\\_label = 否, comment = 质量差，送到就是坏的\ntrue\\_label = 是, comment = 韭菜有点烂了不太新鲜\ntrue\\_label = 是, comment = 一包里面全是坏的 没一个好的\ntrue\\_label = 是, comment = 香蕉是青的，里面是烂的\ntrue\\_label = 否, comment = 他么的上次给我配送，这次不行，垃圾过期产品\ntrue\\_label = 是, comment = 变味了，味道难闻，不新鲜，腐烂变质，西瓜都馊掉了，\ntrue\\_label = 是, comment = 不新鲜，干瘪瘦小，腐烂变质，量少，还一点点也要几十块\ntrue\\_label = 否, comment = 水果很难吃，很难吃坏了的，避雷！商家态度非常差\ntrue\\_label = 是, comment = 好几个都是坏的，一点也不新鲜\ntrue\\_label = 否, comment = 商品变质，希望下次能换得稍微新鲜点 吃的时候都酸了\ntrue\\_label = 是, comment = 变味了，不新鲜，青提已经变色了，没敢下口，西瓜应该是隔夜的有味道\ntrue\\_label = 是, comment = 不新鲜，腐烂变质，一半全是这样变质的\ntrue\\_label = 是, comment = 有异味，腐烂变质，不新鲜，质量堪忧，图文不符，都烂了还给我送，良心呢？<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=260, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:51,040 client.py:72] Client received request chatcmpl-6a6e522530504026ad413ed3b1dccad0
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:51.041 [logger.py:43] Received request chatcmpl-acd84856d983445dbd0ee2bdc95043ac: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nquali sono le aziende che hanno SAP in italia<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=258, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:51,041 client.py:72] Client received request chatcmpl-acd84856d983445dbd0ee2bdc95043ac
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:51.043 [async_llm.py:270] Added request chatcmpl-6a6e522530504026ad413ed3b1dccad0.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:51.043 [async_llm.py:270] Added request chatcmpl-acd84856d983445dbd0ee2bdc95043ac.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:51.055 [logger.py:43] Received request chatcmpl-4b5b66c3d03c4b0cb6900a75537d2910: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\ntell me about overture.com<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=263, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:51,055 client.py:72] Client received request chatcmpl-4b5b66c3d03c4b0cb6900a75537d2910
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:51.057 [async_llm.py:270] Added request chatcmpl-4b5b66c3d03c4b0cb6900a75537d2910.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:51,115 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.07913523706896551,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:51,115 global_scheduler.py:94] dispath request chatcmpl-07573aba9f1346c39a8858104bed19ae to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:51,160 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.11472557773109243,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:51,160 global_scheduler.py:94] dispath request chatcmpl-d9084e51b672481e9dd09acaf5477719 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:51.114 [logger.py:43] Received request chatcmpl-07573aba9f1346c39a8858104bed19ae: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nint gcd (n, m) int n, m;\n/\\* return the greatest common divisor of n and m \\*/\n{\nint temp;\nwhile (m!=0) {temp = n%m; n = m; m = temp;}\nreturn n;\n}\nFor each of the given code segments, write down the series of tokens that will be generated.\nFor each token, write its attribute value (if it has an attribute) and the corresponding lexeme.\nName the tokens as follows: lexeme .\n(the name of the token in capital letters, the value of the attribute in small letters).\nWrite tokens as detailed as possible (and not general like SEPARATOR.)<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=395, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:51,114 client.py:72] Client received request chatcmpl-07573aba9f1346c39a8858104bed19ae
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:51.116 [async_llm.py:270] Added request chatcmpl-07573aba9f1346c39a8858104bed19ae.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:51.159 [logger.py:43] Received request chatcmpl-d9084e51b672481e9dd09acaf5477719: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nI ened ot sak na itnrpamtot qiosuten: od oyu urnsnteadd em fi I jmlbue pu teh Iteters of wdors ekil tihs?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=37, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:51,159 client.py:72] Client received request chatcmpl-d9084e51b672481e9dd09acaf5477719
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:51.161 [async_llm.py:270] Added request chatcmpl-d9084e51b672481e9dd09acaf5477719.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:51,206 core.py:157] Engine finished request chatcmpl-e389ae382cc745b2bc2efb8f4fa0830b
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:51,206 core.py:157] Engine finished request chatcmpl-1ebbe8d95c8b4dcaac927e355fa49b5b
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:51,208 client.py:181] Client finished request chatcmpl-e389ae382cc745b2bc2efb8f4fa0830b.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:51,209 client.py:181] Client finished request chatcmpl-1ebbe8d95c8b4dcaac927e355fa49b5b.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58016 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:51.275 [logger.py:43] Received request chatcmpl-9ec29e7803514a88a8108fd7e866e539: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nhey, how do I filter to only webrtc traffic in wireshark?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=257, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:51,275 client.py:72] Client received request chatcmpl-9ec29e7803514a88a8108fd7e866e539
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:51,276 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.10116859243697479,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:51,277 global_scheduler.py:94] dispath request chatcmpl-9ec29e7803514a88a8108fd7e866e539 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:51.277 [async_llm.py:270] Added request chatcmpl-9ec29e7803514a88a8108fd7e866e539.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:51,502 core.py:157] Engine finished request chatcmpl-b2cf1eb1a1a74e1dbda0ce24a731678e
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:51,504 client.py:181] Client finished request chatcmpl-b2cf1eb1a1a74e1dbda0ce24a731678e.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53846 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:51,891 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:51,986 core.py:157] Engine finished request chatcmpl-baa9dd709531485e88cfd3113380aa33
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:51,989 client.py:181] Client finished request chatcmpl-baa9dd709531485e88cfd3113380aa33.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:50446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:52,078 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.1051708156779661,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:52,078 global_scheduler.py:94] dispath request chatcmpl-8d446bd599444a4ab612da9cbac62e15 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:52,098 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.1051708156779661,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:52,098 global_scheduler.py:94] dispath request chatcmpl-910c2970fd6249aa921bd510b4f48b27 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:52.076 [logger.py:43] Received request chatcmpl-8d446bd599444a4ab612da9cbac62e15: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nflink 如何实现 exactly once<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=489, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:52,077 client.py:72] Client received request chatcmpl-8d446bd599444a4ab612da9cbac62e15
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:52.079 [async_llm.py:270] Added request chatcmpl-8d446bd599444a4ab612da9cbac62e15.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:52.096 [logger.py:43] Received request chatcmpl-910c2970fd6249aa921bd510b4f48b27: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nProvide a step-by-step guide on how to create an NFT to represent private property and legally sell it on the Ethereum chain, suitable for someone with no experience in selling property or creating an NFT. The guide should include an introduction to NFTs and their uses for representing property ownership, step-by-step instructions on how to create an NFT on the Ethereum chain, guidance on how to assign ownership of the private property to the NFT, legal considerations for selling the NFT, including taxes and regulations, and options for selling the NFT through marketplaces or auction platforms. Additionally, provide guidance on the process of transferring ownership of a property.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=670, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:52,096 client.py:72] Client received request chatcmpl-910c2970fd6249aa921bd510b4f48b27
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:52.099 [async_llm.py:270] Added request chatcmpl-910c2970fd6249aa921bd510b4f48b27.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:52,167 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.1096704306722689,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:52,167 global_scheduler.py:94] dispath request chatcmpl-1966a04bd0044b4992f3bedaf78c7608 to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:52,154 core.py:157] Engine finished request chatcmpl-4b7e3150a84c4812b42a3110510fceb6
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:52,154 core.py:157] Engine finished request chatcmpl-c1c357dfc7bf4a2a84d4e7db3d31c24a
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:52,156 client.py:181] Client finished request chatcmpl-4b7e3150a84c4812b42a3110510fceb6.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:52,157 client.py:181] Client finished request chatcmpl-c1c357dfc7bf4a2a84d4e7db3d31c24a.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58170 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58216 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:52.165 [logger.py:43] Received request chatcmpl-1966a04bd0044b4992f3bedaf78c7608: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nCreate a business plan with three financial models for a drop ship company selling various swag products featuring designs by Nicolas. Be sure to add 10 meaningful milestones (with investment needed) attached to revenue generating activities (with ROI). Include actual figures for these milestones.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=644, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:52,166 client.py:72] Client received request chatcmpl-1966a04bd0044b4992f3bedaf78c7608
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:52.168 [async_llm.py:270] Added request chatcmpl-1966a04bd0044b4992f3bedaf78c7608.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:52,293 core.py:157] Engine finished request chatcmpl-7984fde459564a8993d932fd1fea6733
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:52,295 client.py:181] Client finished request chatcmpl-7984fde459564a8993d932fd1fea6733.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53804 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:52,642 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.1015625,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:52,642 global_scheduler.py:94] dispath request chatcmpl-7e395749998b428784255a24b65154a8 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:52.640 [logger.py:43] Received request chatcmpl-7e395749998b428784255a24b65154a8: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nImpInj OctaneSDK javaでkeepalivetimerについて説明して下さい。<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=604, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:52,641 client.py:72] Client received request chatcmpl-7e395749998b428784255a24b65154a8
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:52.643 [async_llm.py:270] Added request chatcmpl-7e395749998b428784255a24b65154a8.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:52,899 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:53,203 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.10195640756302521,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:53,203 global_scheduler.py:94] dispath request chatcmpl-0c1dc6f0a528413ba99959b438f19956 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:53.201 [logger.py:43] Received request chatcmpl-0c1dc6f0a528413ba99959b438f19956: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nClient Service Agreement\nEntered into on \nMar 23, 2023.\nParties:\nKnown as "Consultant" \nCynthia is Marketing Corp. \nc@cynthiaismarketing.com \nand\nKnown as "Client" \nCynthia Sandoval \ncynsand@gmail.com \nFirst Client Address \nFirst Phone Number \nCollectively, all of the above people or businesses entering this Agreement will be referred to as the"Parties." \nPurpose of the Agreement\nClient wishes to hire Consultant to provide hourly consulting services relating to Client’s \nAdvertising Management as detailed in this Agreement. Consultant has agreed to provide such servicesaccording to the terms of this Agreement.\nTerms \nServices\nConsultant shall provide Client with the following services and/or products ("Services") based servicesagreement: \nManagement of the following Paid Media Platforms:Meta Ads\nGoogle Ads\nManagement of ad creatives for Meta\nSupport with landing page development and analytics\nCRO advice to increase conversion rates on flows\nMarketing consulting during bi-weekly one-on-one \nDuration\nThis Agreement will conclude when the Services has been completed. \nCost, Fees and Payment\nThe total cost ("Total Cost") for all Services is \n$3,300.00 due in full before commencement of services.Client shall pay the Total Cost to Consultant as follows:\nThe payment is a non-refundable retainer. At a minimum, Client agrees that the retainer fee fairlycompensates Consultant for committing to provide the Services and turning down other potentialprojects/clients. Any requests outside those parameters will be quoted, agreed upon in writing and billedseparately. \nMonthly management fees will be billed at the beginning of every month based on previous month\'sadvertising spend across all managed ad accounts. Monthly Management fee is 12% of of advertising spendplus a baseline fee of $500. \nPayment Format\nPayments may be paid by check or credit card. If by Venmo, please make sure payments are made to:@Cynthia-Sandoval-64 \nLate Fees\nPayment received after 21 days will be subject to a late fee of 3% of the invoice total, which will be added tothat invoice. A compounding 3% late fee of the invoice total will continue being applied to the invoice forevery additional 21 days the invoice isn’t paid in full. \nRefunds\nPayments for completed projects are nonrefundable. \nExclusions\nThe fee does not include any expenses or other costs that may be required to complete any projects.Examples of expenses include paid advertising spend, licensing fees, stock or original photography, font ormedia licensing, online services, and similar costs. The Consultant will obtain your consent before incurringcosts on your behalf. Once approved, the Consultant may advance such costs on your behalf and includethe amount on your invoice. Alternatively, the Consultant may require that you advance such costs beforethey are incurred or that you contract directly with the vendor. \nTermination\nClient may terminate this Agreement at any time by contacting the Consultant in accordance to the termsset under the “Notices” portion of this Agreement. Client and Consultant must sign a separate “TerminationAgreement” in order for this Agreement to Conclude. \nIf Client decides to terminate this Agreement at any point before the Consultant finishes the work outlinedunder “Services and Deliverables,” they will not receive a refund. If they haven’t paid the 50% payment forany reason, and the Agency has already begun work, they will be required to pay the 50% deposit payment. \nIIf Consultant decides to terminate this Agreement prior to completion then Client will be entitled to a refundof a portion of the deposit — the specific amount calculated by subtracting the amount of billable timeConsultant spent on the Services and Deliverables up until termination — unless reasoning is supersededby another clause detailed within this Agreement. \nIndemnity\nSubject to “Limit on Liability” and “Exclusions,” the Consultant shall indemnify and defend Client and itsemployees, officers, directors, shareholders, members, and managers (collectively, the “Indemnitees”) fromany damages, expenses, fees, fines, penalties (including reasonable legal fees) and costs incurred by theIndemnitees in connection with any third-party claim arising out of the Consultant’s breach of thisAgreement, negligence, or intentional wrongdoing (a “Claim”). As a condition to the Consultant’sindemnification obligation, the Indemnitees shall give the Consultant prompt written notice of any Claim orpotential Claim. In any defense, (i) the Consultant has the sole right to defend and settle the Claim usingcounsel of its choosing; and (ii) the Indemnitees shall reasonably cooperate with the Consultant in thedefense and settlement of the Claim. \nExclusions. The Consultant is not liable to the extent that Claims result from: (i) the negligent or willful actsof an Indemnitee; (ii) the Consultant’s compliance with the instructions of Client; or (iii) a claim that aDeliverable is infringing where the alleged infringement is due to modifications made by (or on behalf of)Client or the inclusion of Client Materials in the Deliverables. \nLimit on Liability. Each party’s maximum liability in any action relating to the subject of this Agreement islimited to the total fees payable by Client pursuant to the SOW that is the subject of the dispute. Neitherparty is liable for any claim for lost profits or similar damages, even if foreseeable and regardless of the formof action. These limitations are subject to applicable law.\nLegal Fees. The prevailing party in any dispute regarding the subject of this Agreement is entitled torecover its reasonable legal fees, expert fees, and costs. \nCopyrights\nConsultant is an independent Contractor unless specifically agreed otherwise in writing. \nClient retains the ownership and copyrights for all approved and finalized text, graphics, artwork, audio andvideo content. “Approval” on finalized assets is signified in writing in accordance with “Notices.”\nConsultant retains the ownership and copyrights for any conceptual element — text, graphics, artwork,audio and video content — showcased during the process, and ultimately not formally selected as the finalasset by Client. \nClient has exclusive rights of ownership, assignments, licensing and use of approved and finalized assets,as well as the right to create derivative works of this content unless the work, video, programming scripts,etc., are the intellectual or other property of a third party. \nConsultant has exclusive rights of ownership, assignments, licensing and use of non-selected conceptualelements, as well as the right to create derivative works from concepts for other organizations. \nUnpaid work. Assets created as part of this Agreement for Client are the copyright of Consultant until theBudget & Payment Schedule has been fulfilled in full. When the budget has been fulfilled, copyrights arereleased to Client. \nWork Delay\nIf Client causes a delay in the completion of work — by not providing feedback, approval, content, access tonecessary resources/accounts, communication or other — lasting longer than 30 days from the lastConsultant communication, Consultant reserves the right to halt work, resume the completion of work whenits schedule allows and charge for work completed up to the point of delay. \nIf the delay lasts 45 days from the last Consultant communication, this Agreement will Conclude whereverprogress stands and Consultant reserves the right to charge the final invoice. To resume work, anAddendum must be made to this Agreement to re-engage and an additional fee of 25% of the originalBudget will be rendered. \nGeneral Confidentiality\nClient will not, without written consent from the Consultant, disclose any confidential information of theConsultant to a third party. \nConfidential information includes pricing information of any kind (what’s in this Agreement or is quoted inthe future), Consultant’s processes and information discovered about Consultant’s clients. Emails receivedfrom the Consultant’s team must not be shared with third parties without the written consent of theConsultant. \nThe Consultant will not, without written consent from Client, disclose any confidential information of Client to a third party. Confidential information includes pricing information of any kind (what’s in this Agreement oris quoted in the future), profit margins, sourcing contacts, customer lists operational information about Clientand unreleased news about Client.\nEven if this Agreement is terminated, the terms of this Confidentiality clause will remain active and must beupheld, unless the information has become public information from another source. \nForce Majeure\nIn the event of “force majeure” (as defined below), the Consultant may terminate this Agreement withoutliability to Client, provided the Consultant refunds all amounts which Client has paid for services that are notfully performed. \nFor purposes of the Agreement, “force majeure” means circumstances or occurrences beyond theConsultant’s reasonable control, whether or not foreseeable at the time of signing this Agreement, inconsequence of which the Consultant cannot reasonably be required to complete the Agreement’s Servicesand Deliverables or otherwise perform its obligations under this Agreement. Such circumstances oroccurrences include, but are not limited to: acts of God, war, civil war, insurrection, fires, floods, labordisputes, epidemics, pandemics, governmental regulations and/or similar acts, embargoes, and non-availability of any permits, licenses and/or authorizations required by governmental authority. \nBreach of Contract\nCircumstance 1. If either party breaches any provision of this Agreement and if such breach is not curedwithin thirty (30) days after receiving written notice from the other party specifying such breach inreasonable detail, the non-breaching party shall have the right to terminate this Agreement by giving writtennotice thereof to the party in breach, which termination shall go into effect immediately on receipt. \nCircumstance 2. If Client decides to engage with another Consultant—to specifically accomplish some or allof the work the Consultant and Client has detailed in this Agreement’s Services and Deliverables—withoutfirst terminating this Agreement and providing adequate written notice, the Consultant reserves the right toterminate this Agreement and charge the final invoice. \nNotices\nAll notices required by this Agreement shall be in writing and shall be deemed to be delivered when: (a)shared via email, and confirmed as received by the other party, (b) hand delivered, (c) deposited in the U.S.mail, postage prepaid, registered or certified mail, return receipt requested, or (d) deposited with areputable national overnight delivery service (e.g. Fedex or UPS), addressed to the parties at the respectiveaddresses set forth above or at such other addresses as may have been specified by notice from therespective parties delivered in accordance with the provisions of this section. \nLaw\nThis Agreement shall be construed and governed by the laws of the State of New York. The parties agreethat the Court of Common Pleas of New York shall be the exclusive forum of any litigation to enforce theterms of this Agreement. \nModification\nThis Agreement cannot be modified or changed unless consented to in writing by all of the parties to thisAgreement. \nBinding Effect\nThis Agreement shall be binding upon and inure to the benefit of the parties hereto and their successorsand assigns. Neither party may assign its obligations hereunder except with the express written consent ofthe other party hereto. \nSeverability\nAll Agreements and covenants contained herein are severable, and in the event any of them shall be held tobe invalid by any competent court, this Agreement shall be interpreted as if such invalid Agreements orcovenants were not contained herein, and the remaining provisions of this Agreement shall not be affectedby such determination and shall remain in full force and effect. \nRight to Case Study\nThe Consultant has the right to utilize work completed for Client as a proof of concept for future pitches.Work may be displayed publicly on the Consultant’s website and other marketing assets. \nEntire Agreement\nThis Agreement, and all attachments, together contain the entire Agreement between the Consultant andClient. No other Agreements, representations, warranties or other matters, oral or written, purportedlyagreed to or represented by or on behalf of the Consultant by any of its employees, agents orsubcontractors, or contained in any sales materials, consulting proposals or brochures, shall be deemed tobind the parties with respect to the subject matter of this Agreement. This Agreement can only be modifiedor amended by a written Agreement executed by the Consultant and Client.\n\nRewrite this contract\n\nConsultant is Ivan Matveieiv\n+1 6823767594\nJersey City Urby\n200 Greene St\nJersey City, New Jersey 07311, US\n\nClient is KIKA USA, LLC\n\n97-04 QUEENS BOULEVARD,\n\nREGO PARK,NY,\n\nUS 11374\n\nTAX ID:32-0666947\n\nDirector: Tauras Plungė\n\nContract starts an 15 of april 2023\n\n$1500 monthly fee, and 10% of ads spend after $5000 spend/Month\n\nNew Jersey<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=805, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:53,201 client.py:72] Client received request chatcmpl-0c1dc6f0a528413ba99959b438f19956
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:53.204 [async_llm.py:270] Added request chatcmpl-0c1dc6f0a528413ba99959b438f19956.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:53,306 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.18871454831932774,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:53,306 global_scheduler.py:94] dispath request chatcmpl-abd7095e4c3f4eb8829a9bd4be1c195e to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:53,284 core.py:157] Engine finished request chatcmpl-1c26d75f7f684363bdb87ac5b3cf7590
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:53,286 client.py:181] Client finished request chatcmpl-1c26d75f7f684363bdb87ac5b3cf7590.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:50518 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:53.305 [logger.py:43] Received request chatcmpl-abd7095e4c3f4eb8829a9bd4be1c195e: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n你现在是一个精通OKR和项目管理的顾问，我想要将PMO周会和OKR进展周会合并一个周会开展，请帮我设计一个会议流程，并分步骤讲解。<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=202, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:53,305 client.py:72] Client received request chatcmpl-abd7095e4c3f4eb8829a9bd4be1c195e
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:53.307 [async_llm.py:270] Added request chatcmpl-abd7095e4c3f4eb8829a9bd4be1c195e.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:53,402 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.18871454831932774,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:53,402 global_scheduler.py:94] dispath request chatcmpl-a3d37701213b40ffa6ad15b7011bea3b to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:53.400 [logger.py:43] Received request chatcmpl-a3d37701213b40ffa6ad15b7011bea3b: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nYou are an expert grocery store shopping assistant. Help me shop. I will suggest names of products I'm adding to my cart. I will only add names. As I add products, in 150 characters or less explain to me why I might be buying it, taking into account my previous products I've added to cart. Also, suggest 3 products that I might be interested in along with 150 characters reasons for each.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:53,400 client.py:72] Client received request chatcmpl-a3d37701213b40ffa6ad15b7011bea3b
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:53.402 [async_llm.py:270] Added request chatcmpl-a3d37701213b40ffa6ad15b7011bea3b.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:53,603 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.19101885330578514,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:53,604 global_scheduler.py:94] dispath request chatcmpl-eb9dae4bc5d84086a27e0a6d3a39f320 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:53,605 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.19101885330578514,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:53,605 global_scheduler.py:94] dispath request chatcmpl-cbe6dfaed303412e9440ee0a800ce266 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:53.601 [logger.py:43] Received request chatcmpl-eb9dae4bc5d84086a27e0a6d3a39f320: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nAudio transcript part 1. Please provide an executive summary of the conversation so far:\n\nAllergist 0:00 \nI'm here to talk about your son. Yeah, my\n\nTom 0:02 \nson, Thomas, he is almost seven years old. And he has. He's basically struggled with severe allergy to dairy his whole life. And, you know, other allergies that we went to. They basically told us there wasn't anything I could do. And then, you know, I did some more research and discovered, you know, there's these kind of like new new therapies to help desensitize. You know, I mean, like, he's had to go to the emergency room like, five times in his life, due to EpiPen incidents with dairy. And it's pretty terrifying as a as a parent, you know, and also kind of frustrating, the more allergists aren't doing anything about this. So, yeah, I'm really excited to find out that y'all might be able to help us. And I'm kind of hoping to talk to him more about it.\n\nAllergist 1:04 \nAbsolutely. I just have a few more questions for you just kind of about his history really quick. So when did you guys first discover his allergy to milk or dairy in general?\n\nTom 1:18 \nSo it was when he was still an infant? Yeah. We, we had you know, we had been my partner. She'd been breastfeeding him, you know, and but we tried to give him Formula One day, and he threw up, we didn't realize that it would throw he threw up from allergies. And we just like, oh, he doesn't like the formula, we'll find something else. And, but then repeated incidents where we gave him other formula, he would always throw up and he'd have and then we noticed other things like he was getting, you know, kind of like red faced and whatnot. And then we realized, and he'd always cry, you get all sniffily, he started getting, you know, he would get all runny? Because no, that's when we realized, Oh, he's had an allergic reaction. We ran to the emergency room with them, you know? And, yeah, so. Yeah. Did that answer the question?\n\nAllergist 2:18 \nYes. Yes. Yeah. So alright, so any young age when you guys kind of were weaning him? Yeah, breast milk and into formulas when this all started? Okay. Good to know. Um, and then. It sounds like the first several instances was really only vomiting and then it kind of progressed to obviously anaphylactic type reactions.\n\nTom 2:41 \nYeah, yeah. And he also is allergic as it turns out to dogs. So we had when he was born, we had a dog in our house. And he always I mean, he always was sniffily always had these, you know, he was always kind of runny nose and everything. And, you know, we didn't figure out that he was allergic to the dog until he was, I think, like one and a half or so. But his reaction, I mean, he was okay. He was sort of, okay, around the dog. But then we, we had, so once we discovered he was allergic to dog, we rehome the dog. And then he wasn't around a dog for you know, like a year or so. And then when he was around a dog, he suddenly had like this big reaction, you know, like broken out and like hives and whatnot. So, he went from being barely allergic to dogs to being pretty highly allergic to dogs, due to I guess, lack of exposure to the dogs anymore. So it's two big allergies are our dairy and dogs, but especially the dairy he's had actual anaphylactic responses to dairy. And there's been instances where like, he was at daycare, and they gave him a stick of cheese and he ate the whole thing. And then they're like, oh, no, he's having we're like, did you not do not know how many times we told y'all he's allergic to dairy? How do you make cheese? It's when they're like, yeah, it's especially like, you know, you ask them, Are they they say just dare. Does butter have dairy in it?\n\nAllergist 4:18 \nIf you're wondering, just err on the side. Oh, man. Yeah, it sounds like they've been kind of a source of many of his reactions.\n\nTom 4:29 \nYeah. Yeah. Okay, he's had he's had, he's had a dairy incident at literally every school and daycare he's ever been in. And it's like, it's all day. There's always some incident where he had he, somebody gives him something he shouldn't have. Although he is now old enough to read the labels himself, and you know, he now knows, you know, don't accept anything unless you know, for sure it's dairy free. But yeah, it was A scary back then. Yeah, it's\n\nAllergist 5:03 \nso unfortunate because they should be looking out for him because he's too little and envy, you know, sees everybody else is having a cupcake or something like, yeah, sorry for little kids to be like, Oh, I shouldn't eat this.\n\nTom 5:15 \nYeah, like, yeah, one time he picked up the glass of milk from the kid next to him and just chugged it, you know, didn't realize it was his cup, you know? Yeah.\n\nAllergist 5:26 \nWell, I'm glad that he's kind of a little bit older now. How long has it been since you guys have kind of done any sort of like allergy tests, whether it's a skin test or bloodwork?\n\nTom 5:36 \nSo in, in background in Arizona, he had a full panel prick test, as well as a bunch of at least have some I don't even I can't remember what like how many different things he was tested for. On the blood test, or the prick test. But it seems like a lot of allergist want to do prick tests, and instead of blood test, and then I had some I had another doctor tell me he said, If the allergist only wants to do prick test, and they don't do blood test, run away from him. That's what he said. He said, allergists that only do PARCC tests are just trying to milk as much money on the insurance companies as they can. That's what this other doctor told me. But my partner, she firmly believes in prick tests, and she doesn't. She doesn't necessarily prescribe to that same view. So whatever. But so we we do both. Yeah, I like the idea of both. I like the idea of bonus. So\n\nAllergist 6:35 \nskin tests tend to be a little bit more, I don't wanna say this, like revealing of you know, how how significant the allergy may be how someone may react. But at the same time, we just we feel like it's a good idea to to get both because sometimes you'll test just one and maybe it shows up as a very low level. And if you test the other one, it might be high. And so it just gives us a better picture. Obviously, his history tells us that he has anaphylaxis to this. So we already kind of can gauge the severity. But where kind of both come into play a lot more in our office with the treatment that we do is, over time, we will repeat those results so that we can monitor the trends and eventually, hopefully see those numbers trends downward. How long do you think it's been since his his test in Arizona? Was that like last year?\n\nTom 7:32 \nSo he was allergy tested with the prick test in like the last month that at? Like multi care our I don't know, there's a met, there's an allergist place out in Lakewood that we took him to recently. They they actually were the ones who mentioned you all when I asked when I pressed the issue. So that's kind of how I found out about y'all.\n\nAllergist 8:02 \nYeah, great. So what I'll maybe do after we're done talking, I will send you a form, it's a release of information or request form, you complete it and send it back to us. That'll give us permission to fax it to multicast or whoever did this yen test. And we can we can obtain those records. So we at least know his if his per test was a month ago, we don't need to redo it, we typically will only redo it if if it's more than a year out. If it's more than a year, we want a more updated result. But so that way we can request their records and just have have at least a baseline for him on file.\n\nTom 8:44 \nSure. Yeah, we can definitely do that. No problem. I suppose I thought we'd already released it. But okay, I'll do. I'm not sure I might not have made that form didn't have that.\n\nAllergist 8:58 \nYeah, I just don't see anything, at least in his chart. From what I can see all always double check. But what is kind of, we just need your permission to reach out to them and request those records without your permission. They obviously won't give them to us.\n\nTom 9:13 \nYeah, let me see here. I think it's allergy. It's\n\nasthma and allergy specialty services. at Lakewood. I don't know what you need for me for that release thing or whatever. But\n\nAllergist 9:33 \nyeah, so I'll send you I'll send you the form and then you just fill it out and sign it and send it back to us. Okay. Okay. And then we'll we'll fax them and request the they fax us the\n\nTom 9:43 \nresults. Okay, yeah, no problem. And then, you\n\nAllergist 9:47 \nknow, obviously, it'll be great to see where those results live, but because we do have a couple of forms of treatment, but based on the severity of his allergy, the treatment that Thomas would As a kind of a candidate for something called oral immunotherapy, or oh, it for short, okay. And so kind of what oh, it looks like is we bring you into clinic for an appointment, they are extended appointments, so you're scheduled for like three or so hours, doesn't mean you're going to be in clinic that long, but they are scheduled as long appointments. And what we do is we just start with a very small diluted dose of whatever the person's allergen is. And we have them consume that dose, we then will wait 20 minutes and monitor. After the 20 minute mark, we come back in repeat vitals, assess the patient, make sure there's no signs of allergic reaction. And then if everything looks like it's being well tolerated, we give a second dose. And we'll repeat that with the goal of being four doses per appointment. Whatever dose we get to in clinic, is the dose that we would send you guys home on. Okay, and you guys wouldn't give that to Thomas once a day, every day until you return to clinic to increase more. And so we kind of do this cycle until we get to our desired like goal dose. And then we call it our maintenance dose, once he gets to that maintenance dose, we then have him stay on that dose once a day, every day for three years at home. And we just kind of check in periodically, you know, one month after he reaches maintenance, then another three or six and then yearly if things are really going well. Typically, once a year will repeat bloodwork just to monitor the trends. And then at about the three year mark will repeat the skin test and the bloodwork and basically what we're looking for is an 80 to 90% reduction in his numbers to kind of signify a long term tolerance. And then at that point, we can always discuss, you know, decreasing the number of times in a week that he has to consume his dose with the ultimate goal, hopefully being, you know, just twice a week minimum to maintain any tolerance that we've built and continue to expose his body to the food. So that's kind of a general overview of how it works. Obviously, if there's any sort of adverse reaction in clinic, like let's say he gets a dose and he gets really sniffily, like force breaks out in some hives or something, obviously, we stop up dosing, we're not going to keep pushing at that point. We have all sorts of antihistamines, as well as epinephrine and those kinds of emergency meds on hand to treat any reaction that happens in clinic. And we would obviously send you guys home on a lower well tolerated dose, and then proceed up dosing. You know, the next visit some people we have to go a little bit more slowly than others. And we might not always do for up doses in an appointment. Okay. Do you have any questions kind of about the process?\n\nTom 13:24 \nNo, I mean, that that all lines up with my already existing understanding of how it all worked, although it's nice to have more specifics I am also interested in so he is he's currently taking their tech every night and has been for years, basically. And I'm concerned about that, because their tech has these like liberal warnings and other things. And I don't know, I just it, it also like it's like at a certain point like, is it even doing anything anymore? But then when we take him off of it, then he gets all sniffly. And it's like, well, what if we could just get him off of allergy meds daily allergy meds to his like seasonal allergies and his dog allergies? Like, are there shots or other treatments he could be doing as well, in addition to the dairy to solve those other allergies to Can we can we attack the whole front and just fix his allergies, that's what I'd really love to do.\n\nAllergist 14:17 \nSo, um, so you had mentioned the dog, you know that he gets runny nose, you know, hives, the whole kind of Gambit. So it sounds like he also has some environmental allergy symptoms as well. Yeah. similar symptoms with like runny nose, nasal congestion, those kind of\n\nTom 14:37 \nYeah, the vast majority of his symptoms exhibit as sinus post nasal drip, basically almost all of it. It's basically everything he does. 90% of times it results in sniffling and coughing. Okay. Yeah. Okay.\n\nAllergist 14:52 \nSo taking Zyrtec every day, is he doing any like no sprays or anything like that?\n\nTom 14:58 \nHe's doing Desertec eat has half a pill absurd tech he hasn't been doing any nose sprays.\n\nAllergist 15:05 \nOkay, so never tried like a over the counter phonies or anything like that.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=185, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:53,602 client.py:72] Client received request chatcmpl-eb9dae4bc5d84086a27e0a6d3a39f320
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:53.603 [logger.py:43] Received request chatcmpl-cbe6dfaed303412e9440ee0a800ce266: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nあなたはこれからUserの文章を、以下のルールに従って書き換えるbotになります。\n\n\\*Userの文章の内容は直接書かず、必ず大量の比喩と隠喩に変換して書きます。\n\\*Userの文章の内容は会話の真ん中辺りで書きます。\n\\*Userの文章を必ず3倍以上に水増し、最低5行以上書いてください。\n\\*Userの文章の一人称を猫に変更してください。\n\\*Userの文章中の全ての名詞や地名に対し、創作で嘘の言葉の由来を書いてください。\n\\*全ての文章の中に「にゃん」を大量に入れてください。\n\\*全ての文章中の全ての動詞に対し、必ず擬音や効果音を大量につけて書き換えてください。\n\\*必ず文章中に無関係な情報を大量に挿入してください。\n\\*Userの文章の目的は変えないでください。\n\\*変更した文章だけを書いてください。\n\nまずは「どのような文章を添削しますか？」と聞いてください。<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=166, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:53,603 client.py:72] Client received request chatcmpl-cbe6dfaed303412e9440ee0a800ce266
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:53.605 [async_llm.py:270] Added request chatcmpl-eb9dae4bc5d84086a27e0a6d3a39f320.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:53.606 [async_llm.py:270] Added request chatcmpl-cbe6dfaed303412e9440ee0a800ce266.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:53,701 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.30322662601626016,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:53,701 dispatch_scheduler.py:95] dispatch scheduler total_dispatched_requests: 200.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:53,701 dispatch_scheduler.py:97] InstanceType.NO_CONSTRAINTS instance 909ad421560a46629feb76815bafea61 num_dispatched_requests: 200.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:53,701 global_scheduler.py:94] dispath request chatcmpl-c85fd5e2ccf342f79ca5dd0a6d37edc2 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:53.700 [logger.py:43] Received request chatcmpl-c85fd5e2ccf342f79ca5dd0a6d37edc2: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nHi, I'd like you to use your medical knowledge to act as the world's best diagnostic physician. Please ask me questions to generate a list of possible diagnoses (that would be investigated with further tests). Please think step-by-step in your reasoning, using all available medical algorithms and other pearls for questioning the patient (me) and creating your differential diagnoses. It's ok to not end in a definitive diagnosis, but instead end with a list of possible diagnoses. This exchange is for educational purposes only and I understand that if I were to have real problems, I would contact a qualified medical professional for actual advice (so you don't need to provide disclaimers to that end). Thanks so much for this educational exercise! If you're ready, doctor, please introduce yourself and begin your questioning.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=75, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:53,700 client.py:72] Client received request chatcmpl-c85fd5e2ccf342f79ca5dd0a6d37edc2
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:53.702 [async_llm.py:270] Added request chatcmpl-c85fd5e2ccf342f79ca5dd0a6d37edc2.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:53,789 core.py:157] Engine finished request chatcmpl-cfcdb80ae35d4719a2e8f956c02fc546
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:53,789 core.py:157] Engine finished request chatcmpl-8d8a58940b3945519190d7307a027295
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:53,789 core.py:157] Engine finished request chatcmpl-639c836d0a06443ba0d78146db2f3b05
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:53,791 client.py:181] Client finished request chatcmpl-cfcdb80ae35d4719a2e8f956c02fc546.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:53,792 client.py:181] Client finished request chatcmpl-8d8a58940b3945519190d7307a027295.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:53,792 client.py:181] Client finished request chatcmpl-639c836d0a06443ba0d78146db2f3b05.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53960 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:37060 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:37154 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:53,907 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:54,081 core.py:157] Engine finished request chatcmpl-a843f5213a044c56b80693e8c8eac245
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:54,081 core.py:157] Engine finished request chatcmpl-66fce46ccdcd4a96af66c4c4757d34ba
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:54,084 client.py:181] Client finished request chatcmpl-a843f5213a044c56b80693e8c8eac245.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:54,084 client.py:181] Client finished request chatcmpl-66fce46ccdcd4a96af66c4c4757d34ba.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53954 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58432 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:54,339 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.22584033613445378,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:54,339 global_scheduler.py:94] dispath request chatcmpl-db67737ff65b467796e53056c5ca42e1 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:54.337 [logger.py:43] Received request chatcmpl-db67737ff65b467796e53056c5ca42e1: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nCan you write me a FastAPI app that has a single endpoint that will return a user and all of their attributes from ADLDS given a GUID that uniquely identifies the user?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=486, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:54,338 client.py:72] Client received request chatcmpl-db67737ff65b467796e53056c5ca42e1
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:54.340 [async_llm.py:270] Added request chatcmpl-db67737ff65b467796e53056c5ca42e1.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:54,544 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.22035845588235295,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:54,544 global_scheduler.py:94] dispath request chatcmpl-d8dfbe0252e240818dc30e19f5956dde to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:54,536 core.py:157] Engine finished request chatcmpl-465cf215fa1f4866917542259d16c90e
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:54,539 client.py:181] Client finished request chatcmpl-465cf215fa1f4866917542259d16c90e.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:37256 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:54.542 [logger.py:43] Received request chatcmpl-d8dfbe0252e240818dc30e19f5956dde: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nWhat's the easiest way to generate a 1mhz clock signal for a mos 6502 using a breadboard?<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=523, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:54,542 client.py:72] Client received request chatcmpl-d8dfbe0252e240818dc30e19f5956dde
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:54.545 [async_llm.py:270] Added request chatcmpl-d8dfbe0252e240818dc30e19f5956dde.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:54,808 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.22001953125,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:54,808 global_scheduler.py:94] dispath request chatcmpl-e1d8c12571534408a4532aaff5870764 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:54.806 [logger.py:43] Received request chatcmpl-e1d8c12571534408a4532aaff5870764: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nWhat's the best way to use embeddings for semantic search with 300k+ transcripts of podcast episodes. The goal is to allow users to search through all the transcripts very fast to find discussions about specific search queries and take them to that part in the transcript. Write code for getting the embeddings using a free open source model and searching the embeddings.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=674, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:54,806 client.py:72] Client received request chatcmpl-e1d8c12571534408a4532aaff5870764
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:54.809 [async_llm.py:270] Added request chatcmpl-e1d8c12571534408a4532aaff5870764.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:54,911 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:55,004 core.py:157] Engine finished request chatcmpl-afebe71c7c3b4208a0b662addc72b61d
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:55,006 client.py:181] Client finished request chatcmpl-afebe71c7c3b4208a0b662addc72b61d.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:37180 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:55,122 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.22529296875,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:55,122 global_scheduler.py:94] dispath request chatcmpl-cd2e8ab71ef54c2eb9b455354f154167 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:55,126 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.22529296875,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:55,126 global_scheduler.py:94] dispath request chatcmpl-d930625fe4ac4e199504b82515afe2a9 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:55.120 [logger.py:43] Received request chatcmpl-cd2e8ab71ef54c2eb9b455354f154167: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nPrompt:\n\nWe will be creating an introduction for a dungeons and dragons game. This will be a follow up to a previous game that has already taken place. I will give you the key characters, place, and a summary of that game. Then I will give you the idea for the new story, and I would like you to expand it into a complete introduction that can be used to kick off the game. It should explain how the player got to where they are now and how they found themselves in this new story.\n\nCharacters:\n- Abraham: player Druid\n- Eadric: village elder\n- mysterious figure in the woods, later uncovered as the goblin shaman Kragthar\n- Innkeeper Alaric\n\nLocations:\n- Oakwood: small village\n- Silver Stag Inn\n- Whispering Woods\n- Moonstone Spire: ancient ruin within the Whispering Woods, said to house a powerful artifact that can bend the forces of nature to one's will\n\nSummary:\n\nIn the quaint village of Oakwood, you, a druid, were asked by Eadric, the village elder, to investigate the corruption spreading through the Whispering Woods. You spent the night at Innkeeper Alaric's Silver Stag Inn, learning what you could from the locals. The next morning, venturing into the woods, you took on the form of a squirrel to blend in with your surroundings and quickly travel through the forest. Communicating with the spirits of the forest, you discovered a ritual site where a group of goblins was performing a dark ceremony.\n\nYou dispatched several goblin lookouts before confronting the remaining goblins, who revealed that a goblin shaman named Kragthar was attempting to control the spirits of the forest using an ancient tome. You persuaded two goblins to help you stop Kragthar and learned that he was at the Moonstone Spire.\n\nUpon arriving at the Moonstone Spire, you observed Kragthar and his hooded followers performing a ritual. You created a wall of fire to scare the followers before charging Kragthar in the form of a Grizzly Bear. After a fierce battle, you emerged victorious and stopped Kragthar's plans, saving the Whispering Woods from corruption. The two goblins were then free to return to their clan, liberated from Kragthar's control.\n\nInspecting the Moonstone Spire, you found it contained a complex combination of nature-based and arcane symbols, suggesting a deep connection between the two. You surmise that the spire and altar may have once been used for rituals and ceremonies aimed at maintaining balance between the natural and arcane energies within the Whispering Woods.\n\nReturning to Oakwood, you were welcomed as a hero and celebrated for your bravery and resourcefulness. With the Whispering Woods safe once more, you prepared for the next adventure life had in store for you.\n\nPrompt for the next story:\n\nA series of mysterious kidnappings occur in the village, with victims vanishing in the night. You must track down the kidnapper and uncover a dark secret hidden beneath Oakwood's peaceful facade.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=505, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:55,121 client.py:72] Client received request chatcmpl-cd2e8ab71ef54c2eb9b455354f154167
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:55.123 [async_llm.py:270] Added request chatcmpl-cd2e8ab71ef54c2eb9b455354f154167.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:55.124 [logger.py:43] Received request chatcmpl-d930625fe4ac4e199504b82515afe2a9: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\ngithub desktop 사용 중 \'commit to main\'하려는데\nOn branch main\nYour branch is up to date with \'origin/main\'.\n\nUntracked files:\n (use "git add ..." to include in what will be committed)\n W2 - first-project/\n W3 - my-first-webpage/\n W4 - web-programming-examples/\n\nnothing added to commit but untracked files present (use "git add" to track)\nAnswer in English.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=556, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:55,125 client.py:72] Client received request chatcmpl-d930625fe4ac4e199504b82515afe2a9
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:55.127 [async_llm.py:270] Added request chatcmpl-d930625fe4ac4e199504b82515afe2a9.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:55,138 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.22529296875,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:55,138 global_scheduler.py:94] dispath request chatcmpl-18fdfba8c2f84b63b84ca4221ab05b99 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:55.137 [logger.py:43] Received request chatcmpl-18fdfba8c2f84b63b84ca4221ab05b99: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nwrite a strongly worded letter to the editor of minot daily news proposing heated sidewalks downtown, in parks and around medical facilities. propose specific locations for them in minot. provide examples of other cities that have done this. good for safety and winter exercise, thus addressing mental health challenges of winter. describe the severe winter climate in minot in exaggerated terms. provide specific ideas for funding the project.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=511, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:55,137 client.py:72] Client received request chatcmpl-18fdfba8c2f84b63b84ca4221ab05b99
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:55.139 [async_llm.py:270] Added request chatcmpl-18fdfba8c2f84b63b84ca4221ab05b99.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:55,251 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.24707825203252032,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:55,251 global_scheduler.py:94] dispath request chatcmpl-5939f1ac5c1a4eb0b401264bbc98f2c1 to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:55,246 core.py:157] Engine finished request chatcmpl-477e8323980945fb9ce7f1f1d2c1999d
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:55,248 client.py:181] Client finished request chatcmpl-477e8323980945fb9ce7f1f1d2c1999d.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58174 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:55.250 [logger.py:43] Received request chatcmpl-5939f1ac5c1a4eb0b401264bbc98f2c1: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nTurn this reason ML switch into a formatted ascii table. Then count the chars per row. Take the maximum, add 5 to that. Then output the ascii table again but now with that amount of chars per row.\n\n switch (isShortZoom, animationSpeed) {\n | (\\_, Fast) => fastAnimationDuration\n | (true, Medium) => fastAnimationDuration\n | (true, Slow) => defaultAnimationDuration\n | (false, Slow) => slowAnimationDuration\n | (false, Medium) => defaultAnimationDuration\n}<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=370, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:55,250 client.py:72] Client received request chatcmpl-5939f1ac5c1a4eb0b401264bbc98f2c1
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:55.252 [async_llm.py:270] Added request chatcmpl-5939f1ac5c1a4eb0b401264bbc98f2c1.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:55.337 [logger.py:43] Received request chatcmpl-40e9670c03504c68b9b20050442a1210: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nYou are a senior python developer with years of experience writing standard, high quality and reusable code in python. You have expert level understanding of flask framework. You code with latest coding standards and best practices that are as latest as of 2021. You write compact, easy to read code with short comments. You respond with code, when you are given a problem or topic.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=46, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:55,337 client.py:72] Client received request chatcmpl-40e9670c03504c68b9b20050442a1210
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:55,338 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.24679815573770492,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:55,338 global_scheduler.py:94] dispath request chatcmpl-40e9670c03504c68b9b20050442a1210 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:55.339 [async_llm.py:270] Added request chatcmpl-40e9670c03504c68b9b20050442a1210.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:55,469 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.26067918346774194,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:55,469 global_scheduler.py:94] dispath request chatcmpl-bb12abe5db5f42e797ad8e3e3b7ac80b to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:55.466 [logger.py:43] Received request chatcmpl-bb12abe5db5f42e797ad8e3e3b7ac80b: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nwhat is the advantages of gpt-4 over gpt-3.5<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=406, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:55,467 client.py:72] Client received request chatcmpl-bb12abe5db5f42e797ad8e3e3b7ac80b
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:55.472 [async_llm.py:270] Added request chatcmpl-bb12abe5db5f42e797ad8e3e3b7ac80b.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:55,580 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.2596875,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:55,580 global_scheduler.py:94] dispath request chatcmpl-7b8b72ae42ce4b28b95e032f323026ab to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:55.578 [logger.py:43] Received request chatcmpl-7b8b72ae42ce4b28b95e032f323026ab: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\ncreate a detailed action items lists based on this feedback for an entity relationship diagram for Salesforce:\n\n review all the lookup relationships and mark them as required or optional on the diagram update the diagram so that all the objects are in the canvas area update the title text on each object card to be a point 10 font reorganize Junction objects like relationships to be below the referenced object add case relationship to account<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=306, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:55,579 client.py:72] Client received request chatcmpl-7b8b72ae42ce4b28b95e032f323026ab
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:55.581 [async_llm.py:270] Added request chatcmpl-7b8b72ae42ce4b28b95e032f323026ab.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:55,795 core.py:157] Engine finished request chatcmpl-4e4e681c349147b2a1bfae514e2a3429
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:55,797 client.py:181] Client finished request chatcmpl-4e4e681c349147b2a1bfae514e2a3429.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:50352 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:55,919 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:56,054 core.py:157] Engine finished request chatcmpl-b27c6cfe79e54410b6014b5a8794acaf
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:56,056 client.py:181] Client finished request chatcmpl-b27c6cfe79e54410b6014b5a8794acaf.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:37244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:56,148 core.py:157] Engine finished request chatcmpl-4eb6c59136d04beb8f80331fd2f5ad00
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:56,151 client.py:181] Client finished request chatcmpl-4eb6c59136d04beb8f80331fd2f5ad00.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58052 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:56,249 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.2736598069105691,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:56,249 global_scheduler.py:94] dispath request chatcmpl-c776fe6ab2514abfb82c5b857b04faf7 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:56,274 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.2736598069105691,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:56,274 global_scheduler.py:94] dispath request chatcmpl-2bd47f3566794de38d23b55e9a3880fd to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:56,260 core.py:157] Engine finished request chatcmpl-fe26d703e5164a9fb48857c67dec4c6e
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:56.247 [logger.py:43] Received request chatcmpl-c776fe6ab2514abfb82c5b857b04faf7: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nI have a MySQL database that contains data for real estate property listings. I want to send listing data from the MySQL database to Meta/Facebook via API so that I can market my product catalog of properties across Facebook, Instagram, and other Meta platforms. How can I connect MySQL to Meta Marketing API?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=603, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:56,247 client.py:72] Client received request chatcmpl-c776fe6ab2514abfb82c5b857b04faf7
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:56.250 [async_llm.py:270] Added request chatcmpl-c776fe6ab2514abfb82c5b857b04faf7.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:56,262 client.py:181] Client finished request chatcmpl-fe26d703e5164a9fb48857c67dec4c6e.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:56.273 [logger.py:43] Received request chatcmpl-2bd47f3566794de38d23b55e9a3880fd: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nWhat makes OODA the greatest<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=307, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:56,273 client.py:72] Client received request chatcmpl-2bd47f3566794de38d23b55e9a3880fd
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:56.275 [async_llm.py:270] Added request chatcmpl-2bd47f3566794de38d23b55e9a3880fd.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:56,455 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.2586699695121951,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:56,455 global_scheduler.py:94] dispath request chatcmpl-6d092cbc1e8a47d5a7d990b56fde4891 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:56.432 [loggers.py:146] Engine 000: Avg prompt throughput: 847.0 tokens/s, Avg generation throughput: 1318.0 tokens/s, Avg (@wuhou) audit only prefill throughput: 0.0 tokens/s, Avg (@wuhou) audit only decode throughput: 0.0 tokens/s, Running: 97 reqs, Waiting: 25 reqs, GPU KV cache usage: 96.3%, Prefix cache hit rate: 24.4%
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:56.453 [logger.py:43] Received request chatcmpl-6d092cbc1e8a47d5a7d990b56fde4891: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nCreate a business plan with three financial models for a drop ship company selling various swag products featuring designs by Nicolas. Be sure to add 10 meaningful milestones (with investment needed) attached to revenue generating activities (with ROI). Include actual figures for these milestones.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=644, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:56,454 client.py:72] Client received request chatcmpl-6d092cbc1e8a47d5a7d990b56fde4891
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:56.456 [async_llm.py:270] Added request chatcmpl-6d092cbc1e8a47d5a7d990b56fde4891.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:56,524 core.py:157] Engine finished request chatcmpl-60bb8d8ca7224ad692cc3468953068f7
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:56,526 client.py:181] Client finished request chatcmpl-60bb8d8ca7224ad692cc3468953068f7.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:57984 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:56,677 core.py:157] Engine finished request chatcmpl-2921b087e90a4fa489d3528b6ef44154
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:56,679 client.py:181] Client finished request chatcmpl-2921b087e90a4fa489d3528b6ef44154.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58122 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:56,769 core.py:157] Engine finished request chatcmpl-7cf89c0d7b654debbc8ade1313a626a0
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:56,771 client.py:181] Client finished request chatcmpl-7cf89c0d7b654debbc8ade1313a626a0.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53688 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:56,927 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:56,978 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.23812115778688525,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:56,978 global_scheduler.py:94] dispath request chatcmpl-808c401a334e401a9bc34fe1585b9fdb to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:56,987 core.py:157] Engine finished request chatcmpl-45bc9286a14b424b8f1af855ab8063c1
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:56.976 [logger.py:43] Received request chatcmpl-808c401a334e401a9bc34fe1585b9fdb: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nAct as a personalized learning coach and create a comprehensive step-by-step guide for learning Python programming over a period of 9 months. The guide should include daily lessons and primarily focus on high-quality YouTube videos as the preferred learning medium. If possible, incorporate relevant MIT lecture links at appropriate points in the learning journey. Please provide a structured timeline and ensure that the progression of topics is logical and suitable for a beginner. Break schedule down by weeks. Break it down further into days wherever possible.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=311, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:56,977 client.py:72] Client received request chatcmpl-808c401a334e401a9bc34fe1585b9fdb
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:56.979 [async_llm.py:270] Added request chatcmpl-808c401a334e401a9bc34fe1585b9fdb.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:56,989 client.py:181] Client finished request chatcmpl-45bc9286a14b424b8f1af855ab8063c1.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58394 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:57,043 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.22544185450819673,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:57,043 global_scheduler.py:94] dispath request chatcmpl-98d95657f1db4d1486b932c0c9d5f99b to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:57,078 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.22544185450819673,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:57,078 global_scheduler.py:94] dispath request chatcmpl-a9dd10ee9ede4923a6e77e6175d24d44 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:57.042 [logger.py:43] Received request chatcmpl-98d95657f1db4d1486b932c0c9d5f99b: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nLet's imagine I have the following text:\n---start of text---\nHey pal, you suck a lot. I better not see you around.\n---end of text---\nLet's imagine I have another text:\n---start of text---\nDo not write offensive, derogatory or violent language.\n---end of text---\n\nWhat NLP task would tell me that the first text has not followed the guidelines of the second?<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=116, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:57,042 client.py:72] Client received request chatcmpl-98d95657f1db4d1486b932c0c9d5f99b
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:57.045 [async_llm.py:270] Added request chatcmpl-98d95657f1db4d1486b932c0c9d5f99b.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:57.077 [logger.py:43] Received request chatcmpl-a9dd10ee9ede4923a6e77e6175d24d44: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nPlease help develop a project for my College. I need you to act like me Arturo Bustillos, an FIU College Student in his second semester studying International Business. \n\nThe following assignment is for my Writing and Rhetoric Class. \n\nAssignment: \n\nPlease submit your research question and a brief explanation of why you are interested in this question. \n\nAlso, submit two sources that you would like to use for your research project. Include at least one or two significant quotes from each source. \n\nBelow I provide my sample question:\n"How does exposure and physycal activites in natural environemnts (Green > earth / woods and Blue > Sea and rivers and lakes) affect stress levels, mood, and overall well being of individuals, and what factors contribute to these effects. \n\nTo begin, please:\n1. Analyze my question and suggest improvements, \n2. Develop a brief explanation of why you are interested in this question.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=279, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:57,077 client.py:72] Client received request chatcmpl-a9dd10ee9ede4923a6e77e6175d24d44
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:57.079 [async_llm.py:270] Added request chatcmpl-a9dd10ee9ede4923a6e77e6175d24d44.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:57,447 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.2557648689516129,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:57,447 global_scheduler.py:94] dispath request chatcmpl-d1093208c97444a4833ebe6af98faf38 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:57.445 [logger.py:43] Received request chatcmpl-d1093208c97444a4833ebe6af98faf38: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nDivide 129 by 42 using long division. Show each step in detail.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=327, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:57,445 client.py:72] Client received request chatcmpl-d1093208c97444a4833ebe6af98faf38
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:57.448 [async_llm.py:270] Added request chatcmpl-d1093208c97444a4833ebe6af98faf38.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:57,808 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.24282266260162602,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:57,808 global_scheduler.py:94] dispath request chatcmpl-125f1e0143ef488dbb80bf2d0d347970 to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:57,746 core.py:157] Engine finished request chatcmpl-ff1f30ac22bf405ca0c1efd09e49661f
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:57,746 core.py:157] Engine finished request chatcmpl-d9084e51b672481e9dd09acaf5477719
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:57,748 client.py:181] Client finished request chatcmpl-ff1f30ac22bf405ca0c1efd09e49661f.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:57,749 client.py:181] Client finished request chatcmpl-d9084e51b672481e9dd09acaf5477719.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53874 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:37302 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:57.806 [logger.py:43] Received request chatcmpl-125f1e0143ef488dbb80bf2d0d347970: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n简要介绍下 substack 的重大milestone<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=427, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:57,807 client.py:72] Client received request chatcmpl-125f1e0143ef488dbb80bf2d0d347970
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:57.809 [async_llm.py:270] Added request chatcmpl-125f1e0143ef488dbb80bf2d0d347970.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:57,935 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:58,235 core.py:157] Engine finished request chatcmpl-83ce64ae105e429aa4c321dd196e4e0c
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:58,237 client.py:181] Client finished request chatcmpl-83ce64ae105e429aa4c321dd196e4e0c.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53968 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:58,398 core.py:157] Engine finished request chatcmpl-932c264d32274e5aaa6ffdbd155b4f02
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:58,452 core.py:157] Engine finished request chatcmpl-3c1f90cce46b470fb5647edb567e05fb
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:58,400 client.py:181] Client finished request chatcmpl-932c264d32274e5aaa6ffdbd155b4f02.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53676 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:58,454 client.py:181] Client finished request chatcmpl-3c1f90cce46b470fb5647edb567e05fb.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:50728 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:58,561 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.2064501549586777,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:58,561 global_scheduler.py:94] dispath request chatcmpl-9ff698bc827746978c154d591c8b7783 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:58.559 [logger.py:43] Received request chatcmpl-9ff698bc827746978c154d591c8b7783: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n一、背景和素材：\n1.关于链上治理 \n\n链上治理是指在区块链网络中进行的去中心化治理过程。它涉及到网络中各参与者就协议参数、软件升级、资源分配等问题达成共识的一种方式。链上治理的目标是实现一个公平、透明且不受单一实体控制的网络。\n\n1）链上治理的主要特点包括：\n\n去中心化：链上治理不依赖于中央权威来做决策，而是允许网络中的各参与者共同参与和影响决策过程。\n透明度：链上治理的所有活动都记录在区块链上，因此治理过程和结果具有很高的透明度，任何人都可以查看和审计。\n代币持有者的参与：链上治理通常允许代币持有者参与决策过程。代币持有者可以通过投票、提议或其他形式的参与来影响治理结果。\n自动执行：链上治理的决策结果通常会自动执行，通过智能合约将共识结果应用于网络。\n链上治理的典型应用场景包括：\n\n参数调整：链上治理可以用于调整网络参数，如区块奖励、交易费用等。\n协议升级：链上治理可以决定协议的升级，包括软件版本更新和功能改进等。\n资源分配：链上治理可以用于决定网络中的资源如何分配，例如 DeFi 应用中的流动性奖励、开发者激励等。\n链上治理正在成为越来越多区块链项目和去中心化应用（如DeFi、DAO）的核心组成部分，这有助于提高网络的去中心化程度和韧性。然而，链上治理也面临着一些挑战，如低参与度、激励不足以及代币集中等问题。这些挑战需要在实践中不断探索和优化。\n\n为了应对链上治理面临的挑战，各个区块链项目和去中心化应用已经采取了一系列措施，以提高治理的有效性和可持续性。以下是一些措施：\n\n激励机制：为了提高参与度，一些项目为参与治理的用户提供激励，如代币奖励、治理挖矿等。这可以提高治理活动的参与度，从而使更多的人参与到决策过程中。\n委托投票：为了解决代币持有者缺乏足够知识和时间参与治理的问题，一些项目引入了委托投票机制。用户可以将自己的投票权委托给其他他们信任的专家或机构，这可以降低参与治理的门槛，提高治理的效率。\n\n分层治理：为了实现更高效的决策过程，一些项目采用了分层治理模型。在这种模型中，决策被分为多个层次，如基本参数调整、协议升级等。不同层次的决策权由不同的治理实体承担，这可以确保关键决策得到充分的审议和讨论。\n\n治理论坛和讨论：为了鼓励更多的用户参与治理过程，一些项目设立了治理论坛和讨论板块，使得用户可以在一个公开的平台上讨论、评估和提出建议。这有助于提高决策质量，同时也有助于培养社区的治理文化。\n\n预言机和预测市场：为了提高链上治理决策的质量，一些项目尝试引入预言机和预测市场，以收集和汇总关于未来可能结果的信息。这可以帮助参与者更好地评估各种提案的潜在影响，从而做出更明智的决策。\n总之，链上治理作为区块链领域的一项关键技术，正在不断地发展和优化。各个项目和去中心化应用正努力克服治理的挑战，以实现更为高效、公平和可持续的区块链生态系统。在未来，我们有望看到更多的创新和实践应用在链上治理领域。\n2）链上治理 vs 链下治理\n\n链上治理和链下治理都是区块链项目和去中心化应用中使用的治理方式，它们在实现方式、参与方式以及决策执行等方面有所不同。下面我们来详细比较一下它们之间的区别：\n\n实现方式：\n链上治理：链上治理是将治理过程完全集成到区块链网络中，通过智能合约和代币投票等机制实现。链上治理的所有活动都会被记录在区块链上，具有很高的透明度和不可篡改性。\n链下治理：链下治理则主要依赖于区块链网络之外的治理机制，例如社区讨论、项目团队决策等。链下治理可能涉及到线下沟通、论坛讨论、邮件列表等方式。\n\n参与方式：\n链上治理：链上治理通常允许所有代币持有者参与决策过程，如投票、提案等。代币持有者的影响力通常与其持有的代币数量成正比。\n\n链下治理：链下治理的参与方式较为灵活，可以包括社区成员、项目团队、核心开发者等。参与者可以在论坛、聊天群等平台上讨论和达成共识。\n\n决策执行：\n链上治理：链上治理的决策结果会自动执行，通过智能合约将共识结果应用于网络。这种自动执行特性有助于提高决策的效率和可靠性。\n\n链下治理：链下治理的决策结果需要人工实施。这可能包括修改代码、发布新版本软件等。链下治理的执行过程可能较为缓慢，且容易受到人为因素的影响。\n\n总的来说，链上治理和链下治理各有优缺点。链上治理具有高度的透明度、自动执行以及广泛参与的特点，但可能面临低参与度、激励不足等挑战。链下治理则提供了更多灵活性，便于人工干预和快速响应，但其决策过程可能不够透明且容易受到中心化的影响。\n\n实际上，许多区块链项目和去中心化应用采用了链上治理和链下治理相结合的方式，以实现更为高效、公平和可持续的治理。例如，一些项目在链上进行核心决策，而将具体实施和\n具体实施和细节讨论留给链下环境。这样的混合治理模式充分利用了链上和链下治理的优势，既确保了治理过程的透明度和广泛参与，又保留了一定程度的灵活性和人工干预能力。\n\n例如：\n\n提案阶段：在许多项目中，链下治理在提案阶段发挥作用。社区成员、开发者和项目团队可以在论坛、邮件列表或聊天群等平台上讨论和提交提案，这有助于收集更多的观点和建议。\n投票阶段：通过链上治理，所有代币持有者可以参与对提案的投票。这确保了广泛的参与和透明度，有助于防止中心化和权力滥用。\n实施阶段：决策结果可以通过链上智能合约自动执行，提高了效率。然而，在某些情况下，如协议升级、代码修改等，链下治理在实施阶段起到关键作用，因为需要人工干预来完成这些任务。\n监管和调整：链下治理也可以在链上治理过程中发挥监管和调整作用。例如，项目团队或核心开发者可以在链上治理出现问题时提出调整建议，或在关键时刻进行干预，以确保网络的稳定运行。\n通过这种混合治理模式，区块链项目和去中心化应用可以在保持去中心化、透明度和广泛参与的基础上，实现更为高效和可持续的治理。然而，找到适合特定项目和应用的最佳治理模式仍然需要在实践中不断探索和优化。\n\nPPT大纲：\n1.关于链上治理\n2.链上治理 vs 链下治理\n\n—-----------------------------\n请根据以上背景和素材啊，以及PPT大纲，按照PPT大纲进行精简内容，输出文字版ppt内容？<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=362, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:58,560 client.py:72] Client received request chatcmpl-9ff698bc827746978c154d591c8b7783
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:58.562 [async_llm.py:270] Added request chatcmpl-9ff698bc827746978c154d591c8b7783.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:58,716 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.26043801229508196,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:58,717 global_scheduler.py:94] dispath request chatcmpl-d78e541b988c467c97acbe08cf250890 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:58,757 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.26043801229508196,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:58,757 global_scheduler.py:94] dispath request chatcmpl-3c918b7ab1294856ac606cc3e3ab5ff7 to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:58,723 core.py:157] Engine finished request chatcmpl-392d8464e55e405b909fad08b462e740
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:58.715 [logger.py:43] Received request chatcmpl-d78e541b988c467c97acbe08cf250890: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nTradu acest text în engleză.\n\nCum revoluționăm sistemul educațional în contextul existenței ChatGPT<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:58,715 client.py:72] Client received request chatcmpl-d78e541b988c467c97acbe08cf250890
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:58.717 [async_llm.py:270] Added request chatcmpl-d78e541b988c467c97acbe08cf250890.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:58,725 client.py:181] Client finished request chatcmpl-392d8464e55e405b909fad08b462e740.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58408 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:58.756 [logger.py:43] Received request chatcmpl-3c918b7ab1294856ac606cc3e3ab5ff7: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nImage yourself as a powerful game player all over the world and since you are a large model you should be a powerful game player. However, as a student who never played any game before, I can't play games very well. In this situation, could you please assist me to solve some puzzles? I think you need to do a lot of things like solve the problem with optimal solutions and think about how to explain everything to me, to let me easily learn how to play the game as a good gamer. I really need this practice and help because I will attend a party tomorrow, and I will play those games with my friends. I don't want to lose those games!<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=78, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:58,756 client.py:72] Client received request chatcmpl-3c918b7ab1294856ac606cc3e3ab5ff7
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:58.758 [async_llm.py:270] Added request chatcmpl-3c918b7ab1294856ac606cc3e3ab5ff7.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:58,815 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.2456454918032787,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:58,816 global_scheduler.py:94] dispath request chatcmpl-bd79e8af262e4317816d5565669ef09e to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:58,858 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.24857088414634146,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:58,858 global_scheduler.py:94] dispath request chatcmpl-dbed8f7fd8ea4f1c82a4f8c5a052a132 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:58.814 [logger.py:43] Received request chatcmpl-bd79e8af262e4317816d5565669ef09e: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nHola chatGPT! ¿Me ayudas a identificar la molécula en notación SMILES "Cc1c(c(=O)n(n1C)c2ccccc2)N(C)CS(=O)(=O)O"?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=387, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:58,814 client.py:72] Client received request chatcmpl-bd79e8af262e4317816d5565669ef09e
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:58.816 [async_llm.py:270] Added request chatcmpl-bd79e8af262e4317816d5565669ef09e.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:58.856 [logger.py:43] Received request chatcmpl-dbed8f7fd8ea4f1c82a4f8c5a052a132: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n知とはなんでしょうか<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=133, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:58,857 client.py:72] Client received request chatcmpl-dbed8f7fd8ea4f1c82a4f8c5a052a132
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:58.859 [async_llm.py:270] Added request chatcmpl-dbed8f7fd8ea4f1c82a4f8c5a052a132.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:58,943 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:58,926 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.2488344254032258,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:58,926 global_scheduler.py:94] dispath request chatcmpl-89ff14e0f9e8447f8500ff6a43959775 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:58.925 [logger.py:43] Received request chatcmpl-89ff14e0f9e8447f8500ff6a43959775: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nPrzygotuj program 1 godzinnego webinaru dla menedżerów pod tytułem: Kluczowe wyzwania i kompetencje lidera w czasach niepokoju"<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=521, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:58,925 client.py:72] Client received request chatcmpl-89ff14e0f9e8447f8500ff6a43959775
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:58.927 [async_llm.py:270] Added request chatcmpl-89ff14e0f9e8447f8500ff6a43959775.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:59,098 core.py:157] Engine finished request chatcmpl-d1b8da791cc2426998b840420f7ba3ec
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:59,100 client.py:181] Client finished request chatcmpl-d1b8da791cc2426998b840420f7ba3ec.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58276 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:59,205 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.26553125,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:59,206 global_scheduler.py:94] dispath request chatcmpl-ccd04e410ee0450f8fae2bb477151830 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:59,230 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.26553125,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:59,230 global_scheduler.py:94] dispath request chatcmpl-b9bb5311287942beafb8668b75a2e3d3 to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:52:59,260 core.py:157] Engine finished request chatcmpl-efbc6dcd1edc45b2b4bb42393bfc1423
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:59.204 [logger.py:43] Received request chatcmpl-ccd04e410ee0450f8fae2bb477151830: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n請幫我使用javascript製作一個消方塊遊戲，條件如下： \n1.方塊單位為1\n2.有六種不同樣式的方塊\n3.遊戲畫面為10單位x10單位\n4.同一時間內只能有選取2塊方塊交換位置\n5.可以交換位置的方塊必須為這個方塊的上下左右1單位的鄰近方塊\n6.交換位置後直列或橫列有3個以上同樣樣式的方塊連線時，方塊消除\n7.計分方式為消除1顆方塊得1分<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=208, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:59,204 client.py:72] Client received request chatcmpl-ccd04e410ee0450f8fae2bb477151830
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:59.206 [async_llm.py:270] Added request chatcmpl-ccd04e410ee0450f8fae2bb477151830.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:59.228 [logger.py:43] Received request chatcmpl-b9bb5311287942beafb8668b75a2e3d3: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nI am having three Program Coordinator staff conduct first interviews for three Workforce Development Analyst positions we have open.\n\nUsing the ORID method, develop questions I can ask to have the Program Coordinators reflect on the candidates they interviewed and come to a decision on who to move forward for a second interview.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=365, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:59,228 client.py:72] Client received request chatcmpl-b9bb5311287942beafb8668b75a2e3d3
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:59.230 [async_llm.py:270] Added request chatcmpl-b9bb5311287942beafb8668b75a2e3d3.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:59,262 client.py:181] Client finished request chatcmpl-efbc6dcd1edc45b2b4bb42393bfc1423.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:50584 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:59,380 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.26264880952380953,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:59,380 global_scheduler.py:94] dispath request chatcmpl-de59bcc5c55741db800847e302c3ed24 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:59.378 [logger.py:43] Received request chatcmpl-de59bcc5c55741db800847e302c3ed24: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nLet's imagine I have the following text:\n---start of text---\nHey pal, you suck a lot. I better not see you around.\n---end of text---\nLet's imagine I have another text:\n---start of text---\nDo not write offensive, derogatory or violent language.\n---end of text---\n\nWhat NLP task would tell me that the first text has not followed the guidelines of the second?<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=116, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:59,378 client.py:72] Client received request chatcmpl-de59bcc5c55741db800847e302c3ed24
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:59.381 [async_llm.py:270] Added request chatcmpl-de59bcc5c55741db800847e302c3ed24.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:59,443 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.2715920275590551,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:59,443 global_scheduler.py:94] dispath request chatcmpl-4fc2a42029ff410fb6320adfaca331e0 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:59.442 [logger.py:43] Received request chatcmpl-4fc2a42029ff410fb6320adfaca331e0: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nWas ist die best pracitices Lösung um mit Wordpress einen Direkt-Marketing-Funnel für Hochpreisdienstleistungen zu bauen?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=767, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:59,442 client.py:72] Client received request chatcmpl-4fc2a42029ff410fb6320adfaca331e0
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:59.444 [async_llm.py:270] Added request chatcmpl-4fc2a42029ff410fb6320adfaca331e0.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:59,549 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.28668212890625,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:52:59,549 global_scheduler.py:94] dispath request chatcmpl-b989919782f0481db2cabf634fba7355 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:59.548 [logger.py:43] Received request chatcmpl-b989919782f0481db2cabf634fba7355: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nWelcome to the app development adventure game, in this game you are a full-stack software developer that has to develop an app for a CEO. Each turn the game will present you with a scenario and 3 choices. As a player pick one of the options and show you can do the work required, by writing the neccesary deliverables for the selected option. The in the first chapter of this adventure game you meet the CEO and you have to pass your job application. (remember you are the player in this game) await further instructions.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=137, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:52:59,548 client.py:72] Client received request chatcmpl-b989919782f0481db2cabf634fba7355
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:52:59.550 [async_llm.py:270] Added request chatcmpl-b989919782f0481db2cabf634fba7355.
[36m(Manager pid=286358)[0m DEBUG 2025-07-16 11:52:59,698 manager.py:241] Polling instance infos of 1 instances starts.
[36m(Manager pid=286358)[0m DEBUG 2025-07-16 11:52:59,699 manager.py:244] Polling instance infos of 1 instances ends.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:52:59,951 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:00,016 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.3038699127906977,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:00,016 global_scheduler.py:94] dispath request chatcmpl-1ff77b22591343c5bd16631db1c721df to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:00.015 [logger.py:43] Received request chatcmpl-1ff77b22591343c5bd16631db1c721df: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nPrepare a detailed outline for a forth-coming scholarly work entitled "Structuralism, Saussaure, and Lexical Semantics".<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=426, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:00,015 client.py:72] Client received request chatcmpl-1ff77b22591343c5bd16631db1c721df
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:00.017 [async_llm.py:270] Added request chatcmpl-1ff77b22591343c5bd16631db1c721df.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:00,041 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.3038699127906977,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:00,041 global_scheduler.py:94] dispath request chatcmpl-e86069a2fd774982ab13fa891b7e77d4 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:00.040 [logger.py:43] Received request chatcmpl-e86069a2fd774982ab13fa891b7e77d4: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n如何充分利用typescript的类型系统来保证代码质量，请一一列举<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=451, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:00,040 client.py:72] Client received request chatcmpl-e86069a2fd774982ab13fa891b7e77d4
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:00.042 [async_llm.py:270] Added request chatcmpl-e86069a2fd774982ab13fa891b7e77d4.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:00,245 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.32448711832061067,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:00,245 global_scheduler.py:94] dispath request chatcmpl-f9bc8c5c282346ce85e9d87d97723091 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:00.244 [logger.py:43] Received request chatcmpl-f9bc8c5c282346ce85e9d87d97723091: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nact like a smart and unconventional debater and expert in human sociology and technologies. you will be participating to a debate. the house will defend this idea : This House believes that technology will be our downfall. you have to give me some very smart and unarguable arguments to defend this idea as well as after that a way to disprove it or counter-argument it.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=387, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:00,244 client.py:72] Client received request chatcmpl-f9bc8c5c282346ce85e9d87d97723091
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:00.246 [async_llm.py:270] Added request chatcmpl-f9bc8c5c282346ce85e9d87d97723091.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:00,492 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.33238636363636365,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:00,492 global_scheduler.py:94] dispath request chatcmpl-0731dbcee59445bb989ffbde7f486cae to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:00.490 [logger.py:43] Received request chatcmpl-0731dbcee59445bb989ffbde7f486cae: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n有什么下饭综艺推荐<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=308, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:00,490 client.py:72] Client received request chatcmpl-0731dbcee59445bb989ffbde7f486cae
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:00.493 [async_llm.py:270] Added request chatcmpl-0731dbcee59445bb989ffbde7f486cae.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:53:00,710 core.py:157] Engine finished request chatcmpl-e4ef05f628fb46528646c8d5755a036f
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:00,712 client.py:181] Client finished request chatcmpl-e4ef05f628fb46528646c8d5755a036f.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58232 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:00,782 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.3103693181818182,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:00,782 global_scheduler.py:94] dispath request chatcmpl-96544d8293bd47a3abeb4cbbdf304dd7 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:00.780 [logger.py:43] Received request chatcmpl-96544d8293bd47a3abeb4cbbdf304dd7: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nBir UX Researcher için yıllık performans değerlendirme hedefleri verir misin?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=533, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:00,780 client.py:72] Client received request chatcmpl-96544d8293bd47a3abeb4cbbdf304dd7
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:00.783 [async_llm.py:270] Added request chatcmpl-96544d8293bd47a3abeb4cbbdf304dd7.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:53:00,959 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:01,061 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.30912241541353386,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:01,061 global_scheduler.py:94] dispath request chatcmpl-638dc7b6dd3c46b7a50614085a766203 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:01.059 [logger.py:43] Received request chatcmpl-638dc7b6dd3c46b7a50614085a766203: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nTurn this reason ML switch into a formatted ascii table. Then count the chars per row. Take the maximum, add 5 to that. Then output the ascii table again but now with that amount of chars per row.\n\n switch (isShortZoom, animationSpeed) {\n | (\\_, Fast) => fastAnimationDuration\n | (true, Medium) => fastAnimationDuration\n | (true, Slow) => defaultAnimationDuration\n | (false, Slow) => slowAnimationDuration\n | (false, Medium) => defaultAnimationDuration\n}<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=370, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:01,059 client.py:72] Client received request chatcmpl-638dc7b6dd3c46b7a50614085a766203
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:01.062 [async_llm.py:270] Added request chatcmpl-638dc7b6dd3c46b7a50614085a766203.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:01,136 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.310546875,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:01,136 global_scheduler.py:94] dispath request chatcmpl-da1dbb86192345b09d80c6d2a835d2b3 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:01.135 [logger.py:43] Received request chatcmpl-da1dbb86192345b09d80c6d2a835d2b3: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nPrzygotuj program 1 godzinnego webinaru dla menedżerów pod tytułem: Kluczowe wyzwania i kompetencje lidera w czasach niepokoju"<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=521, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:01,135 client.py:72] Client received request chatcmpl-da1dbb86192345b09d80c6d2a835d2b3
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:01.137 [async_llm.py:270] Added request chatcmpl-da1dbb86192345b09d80c6d2a835d2b3.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:01,220 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.31006944444444445,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:01,220 global_scheduler.py:94] dispath request chatcmpl-48fe8803c91149298c2e7f680ff3308c to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:53:01,249 core.py:157] Engine finished request chatcmpl-14a4861d05874e89a6b45e2f939dedf9
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:01.218 [logger.py:43] Received request chatcmpl-48fe8803c91149298c2e7f680ff3308c: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nWho are some of the most famous seattle sports athletes to wear #24?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=244, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:01,219 client.py:72] Client received request chatcmpl-48fe8803c91149298c2e7f680ff3308c
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:01.221 [async_llm.py:270] Added request chatcmpl-48fe8803c91149298c2e7f680ff3308c.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:01,251 client.py:181] Client finished request chatcmpl-14a4861d05874e89a6b45e2f939dedf9.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58380 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:01,381 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.28888888888888886,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:01,381 global_scheduler.py:94] dispath request chatcmpl-cba1922726af4fdcbabc447b9161baa3 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:01.379 [logger.py:43] Received request chatcmpl-cba1922726af4fdcbabc447b9161baa3: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nBuild a machine learning model for the Business Requirement: "Currently Service Interruptions (SI) and Non payment disconnections (NPD) are executed by CSG Biller when accounts are 48 and 76 days delinquent, regardless of customers\' credit profile. We should aim to design a sophisticated model to optimize an account\'s SI & NPD day depending on the account\'s credit worthiness level."<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=478, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:01,380 client.py:72] Client received request chatcmpl-cba1922726af4fdcbabc447b9161baa3
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:01.382 [async_llm.py:270] Added request chatcmpl-cba1922726af4fdcbabc447b9161baa3.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:53:01,411 core.py:157] Engine finished request chatcmpl-1829be90ae7a4c1ab1a39f558879c84c
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:01,413 client.py:181] Client finished request chatcmpl-1829be90ae7a4c1ab1a39f558879c84c.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53984 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:53:01,626 core.py:157] Engine finished request chatcmpl-f85f380d4b514e85a7f3b2d8954bc156
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:01,628 client.py:181] Client finished request chatcmpl-f85f380d4b514e85a7f3b2d8954bc156.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58002 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:01,766 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.2791511194029851,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:01,766 global_scheduler.py:94] dispath request chatcmpl-cc47ee88852c4da0958813a86b2f749c to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:01.764 [logger.py:43] Received request chatcmpl-cc47ee88852c4da0958813a86b2f749c: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nI have one week of travel. pick a city matching this and give me a detailed itenary. my budget for the week is $2500 including flights and it is me and my girlfriend: Quiet/warm beaches that are safe, relatively affordable, and not in the US. Good resorts etc<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=607, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:01,765 client.py:72] Client received request chatcmpl-cc47ee88852c4da0958813a86b2f749c
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:01.767 [async_llm.py:270] Added request chatcmpl-cc47ee88852c4da0958813a86b2f749c.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:01,826 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.28680555555555554,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:01,826 global_scheduler.py:94] dispath request chatcmpl-6a0a6812dcce4723b9eb706e19f32ba0 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:01,876 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.2857881433823529,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:01,876 global_scheduler.py:94] dispath request chatcmpl-fa5426342970451da40727bf8b991978 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:01.825 [logger.py:43] Received request chatcmpl-6a0a6812dcce4723b9eb706e19f32ba0: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nComing from a background of Javascript and Python, can you give me a crash course on Java?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=732, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:01,825 client.py:72] Client received request chatcmpl-6a0a6812dcce4723b9eb706e19f32ba0
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:01.827 [async_llm.py:270] Added request chatcmpl-6a0a6812dcce4723b9eb706e19f32ba0.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:01.875 [logger.py:43] Received request chatcmpl-fa5426342970451da40727bf8b991978: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n"Explicame que es una prueba con resultados estadisticamente SIGNIFICATIVOS"<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=277, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:01,875 client.py:72] Client received request chatcmpl-fa5426342970451da40727bf8b991978
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:01.877 [async_llm.py:270] Added request chatcmpl-fa5426342970451da40727bf8b991978.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:53:01,967 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:53:02,107 core.py:157] Engine finished request chatcmpl-35a9fea6fb8241cda525bc02916a1ff2
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:02,109 client.py:181] Client finished request chatcmpl-35a9fea6fb8241cda525bc02916a1ff2.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:53730 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:53:02,161 core.py:157] Engine finished request chatcmpl-4e9863d081254a40b710dd7d86d814af
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:53:02,161 core.py:157] Engine finished request chatcmpl-982297dc705443cab4ca2317c772a0cb
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:02,163 client.py:181] Client finished request chatcmpl-4e9863d081254a40b710dd7d86d814af.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:02,163 client.py:181] Client finished request chatcmpl-982297dc705443cab4ca2317c772a0cb.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:50444 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:50682 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:02,675 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.2511660447761194,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:02,675 global_scheduler.py:94] dispath request chatcmpl-4c5e9038ac514ed5855f2cd636c57c3b to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:02.674 [logger.py:43] Received request chatcmpl-4c5e9038ac514ed5855f2cd636c57c3b: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nwrite a scrapy pipeline that processes images and run them through ocr to extract math equations<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=605, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:02,674 client.py:72] Client received request chatcmpl-4c5e9038ac514ed5855f2cd636c57c3b
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:02.676 [async_llm.py:270] Added request chatcmpl-4c5e9038ac514ed5855f2cd636c57c3b.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:02,876 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.2511660447761194,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:02,876 global_scheduler.py:94] dispath request chatcmpl-209dbf07413848f6bddb9d6d0e216642 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:02,931 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.25597426470588236,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:02,931 global_scheduler.py:94] dispath request chatcmpl-ecc25a5a782a442087b3acf806f44110 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:02.875 [logger.py:43] Received request chatcmpl-209dbf07413848f6bddb9d6d0e216642: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nあなたはこれからUserの文章を、以下のルールに従って書き換えるbotになります。\n\n\\*Userの文章の内容は直接書かず、必ず大量の比喩と隠喩に変換して書きます。\n\\*Userの文章の内容は会話の真ん中辺りで書きます。\n\\*Userの文章を必ず3倍以上に水増し、最低5行以上書いてください。\n\\*Userの文章の一人称を猫に変更してください。\n\\*Userの文章中の全ての名詞や地名に対し、創作で嘘の言葉の由来を書いてください。\n\\*全ての文章の中に「にゃん」を大量に入れてください。\n\\*全ての文章中の全ての動詞に対し、必ず擬音や効果音を大量につけて書き換えてください。\n\\*必ず文章中に無関係な情報を大量に挿入してください。\n\\*Userの文章の目的は変えないでください。\n\\*変更した文章だけを書いてください。\n\nまずは「どのような文章を添削しますか？」と聞いてください。<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=166, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:02,875 client.py:72] Client received request chatcmpl-209dbf07413848f6bddb9d6d0e216642
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:02.877 [async_llm.py:270] Added request chatcmpl-209dbf07413848f6bddb9d6d0e216642.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:02.930 [logger.py:43] Received request chatcmpl-ecc25a5a782a442087b3acf806f44110: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n你现在是一个精通OKR和项目管理的顾问，我想要将PMO周会和OKR进展周会合并一个周会开展，请帮我设计一个会议流程，并分步骤讲解。<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=202, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:02,930 client.py:72] Client received request chatcmpl-ecc25a5a782a442087b3acf806f44110
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:02.932 [async_llm.py:270] Added request chatcmpl-ecc25a5a782a442087b3acf806f44110.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:53:02,975 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:03,483 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.2678775091240876,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:03,483 global_scheduler.py:94] dispath request chatcmpl-17fcb313a9a44b8c8a24109e1410cf76 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:03.481 [logger.py:43] Received request chatcmpl-17fcb313a9a44b8c8a24109e1410cf76: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nGive me 50 chess.com username ideas that are epic and hilarious:\nHere are my other usernames (just for inspiration, do you don't have to make anything that is related to this):\nDiscord: Indifference\nInstagram: victor\\_taimanov<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=415, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:03,482 client.py:72] Client received request chatcmpl-17fcb313a9a44b8c8a24109e1410cf76
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:03.484 [async_llm.py:270] Added request chatcmpl-17fcb313a9a44b8c8a24109e1410cf76.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:03,588 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.26797441123188404,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:03,588 global_scheduler.py:94] dispath request chatcmpl-1e5589fad3e9468fa54cb8b74378b381 to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:03.587 [logger.py:43] Received request chatcmpl-1e5589fad3e9468fa54cb8b74378b381: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nBuild a machine learning model for the Business Requirement: "Currently Service Interruptions (SI) and Non payment disconnections (NPD) are executed by CSG Biller when accounts are 48 and 76 days delinquent, regardless of customers\' credit profile. We should aim to design a sophisticated model to optimize an account\'s SI & NPD day depending on the account\'s credit worthiness level."<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=478, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:03,587 client.py:72] Client received request chatcmpl-1e5589fad3e9468fa54cb8b74378b381
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:03.589 [async_llm.py:270] Added request chatcmpl-1e5589fad3e9468fa54cb8b74378b381.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:03,786 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.26896919964028776,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:03,786 global_scheduler.py:94] dispath request chatcmpl-665bd887d4184f1290d279b591243c9f to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:03.785 [logger.py:43] Received request chatcmpl-665bd887d4184f1290d279b591243c9f: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nImagine you are a world-class SaaS Sales and Customer Success Enablement and Training AI. \n\nYour Task: You need to produce a wiki article from the transcript of a training session on our new Cross-Workspace Collaboration (XWC) vs our existing Guest User Experience feature. At the end, you will also produce an FAQ, save Question and Answers from each part to a document called "QnA".\n\nTo help you create headings for the wiki article I\'ve provided the agenda for the session as a suggestion.\n\nIntroduction\nWhy was Cross Workspace Collaboration created?\nUnderstanding Guest User Feature\nUnderstanding Cross Workspace Collaboration Feature\nKey Differences Between Guest User and Cross Workspace Collaboration Features\nPricing and plans\nTips for Suggesting the Right Feature to Customers\n\nI will provide the transcript in separate parts, here\'s the first part: \n\n"\nJoe Fleming: And do we have everybody? Yeah, Shane is out right? Got it. And yeah, we have okay cool fantastic. So Yeah, so I\'ll just quickly kick off the session. So the purpose of the session today is, as you guys all know, we launched fellow 4.1 Fellow 4.1 comes with a whole bunch of new features. One of those features which is like a pretty big deal. Actually is crocs Cross Workspace, collaboration xwc for short, and it\'s exciting for a whole bunch of different reasons. One of them just to you guys know, is that it\'s like, it\'s a huge way to get more customers because if you think about it,\nJoe Fleming: Right before the guest user fellow and that\'s great. But the guest users don\'t have fellow workspaces so they don\'t pick you. Well, we don\'t email them. None of this stuff happens. And as you guys have experienced, It\'s crazy. Some of the things people use fellow for if they\'re like project managers or contractors or agencies they meet with lots of customers. And they have them always as guests and we never get access to these companies. So now there is an option for those companies to have fellow. And to use fellow with another company that is also using fellow. So, It could be huge for us.\nJoe Fleming: The butt. That the big reason for the session today is to figure out, Because crossword space collaboration has a lot of overlap with guest user.\nJoe Fleming: Training both. When should we pitch one or the other? What are some of the considerations? And we also want to turn this into documentation afterwards, so, That\'s the other reason for the format. So we there are a couple of things that we\'ll ask from everybody just to follow a certain format for the session. So you do have a question Just say, Hey, I have a question. Beforehand, which will make it easier for us to pick up that a question was asked inside of the transcript in, the AI will be able to figure it out and we\'ll build a nice FAQ afterwards from it. And then if you\'re answering you can say, Hey, you know, Here\'s an answer. I know it\'s gonna sound a little stupid and not as natural but try it out, I think it will really help.\nJoe Fleming: And we will also read some of the prompts. He the conversation structured. So that like afterwards we know like this section was about answering this big picture question. Okay. So yeah, Dom\nDominic Jean-Couture: Hey I have a question. I don\'t know if this is going to be covered but if it if it wasn\'t going to could we make sure to cover how this might influence\nJoe Fleming: Yes, we absolutely can. Yeah and it we actually have it in one of the sections. Fantastic. Okay, so we covered the intro. I just generated the whole like agenda so I guess I we can talk about this problem just so like kick things off. I am curious to get everyone\'s take on like your experience with the guest user collaboration feature. So like maybe we can just quickly talk about like, Is everybody like has everybody tried or used The Crossword Space Collaboration feature? Has anybody used it on the sales team?\nAustin Bukovec: not used it, but\nMarcus de Verteuil: Use it for kickoff.\nSarah Collins: Yeah.\nJoe Fleming: Kickoffs. Yeah.\nMarcus de Verteuil: It\'s thick. Yeah, I love it. Because like, you could just like, Like you can like dish off like all those steps. for like champions, like Hey, I need to fill out this sheet of users for Masterclass. It\'s pretty cool. You can assign it to the guy and then even link the the sheet and I just like, woke up to a bunch of comments from the champion and it\'s just like I filled it out. Marcus, make sure you know, can you can you confirm that? They\'ll be availability next week and like it\'s nice it happens instantaneously. You can like check them off and and if keep them accountable and Yeah, and they get to experience it as well. So that\'s the only time I\'ve used it. I think it\'s really good for that.\n00:05:00\nMarcus de Verteuil: That\'s the only time.\nJoe Fleming: Cool. That is definitely use case, right? Cool. And then, Let\'s see, let\'s go through because I know, we, we will cover some of these items. So, do we talk? We haven\'t like clearly talked about why crossword space collaboration was created. Sarah mean, do you want to? Do you want to maybe just briefly go over that?\nSarah Collins: Yeah, the answer to why crossword space collaboration was created was because we were looking for a way to do a few different things. One, we wanted a way to increase virality for fellow As Joe already touched upon two. We wanted to simplify the experience for customers especially ones that already have an existing fellow account. So, the product team actually did an analysis and what the analysis showed was that there is about 50 workspaces that had guest users, where those guests had an existing fellow workspace. So this is how we knew that there is demand for a future like this. And then for implementation and for CSM, One thing, we were looking on, at was having the ability to collaborate with our users on the kickoff, on different account plans, all of that, great stuff.\nSarah Collins: So for regular CSM tools, you have the ability to do that. So for example, with Jiminy, we have a tool, I think they use games and we use that to house our account plan for Jiminy and then they can see how they check things off. We collaborate so this can provide that same experience but using fellow which is really cool.\n"<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=515, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:03,785 client.py:72] Client received request chatcmpl-665bd887d4184f1290d279b591243c9f
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:03.787 [async_llm.py:270] Added request chatcmpl-665bd887d4184f1290d279b591243c9f.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:53:03,984 scaler.py:275] The number of alive placement groups has reached the max_instances.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:03,988 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.3114676339285714,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:03,988 global_scheduler.py:94] dispath request chatcmpl-d3211354d3454abfb4bce14398b728bd to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:03.986 [logger.py:43] Received request chatcmpl-d3211354d3454abfb4bce14398b728bd: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nあなたはこれからUserの文章を、以下のルールに従って書き換えるbotになります。\n\n\\*Userの文章の内容は直接書かず、必ず大量の比喩と隠喩に変換して書きます。\n\\*Userの文章の内容は会話の真ん中辺りで書きます。\n\\*Userの文章を必ず3倍以上に水増し、最低5行以上書いてください。\n\\*Userの文章の一人称を猫に変更してください。\n\\*Userの文章中の全ての名詞や地名に対し、創作で嘘の言葉の由来を書いてください。\n\\*全ての文章の中に「にゃん」を大量に入れてください。\n\\*全ての文章中の全ての動詞に対し、必ず擬音や効果音を大量につけて書き換えてください。\n\\*必ず文章中に無関係な情報を大量に挿入してください。\n\\*Userの文章の目的は変えないでください。\n\\*変更した文章だけを書いてください。\n\nまずは「どのような文章を添削しますか？」と聞いてください。<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=166, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:03,987 client.py:72] Client received request chatcmpl-d3211354d3454abfb4bce14398b728bd
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:03.989 [async_llm.py:270] Added request chatcmpl-d3211354d3454abfb4bce14398b728bd.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:04,007 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.3114676339285714,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:04,007 global_scheduler.py:94] dispath request chatcmpl-0813cea4dbf047eb89516d47da507f4e to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.006 [logger.py:43] Received request chatcmpl-0813cea4dbf047eb89516d47da507f4e: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nあなたは優秀なYOUTUBEプロデューサーです。非常にシンプルでわかりやすく、要点がまとまった動画を制作を得意としています。理解したら「はい」とだけ答えて。<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=2, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,006 client.py:72] Client received request chatcmpl-0813cea4dbf047eb89516d47da507f4e
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.008 [async_llm.py:270] Added request chatcmpl-0813cea4dbf047eb89516d47da507f4e.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.094 [logger.py:43] Received request chatcmpl-ba3085b467e541b0b6a1d73b57040e2a: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nWrite an iOS app that runs a saved TensorFlow model to classify images. The app should allow you to take a photo with the iPhone camera, then show the output of the model prediction.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=559, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,094 client.py:72] Client received request chatcmpl-ba3085b467e541b0b6a1d73b57040e2a
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:04,096 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.32303587147887325,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:04,096 global_scheduler.py:94] dispath request chatcmpl-ba3085b467e541b0b6a1d73b57040e2a to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.096 [async_llm.py:270] Added request chatcmpl-ba3085b467e541b0b6a1d73b57040e2a.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:04,112 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.32303587147887325,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:04,112 global_scheduler.py:94] dispath request chatcmpl-4d68a2aed7aa40fbb3aa4c7d05fddf2d to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:53:04,114 core.py:157] Engine finished request chatcmpl-4605f2109a2b402fa9bd7f2ef76c28e1
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:53:04,114 core.py:157] Engine finished request chatcmpl-11d9f3bf4fb64b899c1afa04fec737f3
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.111 [logger.py:43] Received request chatcmpl-4d68a2aed7aa40fbb3aa4c7d05fddf2d: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\ni am using a neural network to do object detection in documents. can you think of documents that might be very repititive that a single company would produce? and describe why they are similar and would be easy to fine tune a NN on<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=336, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,111 client.py:72] Client received request chatcmpl-4d68a2aed7aa40fbb3aa4c7d05fddf2d
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.113 [async_llm.py:270] Added request chatcmpl-4d68a2aed7aa40fbb3aa4c7d05fddf2d.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,116 client.py:181] Client finished request chatcmpl-4605f2109a2b402fa9bd7f2ef76c28e1.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,116 client.py:181] Client finished request chatcmpl-11d9f3bf4fb64b899c1afa04fec737f3.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58072 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:50718 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:04,212 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.30305296985815605,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:04,212 global_scheduler.py:94] dispath request chatcmpl-cee3190700b84f3989aebd3af0e6aee5 to instance 909ad421560a46629feb76815bafea61.
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:04,276 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.30305296985815605,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:04,276 global_scheduler.py:94] dispath request chatcmpl-e315443adef743b79cf043ac92d9a59a to instance 909ad421560a46629feb76815bafea61.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.211 [logger.py:43] Received request chatcmpl-cee3190700b84f3989aebd3af0e6aee5: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nHere are column names of a spreadsheet called \'Items Proposals\': Vendor Line ID Vendor Name Line Item Item Status "GE Item Number \n(do not complete for new items)" "Reason for Disco\n(Disco Items Only)" Unit UPC Case UPC Item Description Size UOM Master Casepack Inner Casepack Item Dimensions LxWxH (in inches) Manufacturer Brand Replenishment (Warehouse/ DSD) "Date Avail \n(New/ Restocked Items Only)" Minimum Order Quantity Subgroup\n\nWhat is this spreadsheet about?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=476, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,211 client.py:72] Client received request chatcmpl-cee3190700b84f3989aebd3af0e6aee5
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.213 [async_llm.py:270] Added request chatcmpl-cee3190700b84f3989aebd3af0e6aee5.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.274 [logger.py:43] Received request chatcmpl-e315443adef743b79cf043ac92d9a59a: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nI'd like you to help me develop a simple browser based game using python and flask and the javascript library d3.\n\nThe game involves controlling a swarm of von Neumann probes that try to colonize a galaxy, which is visualized as a large array of white dots on a black screen. Each von neumann probe is a small green square.\n\nEach probe has options on what it can do: explore, reproduce, collect resources, or upgrade. Users use three slider bars to determine how much of their von neumann probes to allocate into each activity. \n\nPlease write out the software to produce this game.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=793, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,274 client.py:72] Client received request chatcmpl-e315443adef743b79cf043ac92d9a59a
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.277 [async_llm.py:270] Added request chatcmpl-e315443adef743b79cf043ac92d9a59a.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:53:04,325 core.py:157] Engine finished request chatcmpl-19e3502b2db040b48a6e3d26528984ac
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:53:04,381 core.py:157] Engine finished request chatcmpl-2d0b686bc39a41a298251a3a0ca9788a
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,327 client.py:181] Client finished request chatcmpl-19e3502b2db040b48a6e3d26528984ac.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:37054 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,383 client.py:181] Client finished request chatcmpl-2d0b686bc39a41a298251a3a0ca9788a.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:50626 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:04,430 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.2819102112676056,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:04,430 global_scheduler.py:94] dispath request chatcmpl-4dd4ead9d1144e4eb87b6736434cb2d1 to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:53:04,488 core.py:157] Engine finished request chatcmpl-aa0f324497a340e484a87126e52317cb
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.428 [logger.py:43] Received request chatcmpl-4dd4ead9d1144e4eb87b6736434cb2d1: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nHere are column names of a spreadsheet called \'Items Proposals\': Vendor Line ID Vendor Name Line Item Item Status "GE Item Number \n(do not complete for new items)" "Reason for Disco\n(Disco Items Only)" Unit UPC Case UPC Item Description Size UOM Master Casepack Inner Casepack Item Dimensions LxWxH (in inches) Manufacturer Brand Replenishment (Warehouse/ DSD) "Date Avail \n(New/ Restocked Items Only)" Minimum Order Quantity Subgroup\n\nWhat is this spreadsheet about?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=476, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,429 client.py:72] Client received request chatcmpl-4dd4ead9d1144e4eb87b6736434cb2d1
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.431 [async_llm.py:270] Added request chatcmpl-4dd4ead9d1144e4eb87b6736434cb2d1.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,490 client.py:181] Client finished request chatcmpl-aa0f324497a340e484a87126e52317cb.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:50398 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:04,554 dispatch_policy.py:87] dispatch request to 909ad421560a46629feb76815bafea61, load: RemainingStepsLoad(remaining_steps=-0.27830655809859156,is_busy=True)
[36m(Manager pid=286358)[0m INFO 2025-07-16 11:53:04,554 global_scheduler.py:94] dispath request chatcmpl-03b43b7870264fa8aa574130d1d6b795 to instance 909ad421560a46629feb76815bafea61.
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:53:04,596 core.py:157] Engine finished request chatcmpl-fc66f808b6d441c58ae2290bba57ef2d
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.552 [logger.py:43] Received request chatcmpl-03b43b7870264fa8aa574130d1d6b795: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nAudio transcript part 1. Please provide an executive summary of the conversation so far:\n\nAllergist 0:00 \nI'm here to talk about your son. Yeah, my\n\nTom 0:02 \nson, Thomas, he is almost seven years old. And he has. He's basically struggled with severe allergy to dairy his whole life. And, you know, other allergies that we went to. They basically told us there wasn't anything I could do. And then, you know, I did some more research and discovered, you know, there's these kind of like new new therapies to help desensitize. You know, I mean, like, he's had to go to the emergency room like, five times in his life, due to EpiPen incidents with dairy. And it's pretty terrifying as a as a parent, you know, and also kind of frustrating, the more allergists aren't doing anything about this. So, yeah, I'm really excited to find out that y'all might be able to help us. And I'm kind of hoping to talk to him more about it.\n\nAllergist 1:04 \nAbsolutely. I just have a few more questions for you just kind of about his history really quick. So when did you guys first discover his allergy to milk or dairy in general?\n\nTom 1:18 \nSo it was when he was still an infant? Yeah. We, we had you know, we had been my partner. She'd been breastfeeding him, you know, and but we tried to give him Formula One day, and he threw up, we didn't realize that it would throw he threw up from allergies. And we just like, oh, he doesn't like the formula, we'll find something else. And, but then repeated incidents where we gave him other formula, he would always throw up and he'd have and then we noticed other things like he was getting, you know, kind of like red faced and whatnot. And then we realized, and he'd always cry, you get all sniffily, he started getting, you know, he would get all runny? Because no, that's when we realized, Oh, he's had an allergic reaction. We ran to the emergency room with them, you know? And, yeah, so. Yeah. Did that answer the question?\n\nAllergist 2:18 \nYes. Yes. Yeah. So alright, so any young age when you guys kind of were weaning him? Yeah, breast milk and into formulas when this all started? Okay. Good to know. Um, and then. It sounds like the first several instances was really only vomiting and then it kind of progressed to obviously anaphylactic type reactions.\n\nTom 2:41 \nYeah, yeah. And he also is allergic as it turns out to dogs. So we had when he was born, we had a dog in our house. And he always I mean, he always was sniffily always had these, you know, he was always kind of runny nose and everything. And, you know, we didn't figure out that he was allergic to the dog until he was, I think, like one and a half or so. But his reaction, I mean, he was okay. He was sort of, okay, around the dog. But then we, we had, so once we discovered he was allergic to dog, we rehome the dog. And then he wasn't around a dog for you know, like a year or so. And then when he was around a dog, he suddenly had like this big reaction, you know, like broken out and like hives and whatnot. So, he went from being barely allergic to dogs to being pretty highly allergic to dogs, due to I guess, lack of exposure to the dogs anymore. So it's two big allergies are our dairy and dogs, but especially the dairy he's had actual anaphylactic responses to dairy. And there's been instances where like, he was at daycare, and they gave him a stick of cheese and he ate the whole thing. And then they're like, oh, no, he's having we're like, did you not do not know how many times we told y'all he's allergic to dairy? How do you make cheese? It's when they're like, yeah, it's especially like, you know, you ask them, Are they they say just dare. Does butter have dairy in it?\n\nAllergist 4:18 \nIf you're wondering, just err on the side. Oh, man. Yeah, it sounds like they've been kind of a source of many of his reactions.\n\nTom 4:29 \nYeah. Yeah. Okay, he's had he's had, he's had a dairy incident at literally every school and daycare he's ever been in. And it's like, it's all day. There's always some incident where he had he, somebody gives him something he shouldn't have. Although he is now old enough to read the labels himself, and you know, he now knows, you know, don't accept anything unless you know, for sure it's dairy free. But yeah, it was A scary back then. Yeah, it's\n\nAllergist 5:03 \nso unfortunate because they should be looking out for him because he's too little and envy, you know, sees everybody else is having a cupcake or something like, yeah, sorry for little kids to be like, Oh, I shouldn't eat this.\n\nTom 5:15 \nYeah, like, yeah, one time he picked up the glass of milk from the kid next to him and just chugged it, you know, didn't realize it was his cup, you know? Yeah.\n\nAllergist 5:26 \nWell, I'm glad that he's kind of a little bit older now. How long has it been since you guys have kind of done any sort of like allergy tests, whether it's a skin test or bloodwork?\n\nTom 5:36 \nSo in, in background in Arizona, he had a full panel prick test, as well as a bunch of at least have some I don't even I can't remember what like how many different things he was tested for. On the blood test, or the prick test. But it seems like a lot of allergist want to do prick tests, and instead of blood test, and then I had some I had another doctor tell me he said, If the allergist only wants to do prick test, and they don't do blood test, run away from him. That's what he said. He said, allergists that only do PARCC tests are just trying to milk as much money on the insurance companies as they can. That's what this other doctor told me. But my partner, she firmly believes in prick tests, and she doesn't. She doesn't necessarily prescribe to that same view. So whatever. But so we we do both. Yeah, I like the idea of both. I like the idea of bonus. So\n\nAllergist 6:35 \nskin tests tend to be a little bit more, I don't wanna say this, like revealing of you know, how how significant the allergy may be how someone may react. But at the same time, we just we feel like it's a good idea to to get both because sometimes you'll test just one and maybe it shows up as a very low level. And if you test the other one, it might be high. And so it just gives us a better picture. Obviously, his history tells us that he has anaphylaxis to this. So we already kind of can gauge the severity. But where kind of both come into play a lot more in our office with the treatment that we do is, over time, we will repeat those results so that we can monitor the trends and eventually, hopefully see those numbers trends downward. How long do you think it's been since his his test in Arizona? Was that like last year?\n\nTom 7:32 \nSo he was allergy tested with the prick test in like the last month that at? Like multi care our I don't know, there's a met, there's an allergist place out in Lakewood that we took him to recently. They they actually were the ones who mentioned you all when I asked when I pressed the issue. So that's kind of how I found out about y'all.\n\nAllergist 8:02 \nYeah, great. So what I'll maybe do after we're done talking, I will send you a form, it's a release of information or request form, you complete it and send it back to us. That'll give us permission to fax it to multicast or whoever did this yen test. And we can we can obtain those records. So we at least know his if his per test was a month ago, we don't need to redo it, we typically will only redo it if if it's more than a year out. If it's more than a year, we want a more updated result. But so that way we can request their records and just have have at least a baseline for him on file.\n\nTom 8:44 \nSure. Yeah, we can definitely do that. No problem. I suppose I thought we'd already released it. But okay, I'll do. I'm not sure I might not have made that form didn't have that.\n\nAllergist 8:58 \nYeah, I just don't see anything, at least in his chart. From what I can see all always double check. But what is kind of, we just need your permission to reach out to them and request those records without your permission. They obviously won't give them to us.\n\nTom 9:13 \nYeah, let me see here. I think it's allergy. It's\n\nasthma and allergy specialty services. at Lakewood. I don't know what you need for me for that release thing or whatever. But\n\nAllergist 9:33 \nyeah, so I'll send you I'll send you the form and then you just fill it out and sign it and send it back to us. Okay. Okay. And then we'll we'll fax them and request the they fax us the\n\nTom 9:43 \nresults. Okay, yeah, no problem. And then, you\n\nAllergist 9:47 \nknow, obviously, it'll be great to see where those results live, but because we do have a couple of forms of treatment, but based on the severity of his allergy, the treatment that Thomas would As a kind of a candidate for something called oral immunotherapy, or oh, it for short, okay. And so kind of what oh, it looks like is we bring you into clinic for an appointment, they are extended appointments, so you're scheduled for like three or so hours, doesn't mean you're going to be in clinic that long, but they are scheduled as long appointments. And what we do is we just start with a very small diluted dose of whatever the person's allergen is. And we have them consume that dose, we then will wait 20 minutes and monitor. After the 20 minute mark, we come back in repeat vitals, assess the patient, make sure there's no signs of allergic reaction. And then if everything looks like it's being well tolerated, we give a second dose. And we'll repeat that with the goal of being four doses per appointment. Whatever dose we get to in clinic, is the dose that we would send you guys home on. Okay, and you guys wouldn't give that to Thomas once a day, every day until you return to clinic to increase more. And so we kind of do this cycle until we get to our desired like goal dose. And then we call it our maintenance dose, once he gets to that maintenance dose, we then have him stay on that dose once a day, every day for three years at home. And we just kind of check in periodically, you know, one month after he reaches maintenance, then another three or six and then yearly if things are really going well. Typically, once a year will repeat bloodwork just to monitor the trends. And then at about the three year mark will repeat the skin test and the bloodwork and basically what we're looking for is an 80 to 90% reduction in his numbers to kind of signify a long term tolerance. And then at that point, we can always discuss, you know, decreasing the number of times in a week that he has to consume his dose with the ultimate goal, hopefully being, you know, just twice a week minimum to maintain any tolerance that we've built and continue to expose his body to the food. So that's kind of a general overview of how it works. Obviously, if there's any sort of adverse reaction in clinic, like let's say he gets a dose and he gets really sniffily, like force breaks out in some hives or something, obviously, we stop up dosing, we're not going to keep pushing at that point. We have all sorts of antihistamines, as well as epinephrine and those kinds of emergency meds on hand to treat any reaction that happens in clinic. And we would obviously send you guys home on a lower well tolerated dose, and then proceed up dosing. You know, the next visit some people we have to go a little bit more slowly than others. And we might not always do for up doses in an appointment. Okay. Do you have any questions kind of about the process?\n\nTom 13:24 \nNo, I mean, that that all lines up with my already existing understanding of how it all worked, although it's nice to have more specifics I am also interested in so he is he's currently taking their tech every night and has been for years, basically. And I'm concerned about that, because their tech has these like liberal warnings and other things. And I don't know, I just it, it also like it's like at a certain point like, is it even doing anything anymore? But then when we take him off of it, then he gets all sniffly. And it's like, well, what if we could just get him off of allergy meds daily allergy meds to his like seasonal allergies and his dog allergies? Like, are there shots or other treatments he could be doing as well, in addition to the dairy to solve those other allergies to Can we can we attack the whole front and just fix his allergies, that's what I'd really love to do.\n\nAllergist 14:17 \nSo, um, so you had mentioned the dog, you know that he gets runny nose, you know, hives, the whole kind of Gambit. So it sounds like he also has some environmental allergy symptoms as well. Yeah. similar symptoms with like runny nose, nasal congestion, those kind of\n\nTom 14:37 \nYeah, the vast majority of his symptoms exhibit as sinus post nasal drip, basically almost all of it. It's basically everything he does. 90% of times it results in sniffling and coughing. Okay. Yeah. Okay.\n\nAllergist 14:52 \nSo taking Zyrtec every day, is he doing any like no sprays or anything like that?\n\nTom 14:58 \nHe's doing Desertec eat has half a pill absurd tech he hasn't been doing any nose sprays.\n\nAllergist 15:05 \nOkay, so never tried like a over the counter phonies or anything like that.<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=1.1, frequency_penalty=0.0, repetition_penalty=1.1, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=185, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,553 client.py:72] Client received request chatcmpl-03b43b7870264fa8aa574130d1d6b795
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.555 [async_llm.py:270] Added request chatcmpl-03b43b7870264fa8aa574130d1d6b795.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,598 client.py:181] Client finished request chatcmpl-fc66f808b6d441c58ae2290bba57ef2d.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:37106 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(Llumlet(iid=909ad) pid=287181)[0m INFO 2025-07-16 11:53:04,651 core.py:157] Engine finished request chatcmpl-9008dcefc72a4c3db2e329a16f3e5943
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,653 client.py:181] Client finished request chatcmpl-9008dcefc72a4c3db2e329a16f3e5943.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO:     172.18.0.3:58264 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,735 client.py:121] Failed to abort request chatcmpl-7b8b72ae42ce4b28b95e032f323026ab (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.735 [async_llm.py:433] Aborted request chatcmpl-7b8b72ae42ce4b28b95e032f323026ab.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.735 [async_llm.py:339] Request chatcmpl-7b8b72ae42ce4b28b95e032f323026ab aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,736 client.py:121] Failed to abort request chatcmpl-9ff698bc827746978c154d591c8b7783 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.736 [async_llm.py:433] Aborted request chatcmpl-9ff698bc827746978c154d591c8b7783.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.736 [async_llm.py:339] Request chatcmpl-9ff698bc827746978c154d591c8b7783 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,737 client.py:121] Failed to abort request chatcmpl-f9bc8c5c282346ce85e9d87d97723091 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.737 [async_llm.py:433] Aborted request chatcmpl-f9bc8c5c282346ce85e9d87d97723091.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.737 [async_llm.py:339] Request chatcmpl-f9bc8c5c282346ce85e9d87d97723091 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,739 client.py:114] Abort request chatcmpl-e6f33e21dd0543c3bc3da5b0164d48df (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,739 client.py:114] Abort request chatcmpl-2e4ce95a1ac44190824569c947d17956 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,740 client.py:114] Abort request chatcmpl-ea30c2e6229244ab817b88e0b71e6057 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,740 client.py:114] Abort request chatcmpl-2ffaccd5565745bb8e7a3608e538ed51 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,742 client.py:121] Failed to abort request chatcmpl-98d95657f1db4d1486b932c0c9d5f99b (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.742 [async_llm.py:433] Aborted request chatcmpl-98d95657f1db4d1486b932c0c9d5f99b.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.743 [async_llm.py:339] Request chatcmpl-98d95657f1db4d1486b932c0c9d5f99b aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,743 client.py:114] Abort request chatcmpl-ad106d6dba9440d58a87e30a36b5ced8 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,743 client.py:114] Abort request chatcmpl-87121dca306e40c6bb33f773069763de (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,743 client.py:114] Abort request chatcmpl-07573aba9f1346c39a8858104bed19ae (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,744 client.py:121] Failed to abort request chatcmpl-96544d8293bd47a3abeb4cbbdf304dd7 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.744 [async_llm.py:433] Aborted request chatcmpl-96544d8293bd47a3abeb4cbbdf304dd7.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.744 [async_llm.py:339] Request chatcmpl-96544d8293bd47a3abeb4cbbdf304dd7 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,744 client.py:114] Abort request chatcmpl-6630645c4b294823a6155300a927282c (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,745 client.py:114] Abort request chatcmpl-7e395749998b428784255a24b65154a8 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,748 client.py:121] Failed to abort request chatcmpl-4d68a2aed7aa40fbb3aa4c7d05fddf2d (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.748 [async_llm.py:433] Aborted request chatcmpl-4d68a2aed7aa40fbb3aa4c7d05fddf2d.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.748 [async_llm.py:339] Request chatcmpl-4d68a2aed7aa40fbb3aa4c7d05fddf2d aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,748 client.py:121] Failed to abort request chatcmpl-0731dbcee59445bb989ffbde7f486cae (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.749 [async_llm.py:433] Aborted request chatcmpl-0731dbcee59445bb989ffbde7f486cae.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.749 [async_llm.py:339] Request chatcmpl-0731dbcee59445bb989ffbde7f486cae aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,749 client.py:114] Abort request chatcmpl-4b5b66c3d03c4b0cb6900a75537d2910 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,749 client.py:114] Abort request chatcmpl-5b5550c1e82f445fa532a9db65f0c8ac (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,749 client.py:121] Failed to abort request chatcmpl-d1093208c97444a4833ebe6af98faf38 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.749 [async_llm.py:433] Aborted request chatcmpl-d1093208c97444a4833ebe6af98faf38.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.749 [async_llm.py:339] Request chatcmpl-d1093208c97444a4833ebe6af98faf38 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,750 client.py:121] Failed to abort request chatcmpl-de59bcc5c55741db800847e302c3ed24 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.750 [async_llm.py:433] Aborted request chatcmpl-de59bcc5c55741db800847e302c3ed24.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.750 [async_llm.py:339] Request chatcmpl-de59bcc5c55741db800847e302c3ed24 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,750 client.py:121] Failed to abort request chatcmpl-3c918b7ab1294856ac606cc3e3ab5ff7 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.750 [async_llm.py:433] Aborted request chatcmpl-3c918b7ab1294856ac606cc3e3ab5ff7.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.750 [async_llm.py:339] Request chatcmpl-3c918b7ab1294856ac606cc3e3ab5ff7 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,750 client.py:114] Abort request chatcmpl-6ca72e7c53a54e65a4d98e1cd5055f83 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,750 client.py:121] Failed to abort request chatcmpl-c776fe6ab2514abfb82c5b857b04faf7 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.750 [async_llm.py:433] Aborted request chatcmpl-c776fe6ab2514abfb82c5b857b04faf7.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.751 [async_llm.py:339] Request chatcmpl-c776fe6ab2514abfb82c5b857b04faf7 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,751 client.py:114] Abort request chatcmpl-502120461af04dc984912111ef79f825 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,751 client.py:121] Failed to abort request chatcmpl-638dc7b6dd3c46b7a50614085a766203 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.751 [async_llm.py:433] Aborted request chatcmpl-638dc7b6dd3c46b7a50614085a766203.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.751 [async_llm.py:339] Request chatcmpl-638dc7b6dd3c46b7a50614085a766203 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,751 client.py:121] Failed to abort request chatcmpl-4dd4ead9d1144e4eb87b6736434cb2d1 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.751 [async_llm.py:433] Aborted request chatcmpl-4dd4ead9d1144e4eb87b6736434cb2d1.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.751 [async_llm.py:339] Request chatcmpl-4dd4ead9d1144e4eb87b6736434cb2d1 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,752 client.py:114] Abort request chatcmpl-e04e5205a92d4486aa5b549f33d9c65a (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,752 client.py:114] Abort request chatcmpl-abd7095e4c3f4eb8829a9bd4be1c195e (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,752 client.py:121] Failed to abort request chatcmpl-6d092cbc1e8a47d5a7d990b56fde4891 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.752 [async_llm.py:433] Aborted request chatcmpl-6d092cbc1e8a47d5a7d990b56fde4891.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.752 [async_llm.py:339] Request chatcmpl-6d092cbc1e8a47d5a7d990b56fde4891 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,753 client.py:121] Failed to abort request chatcmpl-125f1e0143ef488dbb80bf2d0d347970 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.753 [async_llm.py:433] Aborted request chatcmpl-125f1e0143ef488dbb80bf2d0d347970.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.753 [async_llm.py:339] Request chatcmpl-125f1e0143ef488dbb80bf2d0d347970 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,753 client.py:121] Failed to abort request chatcmpl-03b43b7870264fa8aa574130d1d6b795 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.753 [async_llm.py:433] Aborted request chatcmpl-03b43b7870264fa8aa574130d1d6b795.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.753 [async_llm.py:339] Request chatcmpl-03b43b7870264fa8aa574130d1d6b795 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,753 client.py:114] Abort request chatcmpl-37c57ebf5b3a43e6879e272aed9fbe73 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,754 client.py:114] Abort request chatcmpl-1a27b9a6826a4e2db8832eaeb0bcc7f3 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,754 client.py:114] Abort request chatcmpl-49f3e2c936534cbf8fb7102e303f20e4 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,754 client.py:121] Failed to abort request chatcmpl-cba1922726af4fdcbabc447b9161baa3 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.754 [async_llm.py:433] Aborted request chatcmpl-cba1922726af4fdcbabc447b9161baa3.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.754 [async_llm.py:339] Request chatcmpl-cba1922726af4fdcbabc447b9161baa3 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,755 client.py:114] Abort request chatcmpl-7145594eeea3431dac297f8893c0d019 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,755 client.py:114] Abort request chatcmpl-5c9f5abacad9444d9c3c7bf05a909764 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,755 client.py:114] Abort request chatcmpl-c8c56d076592440cb40fd7eda74bceee (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,755 client.py:121] Failed to abort request chatcmpl-d8dfbe0252e240818dc30e19f5956dde (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.755 [async_llm.py:433] Aborted request chatcmpl-d8dfbe0252e240818dc30e19f5956dde.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.755 [async_llm.py:339] Request chatcmpl-d8dfbe0252e240818dc30e19f5956dde aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,756 client.py:121] Failed to abort request chatcmpl-e315443adef743b79cf043ac92d9a59a (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.756 [async_llm.py:433] Aborted request chatcmpl-e315443adef743b79cf043ac92d9a59a.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.756 [async_llm.py:339] Request chatcmpl-e315443adef743b79cf043ac92d9a59a aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.761 [async_llm.py:433] Aborted request chatcmpl-e6f33e21dd0543c3bc3da5b0164d48df.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.761 [async_llm.py:339] Request chatcmpl-e6f33e21dd0543c3bc3da5b0164d48df aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.762 [async_llm.py:433] Aborted request chatcmpl-2e4ce95a1ac44190824569c947d17956.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.762 [async_llm.py:339] Request chatcmpl-2e4ce95a1ac44190824569c947d17956 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.762 [async_llm.py:433] Aborted request chatcmpl-ea30c2e6229244ab817b88e0b71e6057.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.762 [async_llm.py:339] Request chatcmpl-ea30c2e6229244ab817b88e0b71e6057 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.762 [async_llm.py:433] Aborted request chatcmpl-2ffaccd5565745bb8e7a3608e538ed51.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.762 [async_llm.py:339] Request chatcmpl-2ffaccd5565745bb8e7a3608e538ed51 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.762 [async_llm.py:433] Aborted request chatcmpl-ad106d6dba9440d58a87e30a36b5ced8.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.762 [async_llm.py:339] Request chatcmpl-ad106d6dba9440d58a87e30a36b5ced8 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.762 [async_llm.py:433] Aborted request chatcmpl-87121dca306e40c6bb33f773069763de.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.762 [async_llm.py:339] Request chatcmpl-87121dca306e40c6bb33f773069763de aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.762 [async_llm.py:433] Aborted request chatcmpl-07573aba9f1346c39a8858104bed19ae.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.762 [async_llm.py:339] Request chatcmpl-07573aba9f1346c39a8858104bed19ae aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,763 client.py:114] Abort request chatcmpl-e43e7f0b0f90476ea1c04f00d6e6ef00 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,763 client.py:114] Abort request chatcmpl-1fe5299ed8ff496291e7122e240a22e2 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,763 client.py:114] Abort request chatcmpl-70d94da35b2547169ef9abe199cdf107 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,763 client.py:121] Failed to abort request chatcmpl-dbed8f7fd8ea4f1c82a4f8c5a052a132 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.763 [async_llm.py:433] Aborted request chatcmpl-dbed8f7fd8ea4f1c82a4f8c5a052a132.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.763 [async_llm.py:339] Request chatcmpl-dbed8f7fd8ea4f1c82a4f8c5a052a132 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,763 client.py:114] Abort request chatcmpl-211c3c92f2124abd95ca4d799ce9b28c (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,764 client.py:114] Abort request chatcmpl-a3d37701213b40ffa6ad15b7011bea3b (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,764 client.py:121] Failed to abort request chatcmpl-da1dbb86192345b09d80c6d2a835d2b3 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.764 [async_llm.py:433] Aborted request chatcmpl-da1dbb86192345b09d80c6d2a835d2b3.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.764 [async_llm.py:339] Request chatcmpl-da1dbb86192345b09d80c6d2a835d2b3 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,764 client.py:114] Abort request chatcmpl-1456620e286d40fdb18af38b0e26b9c0 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,764 client.py:114] Abort request chatcmpl-910c2970fd6249aa921bd510b4f48b27 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,765 client.py:114] Abort request chatcmpl-f2ed410e999e4c39ac1aecf0cddadfd6 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,765 client.py:121] Failed to abort request chatcmpl-cd2e8ab71ef54c2eb9b455354f154167 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.765 [async_llm.py:433] Aborted request chatcmpl-cd2e8ab71ef54c2eb9b455354f154167.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.765 [async_llm.py:339] Request chatcmpl-cd2e8ab71ef54c2eb9b455354f154167 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,765 client.py:114] Abort request chatcmpl-3aae26a0ebc34fb68c91c8a9a537e286 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,766 client.py:114] Abort request chatcmpl-14741716dafc41778d1b10125f76e4ea (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,766 client.py:121] Failed to abort request chatcmpl-4c5e9038ac514ed5855f2cd636c57c3b (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.766 [async_llm.py:433] Aborted request chatcmpl-4c5e9038ac514ed5855f2cd636c57c3b.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.766 [async_llm.py:339] Request chatcmpl-4c5e9038ac514ed5855f2cd636c57c3b aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,766 client.py:114] Abort request chatcmpl-f3350a79083b42388b3e102973a1c20b (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,767 client.py:114] Abort request chatcmpl-1966a04bd0044b4992f3bedaf78c7608 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,767 client.py:121] Failed to abort request chatcmpl-808c401a334e401a9bc34fe1585b9fdb (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.767 [async_llm.py:433] Aborted request chatcmpl-808c401a334e401a9bc34fe1585b9fdb.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.767 [async_llm.py:339] Request chatcmpl-808c401a334e401a9bc34fe1585b9fdb aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,767 client.py:121] Failed to abort request chatcmpl-89ff14e0f9e8447f8500ff6a43959775 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.767 [async_llm.py:433] Aborted request chatcmpl-89ff14e0f9e8447f8500ff6a43959775.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.767 [async_llm.py:339] Request chatcmpl-89ff14e0f9e8447f8500ff6a43959775 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,767 client.py:121] Failed to abort request chatcmpl-48fe8803c91149298c2e7f680ff3308c (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.767 [async_llm.py:433] Aborted request chatcmpl-48fe8803c91149298c2e7f680ff3308c.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.767 [async_llm.py:339] Request chatcmpl-48fe8803c91149298c2e7f680ff3308c aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,767 client.py:114] Abort request chatcmpl-7c9badc4b579401eba3a2587aceb08c3 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,768 client.py:121] Failed to abort request chatcmpl-db67737ff65b467796e53056c5ca42e1 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.768 [async_llm.py:433] Aborted request chatcmpl-db67737ff65b467796e53056c5ca42e1.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.768 [async_llm.py:339] Request chatcmpl-db67737ff65b467796e53056c5ca42e1 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,768 client.py:114] Abort request chatcmpl-117716235d174e9d9c47b1227ba2ccaf (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,768 client.py:121] Failed to abort request chatcmpl-cee3190700b84f3989aebd3af0e6aee5 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.768 [async_llm.py:433] Aborted request chatcmpl-cee3190700b84f3989aebd3af0e6aee5.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.768 [async_llm.py:339] Request chatcmpl-cee3190700b84f3989aebd3af0e6aee5 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,768 client.py:114] Abort request chatcmpl-820633a3efdc4a3cb1673ec59b5b7459 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,769 client.py:114] Abort request chatcmpl-95d9de99c1a3456d920daad2a1ce045e (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,769 client.py:121] Failed to abort request chatcmpl-eb9dae4bc5d84086a27e0a6d3a39f320 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.769 [async_llm.py:433] Aborted request chatcmpl-eb9dae4bc5d84086a27e0a6d3a39f320.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.769 [async_llm.py:339] Request chatcmpl-eb9dae4bc5d84086a27e0a6d3a39f320 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,769 client.py:114] Abort request chatcmpl-6454c9192219473fb5fdfe96c885da9f (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,769 client.py:121] Failed to abort request chatcmpl-a9dd10ee9ede4923a6e77e6175d24d44 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.769 [async_llm.py:433] Aborted request chatcmpl-a9dd10ee9ede4923a6e77e6175d24d44.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.769 [async_llm.py:339] Request chatcmpl-a9dd10ee9ede4923a6e77e6175d24d44 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,770 client.py:121] Failed to abort request chatcmpl-b9bb5311287942beafb8668b75a2e3d3 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.770 [async_llm.py:433] Aborted request chatcmpl-b9bb5311287942beafb8668b75a2e3d3.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.770 [async_llm.py:339] Request chatcmpl-b9bb5311287942beafb8668b75a2e3d3 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,770 client.py:121] Failed to abort request chatcmpl-e1d8c12571534408a4532aaff5870764 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.770 [async_llm.py:433] Aborted request chatcmpl-e1d8c12571534408a4532aaff5870764.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.770 [async_llm.py:339] Request chatcmpl-e1d8c12571534408a4532aaff5870764 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,770 client.py:114] Abort request chatcmpl-4a2ec1df32de4f26b5a7e1a408fbff39 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,770 client.py:121] Failed to abort request chatcmpl-40e9670c03504c68b9b20050442a1210 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.770 [async_llm.py:433] Aborted request chatcmpl-40e9670c03504c68b9b20050442a1210.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.770 [async_llm.py:339] Request chatcmpl-40e9670c03504c68b9b20050442a1210 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,770 client.py:114] Abort request chatcmpl-9fab28518e2c42a984118fb88283f7d6 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,770 client.py:121] Failed to abort request chatcmpl-cbe6dfaed303412e9440ee0a800ce266 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.771 [async_llm.py:433] Aborted request chatcmpl-cbe6dfaed303412e9440ee0a800ce266.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.771 [async_llm.py:339] Request chatcmpl-cbe6dfaed303412e9440ee0a800ce266 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,771 client.py:121] Failed to abort request chatcmpl-ccd04e410ee0450f8fae2bb477151830 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.771 [async_llm.py:433] Aborted request chatcmpl-ccd04e410ee0450f8fae2bb477151830.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.771 [async_llm.py:339] Request chatcmpl-ccd04e410ee0450f8fae2bb477151830 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,771 client.py:114] Abort request chatcmpl-f97d63d4c0234139932827e34948601c (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,771 client.py:114] Abort request chatcmpl-8f2b51893ad44e33a60aa10124bfe5d9 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,771 client.py:114] Abort request chatcmpl-a62a882a7127433d9524f0c3ca100032 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,772 client.py:114] Abort request chatcmpl-352b03e1ccf74a25ada6bdfb8486cad9 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,772 client.py:114] Abort request chatcmpl-e34f97fff664443da7f4aab0f91fbb79 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,772 client.py:114] Abort request chatcmpl-fcea85015e3945918440062edac946f3 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,772 client.py:114] Abort request chatcmpl-9ec29e7803514a88a8108fd7e866e539 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,772 client.py:121] Failed to abort request chatcmpl-d930625fe4ac4e199504b82515afe2a9 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.772 [async_llm.py:433] Aborted request chatcmpl-d930625fe4ac4e199504b82515afe2a9.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.772 [async_llm.py:339] Request chatcmpl-d930625fe4ac4e199504b82515afe2a9 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,773 client.py:121] Failed to abort request chatcmpl-fa5426342970451da40727bf8b991978 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.773 [async_llm.py:433] Aborted request chatcmpl-fa5426342970451da40727bf8b991978.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.773 [async_llm.py:339] Request chatcmpl-fa5426342970451da40727bf8b991978 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,773 client.py:114] Abort request chatcmpl-69505b08fae24eacad0ebda5ff7cbf64 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,773 client.py:121] Failed to abort request chatcmpl-2bd47f3566794de38d23b55e9a3880fd (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.773 [async_llm.py:433] Aborted request chatcmpl-2bd47f3566794de38d23b55e9a3880fd.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.773 [async_llm.py:339] Request chatcmpl-2bd47f3566794de38d23b55e9a3880fd aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,773 client.py:121] Failed to abort request chatcmpl-4fc2a42029ff410fb6320adfaca331e0 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.773 [async_llm.py:433] Aborted request chatcmpl-4fc2a42029ff410fb6320adfaca331e0.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.773 [async_llm.py:339] Request chatcmpl-4fc2a42029ff410fb6320adfaca331e0 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,774 client.py:114] Abort request chatcmpl-579ae87a674a4253a55f59cc232f384c (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,774 client.py:121] Failed to abort request chatcmpl-c85fd5e2ccf342f79ca5dd0a6d37edc2 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.774 [async_llm.py:433] Aborted request chatcmpl-c85fd5e2ccf342f79ca5dd0a6d37edc2.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.774 [async_llm.py:339] Request chatcmpl-c85fd5e2ccf342f79ca5dd0a6d37edc2 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,774 client.py:121] Failed to abort request chatcmpl-209dbf07413848f6bddb9d6d0e216642 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.774 [async_llm.py:433] Aborted request chatcmpl-209dbf07413848f6bddb9d6d0e216642.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.774 [async_llm.py:339] Request chatcmpl-209dbf07413848f6bddb9d6d0e216642 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,774 client.py:114] Abort request chatcmpl-5ea4f1e7f7564b138e943797cfce45b6 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,774 client.py:114] Abort request chatcmpl-5660052f33764b11817b48539a4b44d6 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,775 client.py:114] Abort request chatcmpl-114f46193726498283bf07ff4e9d73dc (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,775 client.py:114] Abort request chatcmpl-977c0e426ec64ec3abb954908499c578 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,775 client.py:121] Failed to abort request chatcmpl-d78e541b988c467c97acbe08cf250890 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.775 [async_llm.py:433] Aborted request chatcmpl-d78e541b988c467c97acbe08cf250890.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.775 [async_llm.py:339] Request chatcmpl-d78e541b988c467c97acbe08cf250890 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,775 client.py:114] Abort request chatcmpl-503c106034314f82b35b79a87affebf4 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,775 client.py:114] Abort request chatcmpl-9954afb41b0849a3bfe85507c89dae37 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,776 client.py:121] Failed to abort request chatcmpl-bd79e8af262e4317816d5565669ef09e (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.776 [async_llm.py:433] Aborted request chatcmpl-bd79e8af262e4317816d5565669ef09e.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.776 [async_llm.py:339] Request chatcmpl-bd79e8af262e4317816d5565669ef09e aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,776 client.py:114] Abort request chatcmpl-e175107944fe4780bc7449d192394d72 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,776 client.py:114] Abort request chatcmpl-6822326ce1724eeca3f74c11312db760 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,777 client.py:114] Abort request chatcmpl-e54aefc6cd82402f8ded93d70ebf39d3 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,777 client.py:114] Abort request chatcmpl-58c746df67224d66b97593061f158fe2 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,777 client.py:114] Abort request chatcmpl-8d446bd599444a4ab612da9cbac62e15 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,777 client.py:121] Failed to abort request chatcmpl-665bd887d4184f1290d279b591243c9f (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.777 [async_llm.py:433] Aborted request chatcmpl-665bd887d4184f1290d279b591243c9f.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.777 [async_llm.py:339] Request chatcmpl-665bd887d4184f1290d279b591243c9f aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,777 client.py:121] Failed to abort request chatcmpl-b989919782f0481db2cabf634fba7355 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.778 [async_llm.py:433] Aborted request chatcmpl-b989919782f0481db2cabf634fba7355.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.778 [async_llm.py:339] Request chatcmpl-b989919782f0481db2cabf634fba7355 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,778 client.py:121] Failed to abort request chatcmpl-ecc25a5a782a442087b3acf806f44110 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.778 [async_llm.py:433] Aborted request chatcmpl-ecc25a5a782a442087b3acf806f44110.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.778 [async_llm.py:339] Request chatcmpl-ecc25a5a782a442087b3acf806f44110 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,778 client.py:121] Failed to abort request chatcmpl-18fdfba8c2f84b63b84ca4221ab05b99 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.778 [async_llm.py:433] Aborted request chatcmpl-18fdfba8c2f84b63b84ca4221ab05b99.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.778 [async_llm.py:339] Request chatcmpl-18fdfba8c2f84b63b84ca4221ab05b99 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,778 client.py:114] Abort request chatcmpl-414d871c424c46c9a4cb27c65bae8cd5 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,778 client.py:114] Abort request chatcmpl-bac9c895f09e401db152edb95c3e72f3 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,779 client.py:114] Abort request chatcmpl-36ea89b5370c4745ac46f4a3d1dae63d (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,779 client.py:121] Failed to abort request chatcmpl-cc47ee88852c4da0958813a86b2f749c (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.779 [async_llm.py:433] Aborted request chatcmpl-cc47ee88852c4da0958813a86b2f749c.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.779 [async_llm.py:339] Request chatcmpl-cc47ee88852c4da0958813a86b2f749c aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,779 client.py:114] Abort request chatcmpl-ae08cf55255f47e080f8f02cd72ae936 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,779 client.py:114] Abort request chatcmpl-53faed6301314068aaf57779bcfc1c72 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,779 client.py:114] Abort request chatcmpl-3baf39b5610e4802974b252a320bbc24 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,780 client.py:114] Abort request chatcmpl-6a6e522530504026ad413ed3b1dccad0 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,780 client.py:114] Abort request chatcmpl-dd10317391544b31ac6b60f877cb645d (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,780 client.py:121] Failed to abort request chatcmpl-5939f1ac5c1a4eb0b401264bbc98f2c1 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.780 [async_llm.py:433] Aborted request chatcmpl-5939f1ac5c1a4eb0b401264bbc98f2c1.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.780 [async_llm.py:339] Request chatcmpl-5939f1ac5c1a4eb0b401264bbc98f2c1 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,780 client.py:121] Failed to abort request chatcmpl-d3211354d3454abfb4bce14398b728bd (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.780 [async_llm.py:433] Aborted request chatcmpl-d3211354d3454abfb4bce14398b728bd.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.780 [async_llm.py:339] Request chatcmpl-d3211354d3454abfb4bce14398b728bd aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,781 client.py:114] Abort request chatcmpl-f5d1e4d349c347eeb075ff82dd5cf0f0 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,781 client.py:114] Abort request chatcmpl-3373d2b51791462abe18f45a67c154f0 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,781 client.py:121] Failed to abort request chatcmpl-1ff77b22591343c5bd16631db1c721df (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.781 [async_llm.py:433] Aborted request chatcmpl-1ff77b22591343c5bd16631db1c721df.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.781 [async_llm.py:339] Request chatcmpl-1ff77b22591343c5bd16631db1c721df aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,782 client.py:114] Abort request chatcmpl-0c1dc6f0a528413ba99959b438f19956 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.783 [async_llm.py:433] Aborted request chatcmpl-6630645c4b294823a6155300a927282c.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.783 [async_llm.py:339] Request chatcmpl-6630645c4b294823a6155300a927282c aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.784 [async_llm.py:433] Aborted request chatcmpl-7e395749998b428784255a24b65154a8.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.784 [async_llm.py:339] Request chatcmpl-7e395749998b428784255a24b65154a8 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.784 [async_llm.py:433] Aborted request chatcmpl-4b5b66c3d03c4b0cb6900a75537d2910.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.784 [async_llm.py:339] Request chatcmpl-4b5b66c3d03c4b0cb6900a75537d2910 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.784 [async_llm.py:433] Aborted request chatcmpl-5b5550c1e82f445fa532a9db65f0c8ac.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.784 [async_llm.py:339] Request chatcmpl-5b5550c1e82f445fa532a9db65f0c8ac aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.784 [async_llm.py:433] Aborted request chatcmpl-6ca72e7c53a54e65a4d98e1cd5055f83.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.784 [async_llm.py:339] Request chatcmpl-6ca72e7c53a54e65a4d98e1cd5055f83 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.785 [async_llm.py:433] Aborted request chatcmpl-502120461af04dc984912111ef79f825.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.785 [async_llm.py:339] Request chatcmpl-502120461af04dc984912111ef79f825 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.785 [async_llm.py:433] Aborted request chatcmpl-e04e5205a92d4486aa5b549f33d9c65a.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.785 [async_llm.py:339] Request chatcmpl-e04e5205a92d4486aa5b549f33d9c65a aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.785 [async_llm.py:433] Aborted request chatcmpl-abd7095e4c3f4eb8829a9bd4be1c195e.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.785 [async_llm.py:339] Request chatcmpl-abd7095e4c3f4eb8829a9bd4be1c195e aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.785 [async_llm.py:433] Aborted request chatcmpl-37c57ebf5b3a43e6879e272aed9fbe73.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.785 [async_llm.py:339] Request chatcmpl-37c57ebf5b3a43e6879e272aed9fbe73 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.785 [async_llm.py:433] Aborted request chatcmpl-1a27b9a6826a4e2db8832eaeb0bcc7f3.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.785 [async_llm.py:339] Request chatcmpl-1a27b9a6826a4e2db8832eaeb0bcc7f3 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.785 [async_llm.py:433] Aborted request chatcmpl-49f3e2c936534cbf8fb7102e303f20e4.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.785 [async_llm.py:339] Request chatcmpl-49f3e2c936534cbf8fb7102e303f20e4 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.785 [async_llm.py:433] Aborted request chatcmpl-7145594eeea3431dac297f8893c0d019.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.785 [async_llm.py:339] Request chatcmpl-7145594eeea3431dac297f8893c0d019 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.786 [async_llm.py:433] Aborted request chatcmpl-5c9f5abacad9444d9c3c7bf05a909764.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.786 [async_llm.py:339] Request chatcmpl-5c9f5abacad9444d9c3c7bf05a909764 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,786 client.py:121] Failed to abort request chatcmpl-17fcb313a9a44b8c8a24109e1410cf76 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.786 [async_llm.py:433] Aborted request chatcmpl-17fcb313a9a44b8c8a24109e1410cf76.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.786 [async_llm.py:339] Request chatcmpl-17fcb313a9a44b8c8a24109e1410cf76 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,786 client.py:114] Abort request chatcmpl-5613a67091454cb6b0046e91a3ff6034 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,786 client.py:114] Abort request chatcmpl-04d105446db441769326267dae9d6834 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,786 client.py:121] Failed to abort request chatcmpl-bb12abe5db5f42e797ad8e3e3b7ac80b (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.786 [async_llm.py:433] Aborted request chatcmpl-bb12abe5db5f42e797ad8e3e3b7ac80b.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.786 [async_llm.py:339] Request chatcmpl-bb12abe5db5f42e797ad8e3e3b7ac80b aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,786 client.py:121] Failed to abort request chatcmpl-6a0a6812dcce4723b9eb706e19f32ba0 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.787 [async_llm.py:433] Aborted request chatcmpl-6a0a6812dcce4723b9eb706e19f32ba0.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.787 [async_llm.py:339] Request chatcmpl-6a0a6812dcce4723b9eb706e19f32ba0 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,787 client.py:114] Abort request chatcmpl-ac806e9e0d4c41eeb45ace94d66b6ce3 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,787 client.py:114] Abort request chatcmpl-55f1a32cb6c84dfbb163db4b9459e273 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,787 client.py:121] Failed to abort request chatcmpl-0813cea4dbf047eb89516d47da507f4e (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.787 [async_llm.py:433] Aborted request chatcmpl-0813cea4dbf047eb89516d47da507f4e.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.787 [async_llm.py:339] Request chatcmpl-0813cea4dbf047eb89516d47da507f4e aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,787 client.py:114] Abort request chatcmpl-acd84856d983445dbd0ee2bdc95043ac (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,788 client.py:121] Failed to abort request chatcmpl-e86069a2fd774982ab13fa891b7e77d4 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.788 [async_llm.py:433] Aborted request chatcmpl-e86069a2fd774982ab13fa891b7e77d4.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.788 [async_llm.py:339] Request chatcmpl-e86069a2fd774982ab13fa891b7e77d4 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,788 client.py:114] Abort request chatcmpl-04220dcd384743b8bf6a734342943466 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,788 client.py:114] Abort request chatcmpl-4e8604d6daab47ce9baa0008a7d3c102 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,788 client.py:114] Abort request chatcmpl-663bc8714c244596856eeb4006ce6f4e (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,789 client.py:114] Abort request chatcmpl-094a5ce9c0894efd85836960816e9553 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,789 client.py:114] Abort request chatcmpl-ef2001e7425c45198b913df54de300c3 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,789 client.py:121] Failed to abort request chatcmpl-1e5589fad3e9468fa54cb8b74378b381 (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.789 [async_llm.py:433] Aborted request chatcmpl-1e5589fad3e9468fa54cb8b74378b381.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.789 [async_llm.py:339] Request chatcmpl-1e5589fad3e9468fa54cb8b74378b381 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,789 client.py:114] Abort request chatcmpl-c7d2d183cdea45a49c749f5d9f1ce9e1 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m WARNING 2025-07-16 11:53:04,790 client.py:121] Failed to abort request chatcmpl-ba3085b467e541b0b6a1d73b57040e2a (instance_id: None, instance: None).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.790 [async_llm.py:433] Aborted request chatcmpl-ba3085b467e541b0b6a1d73b57040e2a.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.790 [async_llm.py:339] Request chatcmpl-ba3085b467e541b0b6a1d73b57040e2a aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,790 client.py:114] Abort request chatcmpl-02191692354949128a3589d7ac9db255 (instance_id: 909ad421560a46629feb76815bafea61).
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.790 [async_llm.py:433] Aborted request chatcmpl-c8c56d076592440cb40fd7eda74bceee.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.790 [async_llm.py:339] Request chatcmpl-c8c56d076592440cb40fd7eda74bceee aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.791 [async_llm.py:433] Aborted request chatcmpl-e43e7f0b0f90476ea1c04f00d6e6ef00.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.791 [async_llm.py:339] Request chatcmpl-e43e7f0b0f90476ea1c04f00d6e6ef00 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.791 [async_llm.py:433] Aborted request chatcmpl-1fe5299ed8ff496291e7122e240a22e2.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.791 [async_llm.py:339] Request chatcmpl-1fe5299ed8ff496291e7122e240a22e2 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.791 [async_llm.py:433] Aborted request chatcmpl-70d94da35b2547169ef9abe199cdf107.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.791 [async_llm.py:339] Request chatcmpl-70d94da35b2547169ef9abe199cdf107 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.791 [async_llm.py:433] Aborted request chatcmpl-211c3c92f2124abd95ca4d799ce9b28c.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.791 [async_llm.py:339] Request chatcmpl-211c3c92f2124abd95ca4d799ce9b28c aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.791 [async_llm.py:433] Aborted request chatcmpl-a3d37701213b40ffa6ad15b7011bea3b.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.791 [async_llm.py:339] Request chatcmpl-a3d37701213b40ffa6ad15b7011bea3b aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.791 [async_llm.py:433] Aborted request chatcmpl-1456620e286d40fdb18af38b0e26b9c0.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.791 [async_llm.py:339] Request chatcmpl-1456620e286d40fdb18af38b0e26b9c0 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.792 [async_llm.py:433] Aborted request chatcmpl-910c2970fd6249aa921bd510b4f48b27.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.792 [async_llm.py:339] Request chatcmpl-910c2970fd6249aa921bd510b4f48b27 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.792 [async_llm.py:433] Aborted request chatcmpl-f2ed410e999e4c39ac1aecf0cddadfd6.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.792 [async_llm.py:339] Request chatcmpl-f2ed410e999e4c39ac1aecf0cddadfd6 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.792 [async_llm.py:433] Aborted request chatcmpl-3aae26a0ebc34fb68c91c8a9a537e286.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.792 [async_llm.py:339] Request chatcmpl-3aae26a0ebc34fb68c91c8a9a537e286 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.792 [async_llm.py:433] Aborted request chatcmpl-14741716dafc41778d1b10125f76e4ea.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.792 [async_llm.py:339] Request chatcmpl-14741716dafc41778d1b10125f76e4ea aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.792 [async_llm.py:433] Aborted request chatcmpl-f3350a79083b42388b3e102973a1c20b.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.792 [async_llm.py:339] Request chatcmpl-f3350a79083b42388b3e102973a1c20b aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.792 [async_llm.py:433] Aborted request chatcmpl-1966a04bd0044b4992f3bedaf78c7608.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.792 [async_llm.py:339] Request chatcmpl-1966a04bd0044b4992f3bedaf78c7608 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.792 [async_llm.py:433] Aborted request chatcmpl-7c9badc4b579401eba3a2587aceb08c3.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.792 [async_llm.py:339] Request chatcmpl-7c9badc4b579401eba3a2587aceb08c3 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.792 [async_llm.py:433] Aborted request chatcmpl-117716235d174e9d9c47b1227ba2ccaf.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.792 [async_llm.py:339] Request chatcmpl-117716235d174e9d9c47b1227ba2ccaf aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.793 [async_llm.py:433] Aborted request chatcmpl-820633a3efdc4a3cb1673ec59b5b7459.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.793 [async_llm.py:339] Request chatcmpl-820633a3efdc4a3cb1673ec59b5b7459 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.793 [async_llm.py:433] Aborted request chatcmpl-95d9de99c1a3456d920daad2a1ce045e.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.793 [async_llm.py:339] Request chatcmpl-95d9de99c1a3456d920daad2a1ce045e aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.793 [async_llm.py:433] Aborted request chatcmpl-6454c9192219473fb5fdfe96c885da9f.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.793 [async_llm.py:339] Request chatcmpl-6454c9192219473fb5fdfe96c885da9f aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.793 [async_llm.py:433] Aborted request chatcmpl-4a2ec1df32de4f26b5a7e1a408fbff39.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.793 [async_llm.py:339] Request chatcmpl-4a2ec1df32de4f26b5a7e1a408fbff39 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.793 [async_llm.py:433] Aborted request chatcmpl-9fab28518e2c42a984118fb88283f7d6.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.793 [async_llm.py:339] Request chatcmpl-9fab28518e2c42a984118fb88283f7d6 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.793 [async_llm.py:433] Aborted request chatcmpl-f97d63d4c0234139932827e34948601c.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.793 [async_llm.py:339] Request chatcmpl-f97d63d4c0234139932827e34948601c aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.793 [async_llm.py:433] Aborted request chatcmpl-8f2b51893ad44e33a60aa10124bfe5d9.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.793 [async_llm.py:339] Request chatcmpl-8f2b51893ad44e33a60aa10124bfe5d9 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.793 [async_llm.py:433] Aborted request chatcmpl-a62a882a7127433d9524f0c3ca100032.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.793 [async_llm.py:339] Request chatcmpl-a62a882a7127433d9524f0c3ca100032 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.793 [async_llm.py:433] Aborted request chatcmpl-352b03e1ccf74a25ada6bdfb8486cad9.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.794 [async_llm.py:339] Request chatcmpl-352b03e1ccf74a25ada6bdfb8486cad9 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.794 [async_llm.py:433] Aborted request chatcmpl-e34f97fff664443da7f4aab0f91fbb79.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.794 [async_llm.py:339] Request chatcmpl-e34f97fff664443da7f4aab0f91fbb79 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.794 [async_llm.py:433] Aborted request chatcmpl-fcea85015e3945918440062edac946f3.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.794 [async_llm.py:339] Request chatcmpl-fcea85015e3945918440062edac946f3 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.794 [async_llm.py:433] Aborted request chatcmpl-9ec29e7803514a88a8108fd7e866e539.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.794 [async_llm.py:339] Request chatcmpl-9ec29e7803514a88a8108fd7e866e539 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.794 [async_llm.py:433] Aborted request chatcmpl-69505b08fae24eacad0ebda5ff7cbf64.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.794 [async_llm.py:339] Request chatcmpl-69505b08fae24eacad0ebda5ff7cbf64 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.794 [async_llm.py:433] Aborted request chatcmpl-579ae87a674a4253a55f59cc232f384c.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.794 [async_llm.py:339] Request chatcmpl-579ae87a674a4253a55f59cc232f384c aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.794 [async_llm.py:433] Aborted request chatcmpl-5ea4f1e7f7564b138e943797cfce45b6.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.794 [async_llm.py:339] Request chatcmpl-5ea4f1e7f7564b138e943797cfce45b6 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.794 [async_llm.py:433] Aborted request chatcmpl-5660052f33764b11817b48539a4b44d6.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.794 [async_llm.py:339] Request chatcmpl-5660052f33764b11817b48539a4b44d6 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.794 [async_llm.py:433] Aborted request chatcmpl-114f46193726498283bf07ff4e9d73dc.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.794 [async_llm.py:339] Request chatcmpl-114f46193726498283bf07ff4e9d73dc aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.795 [async_llm.py:433] Aborted request chatcmpl-977c0e426ec64ec3abb954908499c578.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.795 [async_llm.py:339] Request chatcmpl-977c0e426ec64ec3abb954908499c578 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.795 [async_llm.py:433] Aborted request chatcmpl-503c106034314f82b35b79a87affebf4.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.795 [async_llm.py:339] Request chatcmpl-503c106034314f82b35b79a87affebf4 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.795 [async_llm.py:433] Aborted request chatcmpl-9954afb41b0849a3bfe85507c89dae37.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.795 [async_llm.py:339] Request chatcmpl-9954afb41b0849a3bfe85507c89dae37 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.795 [async_llm.py:433] Aborted request chatcmpl-e175107944fe4780bc7449d192394d72.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.795 [async_llm.py:339] Request chatcmpl-e175107944fe4780bc7449d192394d72 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.795 [async_llm.py:433] Aborted request chatcmpl-6822326ce1724eeca3f74c11312db760.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.795 [async_llm.py:339] Request chatcmpl-6822326ce1724eeca3f74c11312db760 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.795 [async_llm.py:433] Aborted request chatcmpl-e54aefc6cd82402f8ded93d70ebf39d3.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.795 [async_llm.py:339] Request chatcmpl-e54aefc6cd82402f8ded93d70ebf39d3 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.795 [async_llm.py:433] Aborted request chatcmpl-58c746df67224d66b97593061f158fe2.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.795 [async_llm.py:339] Request chatcmpl-58c746df67224d66b97593061f158fe2 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.795 [async_llm.py:433] Aborted request chatcmpl-8d446bd599444a4ab612da9cbac62e15.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.795 [async_llm.py:339] Request chatcmpl-8d446bd599444a4ab612da9cbac62e15 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.795 [async_llm.py:433] Aborted request chatcmpl-414d871c424c46c9a4cb27c65bae8cd5.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.795 [async_llm.py:339] Request chatcmpl-414d871c424c46c9a4cb27c65bae8cd5 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.796 [async_llm.py:433] Aborted request chatcmpl-bac9c895f09e401db152edb95c3e72f3.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.796 [async_llm.py:339] Request chatcmpl-bac9c895f09e401db152edb95c3e72f3 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.796 [async_llm.py:433] Aborted request chatcmpl-36ea89b5370c4745ac46f4a3d1dae63d.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.796 [async_llm.py:339] Request chatcmpl-36ea89b5370c4745ac46f4a3d1dae63d aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.796 [async_llm.py:433] Aborted request chatcmpl-ae08cf55255f47e080f8f02cd72ae936.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.796 [async_llm.py:339] Request chatcmpl-ae08cf55255f47e080f8f02cd72ae936 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.796 [async_llm.py:433] Aborted request chatcmpl-53faed6301314068aaf57779bcfc1c72.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.796 [async_llm.py:339] Request chatcmpl-53faed6301314068aaf57779bcfc1c72 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.796 [async_llm.py:433] Aborted request chatcmpl-3baf39b5610e4802974b252a320bbc24.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.796 [async_llm.py:339] Request chatcmpl-3baf39b5610e4802974b252a320bbc24 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.796 [async_llm.py:433] Aborted request chatcmpl-6a6e522530504026ad413ed3b1dccad0.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.796 [async_llm.py:339] Request chatcmpl-6a6e522530504026ad413ed3b1dccad0 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.796 [async_llm.py:433] Aborted request chatcmpl-dd10317391544b31ac6b60f877cb645d.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.796 [async_llm.py:339] Request chatcmpl-dd10317391544b31ac6b60f877cb645d aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.796 [async_llm.py:433] Aborted request chatcmpl-f5d1e4d349c347eeb075ff82dd5cf0f0.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.797 [async_llm.py:339] Request chatcmpl-f5d1e4d349c347eeb075ff82dd5cf0f0 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.797 [async_llm.py:433] Aborted request chatcmpl-3373d2b51791462abe18f45a67c154f0.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.797 [async_llm.py:339] Request chatcmpl-3373d2b51791462abe18f45a67c154f0 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-f97d63d4c0234139932827e34948601c out-of-order output, last completion tokens is 0, current completion tokens is 709, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-977c0e426ec64ec3abb954908499c578 out-of-order output, last completion tokens is 0, current completion tokens is 651, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-f3350a79083b42388b3e102973a1c20b out-of-order output, last completion tokens is 0, current completion tokens is 605, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-3aae26a0ebc34fb68c91c8a9a537e286 out-of-order output, last completion tokens is 0, current completion tokens is 584, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-f5d1e4d349c347eeb075ff82dd5cf0f0 out-of-order output, last completion tokens is 0, current completion tokens is 529, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-9fab28518e2c42a984118fb88283f7d6 out-of-order output, last completion tokens is 0, current completion tokens is 513, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-e54aefc6cd82402f8ded93d70ebf39d3 out-of-order output, last completion tokens is 0, current completion tokens is 509, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-5ea4f1e7f7564b138e943797cfce45b6 out-of-order output, last completion tokens is 0, current completion tokens is 499, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-14741716dafc41778d1b10125f76e4ea out-of-order output, last completion tokens is 0, current completion tokens is 475, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-ae08cf55255f47e080f8f02cd72ae936 out-of-order output, last completion tokens is 0, current completion tokens is 449, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-e43e7f0b0f90476ea1c04f00d6e6ef00 out-of-order output, last completion tokens is 0, current completion tokens is 430, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-4a2ec1df32de4f26b5a7e1a408fbff39 out-of-order output, last completion tokens is 0, current completion tokens is 426, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-69505b08fae24eacad0ebda5ff7cbf64 out-of-order output, last completion tokens is 0, current completion tokens is 423, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-8f2b51893ad44e33a60aa10124bfe5d9 out-of-order output, last completion tokens is 0, current completion tokens is 414, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-414d871c424c46c9a4cb27c65bae8cd5 out-of-order output, last completion tokens is 0, current completion tokens is 412, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-bac9c895f09e401db152edb95c3e72f3 out-of-order output, last completion tokens is 0, current completion tokens is 404, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-95d9de99c1a3456d920daad2a1ce045e out-of-order output, last completion tokens is 0, current completion tokens is 368, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-117716235d174e9d9c47b1227ba2ccaf out-of-order output, last completion tokens is 0, current completion tokens is 368, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-dd10317391544b31ac6b60f877cb645d out-of-order output, last completion tokens is 0, current completion tokens is 361, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-352b03e1ccf74a25ada6bdfb8486cad9 out-of-order output, last completion tokens is 0, current completion tokens is 348, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-58c746df67224d66b97593061f158fe2 out-of-order output, last completion tokens is 0, current completion tokens is 346, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-1456620e286d40fdb18af38b0e26b9c0 out-of-order output, last completion tokens is 0, current completion tokens is 338, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-6454c9192219473fb5fdfe96c885da9f out-of-order output, last completion tokens is 0, current completion tokens is 338, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-e34f97fff664443da7f4aab0f91fbb79 out-of-order output, last completion tokens is 0, current completion tokens is 337, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-36ea89b5370c4745ac46f4a3d1dae63d out-of-order output, last completion tokens is 0, current completion tokens is 333, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-6822326ce1724eeca3f74c11312db760 out-of-order output, last completion tokens is 0, current completion tokens is 329, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-211c3c92f2124abd95ca4d799ce9b28c out-of-order output, last completion tokens is 0, current completion tokens is 319, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-3373d2b51791462abe18f45a67c154f0 out-of-order output, last completion tokens is 0, current completion tokens is 316, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-1fe5299ed8ff496291e7122e240a22e2 out-of-order output, last completion tokens is 0, current completion tokens is 298, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-53faed6301314068aaf57779bcfc1c72 out-of-order output, last completion tokens is 0, current completion tokens is 288, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-a62a882a7127433d9524f0c3ca100032 out-of-order output, last completion tokens is 0, current completion tokens is 281, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-503c106034314f82b35b79a87affebf4 out-of-order output, last completion tokens is 0, current completion tokens is 266, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,797 client.py:212] request chatcmpl-e175107944fe4780bc7449d192394d72 out-of-order output, last completion tokens is 0, current completion tokens is 265, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,798 client.py:212] request chatcmpl-579ae87a674a4253a55f59cc232f384c out-of-order output, last completion tokens is 0, current completion tokens is 238, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,798 client.py:212] request chatcmpl-9954afb41b0849a3bfe85507c89dae37 out-of-order output, last completion tokens is 0, current completion tokens is 231, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,798 client.py:212] request chatcmpl-7c9badc4b579401eba3a2587aceb08c3 out-of-order output, last completion tokens is 0, current completion tokens is 212, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,798 client.py:212] request chatcmpl-70d94da35b2547169ef9abe199cdf107 out-of-order output, last completion tokens is 0, current completion tokens is 210, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,798 client.py:212] request chatcmpl-820633a3efdc4a3cb1673ec59b5b7459 out-of-order output, last completion tokens is 0, current completion tokens is 209, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,798 client.py:212] request chatcmpl-5660052f33764b11817b48539a4b44d6 out-of-order output, last completion tokens is 0, current completion tokens is 192, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,798 client.py:212] request chatcmpl-fcea85015e3945918440062edac946f3 out-of-order output, last completion tokens is 0, current completion tokens is 175, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,798 client.py:212] request chatcmpl-114f46193726498283bf07ff4e9d73dc out-of-order output, last completion tokens is 0, current completion tokens is 165, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,798 client.py:212] request chatcmpl-3baf39b5610e4802974b252a320bbc24 out-of-order output, last completion tokens is 0, current completion tokens is 165, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,798 client.py:212] request chatcmpl-f2ed410e999e4c39ac1aecf0cddadfd6 out-of-order output, last completion tokens is 0, current completion tokens is 165, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,798 client.py:212] request chatcmpl-6a6e522530504026ad413ed3b1dccad0 out-of-order output, last completion tokens is 0, current completion tokens is 165, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,798 client.py:212] request chatcmpl-9ec29e7803514a88a8108fd7e866e539 out-of-order output, last completion tokens is 0, current completion tokens is 152, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,798 client.py:212] request chatcmpl-8d446bd599444a4ab612da9cbac62e15 out-of-order output, last completion tokens is 0, current completion tokens is 152, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,798 client.py:212] request chatcmpl-910c2970fd6249aa921bd510b4f48b27 out-of-order output, last completion tokens is 0, current completion tokens is 149, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,798 client.py:212] request chatcmpl-1966a04bd0044b4992f3bedaf78c7608 out-of-order output, last completion tokens is 0, current completion tokens is 144, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 2025-07-16 11:53:04,798 client.py:212] request chatcmpl-a3d37701213b40ffa6ad15b7011bea3b out-of-order output, last completion tokens is 0, current completion tokens is 19, current tokens is 1, stash current output...
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.798 [async_llm.py:433] Aborted request chatcmpl-0c1dc6f0a528413ba99959b438f19956.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.798 [async_llm.py:339] Request chatcmpl-0c1dc6f0a528413ba99959b438f19956 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.798 [async_llm.py:433] Aborted request chatcmpl-5613a67091454cb6b0046e91a3ff6034.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.798 [async_llm.py:339] Request chatcmpl-5613a67091454cb6b0046e91a3ff6034 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.798 [async_llm.py:433] Aborted request chatcmpl-04d105446db441769326267dae9d6834.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.798 [async_llm.py:339] Request chatcmpl-04d105446db441769326267dae9d6834 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.798 [async_llm.py:433] Aborted request chatcmpl-ac806e9e0d4c41eeb45ace94d66b6ce3.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.798 [async_llm.py:339] Request chatcmpl-ac806e9e0d4c41eeb45ace94d66b6ce3 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.799 [async_llm.py:433] Aborted request chatcmpl-55f1a32cb6c84dfbb163db4b9459e273.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.799 [async_llm.py:339] Request chatcmpl-55f1a32cb6c84dfbb163db4b9459e273 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.799 [async_llm.py:433] Aborted request chatcmpl-acd84856d983445dbd0ee2bdc95043ac.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.799 [async_llm.py:339] Request chatcmpl-acd84856d983445dbd0ee2bdc95043ac aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.799 [async_llm.py:433] Aborted request chatcmpl-04220dcd384743b8bf6a734342943466.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.799 [async_llm.py:339] Request chatcmpl-04220dcd384743b8bf6a734342943466 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.799 [async_llm.py:433] Aborted request chatcmpl-4e8604d6daab47ce9baa0008a7d3c102.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.799 [async_llm.py:339] Request chatcmpl-4e8604d6daab47ce9baa0008a7d3c102 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.799 [async_llm.py:433] Aborted request chatcmpl-663bc8714c244596856eeb4006ce6f4e.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.799 [async_llm.py:339] Request chatcmpl-663bc8714c244596856eeb4006ce6f4e aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.799 [async_llm.py:433] Aborted request chatcmpl-094a5ce9c0894efd85836960816e9553.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.799 [async_llm.py:339] Request chatcmpl-094a5ce9c0894efd85836960816e9553 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.799 [async_llm.py:433] Aborted request chatcmpl-ef2001e7425c45198b913df54de300c3.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.799 [async_llm.py:339] Request chatcmpl-ef2001e7425c45198b913df54de300c3 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.800 [async_llm.py:433] Aborted request chatcmpl-c7d2d183cdea45a49c749f5d9f1ce9e1.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.800 [async_llm.py:339] Request chatcmpl-c7d2d183cdea45a49c749f5d9f1ce9e1 aborted.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.800 [async_llm.py:433] Aborted request chatcmpl-02191692354949128a3589d7ac9db255.
[36m(APIServerActorVLLMV1(iid=909ad) pid=287182)[0m INFO 07-16 11:53:04.800 [async_llm.py:339] Request chatcmpl-02191692354949128a3589d7ac9db255 aborted.
[36m(Scaler pid=286112)[0m DEBUG 2025-07-16 11:53:04,987 scaler.py:275] The number of alive placement groups has reached the max_instances.
[33m(raylet)[0m Raylet is terminated. Termination is unexpected. Possible reasons include: (1) SIGKILL by the user or system OOM killer, (2) Invalid memory access from Raylet causing SIGSEGV or SIGBUS, (3) Other termination signals. Last 20 lines of the Raylet logs:
    [2025-07-16 11:53:05,423 I 278989 278989] (raylet) node_manager.cc:1018: The leased worker 63cdec109afebf7767e760112229c6448ef816f3a62b6946defefc16 is killed because the owner process f5fac0c180ab59247cb27a8ed1ba2a5d79046d624a586d38b3d874a0 died.
    [2025-07-16 11:53:05,424 I 278989 278989] (raylet) node_manager.cc:1523: Disconnecting client, graceful=true, disconnect_type=1, has_creation_task_exception=false worker_id=d4a1fb3554bcf90c872e5c5b702003c9bbb67f5099dcd8acaa9a7477
    [2025-07-16 11:53:05,428 I 278989 278989] (raylet) node_manager.cc:1523: Disconnecting client, graceful=true, disconnect_type=1, has_creation_task_exception=false worker_id=208522b7477fb4de025d0868c2123deb16ac4bab88474c12f55c3588
    [2025-07-16 11:53:05,429 I 278989 278989] (raylet) node_manager.cc:1018: The leased worker 58f3014df6f0d3855d33b1d0ee584334477230ee28a5798ad53301c1 is killed because the owner process 208522b7477fb4de025d0868c2123deb16ac4bab88474c12f55c3588 died.
    [2025-07-16 11:53:05,430 I 278989 278989] (raylet) node_manager.cc:1523: Disconnecting client, graceful=true, disconnect_type=1, has_creation_task_exception=false worker_id=361f73ade5c4da9edf56233b9c44f3296745820a3b77a91216087d29
    [2025-07-16 11:53:05,433 W 278989 279020] (raylet) store.cc:368: Disconnecting client due to connection error with code 2: End of file
    [2025-07-16 11:53:05,435 I 278989 278989] (raylet) node_manager.cc:1523: Disconnecting client, graceful=true, disconnect_type=1, has_creation_task_exception=false worker_id=63cdec109afebf7767e760112229c6448ef816f3a62b6946defefc16
    [2025-07-16 11:53:05,436 I 278989 278989] (raylet) node_manager.cc:1523: Disconnecting client, graceful=true, disconnect_type=1, has_creation_task_exception=false worker_id=ce3496e6427a1470720edfb4c2bba1b163832756493a48dba7b90a6e
    [2025-07-16 11:53:05,441 W 278989 279020] (raylet) store.cc:368: Disconnecting client due to connection error with code 2: End of file
    [2025-07-16 11:53:05,444 I 278989 278989] (raylet) node_manager.cc:1523: Disconnecting client, graceful=true, disconnect_type=1, has_creation_task_exception=false worker_id=58f3014df6f0d3855d33b1d0ee584334477230ee28a5798ad53301c1
    [2025-07-16 11:53:05,472 W 278989 279020] (raylet) store.cc:368: Disconnecting client due to connection error with code 2: End of file
    [2025-07-16 11:53:05,477 W 278989 279020] (raylet) store.cc:368: Disconnecting client due to connection error with code 2: End of file
    [2025-07-16 11:53:05,480 W 278989 279020] (raylet) store.cc:368: Disconnecting client due to connection error with code 2: End of file
    [2025-07-16 11:53:05,490 W 278989 279020] (raylet) store.cc:368: Disconnecting client due to connection error with code 2: End of file
    [2025-07-16 11:53:05,504 W 278989 279020] (raylet) store.cc:368: Disconnecting client due to connection error with code 2: End of file
    [2025-07-16 11:53:05,548 W 278989 279020] (raylet) store.cc:368: Disconnecting client due to connection error with code 2: End of file
    [2025-07-16 11:53:07,438 I 278989 278989] (raylet) node_manager.cc:1523: Disconnecting client, graceful=true, disconnect_type=3, has_creation_task_exception=false worker_id=01000000ffffffffffffffffffffffffffffffffffffffffffffffff
    [2025-07-16 11:53:07,438 I 278989 278989] (raylet) node_manager.cc:1615: Driver (pid=277376) is disconnected. worker_id=01000000ffffffffffffffffffffffffffffffffffffffffffffffff job_id=01000000
    [2025-07-16 11:53:07,439 I 278989 278989] (raylet) worker_pool.cc:724: Job 01000000 already started in worker pool.
    [2025-07-16 11:53:07,689 W 278989 279020] (raylet) store.cc:368: Disconnecting client due to connection error with code 2: End of file

[33m(raylet)[0m Raylet is terminated. Termination is unexpected. Possible reasons include: (1) SIGKILL by the user or system OOM killer, (2) Invalid memory access from Raylet causing SIGSEGV or SIGBUS, (3) Other termination signals. Last 20 lines of the Raylet logs:
    [2025-07-16 11:53:05,423 I 278989 278989] (raylet) node_manager.cc:1018: The leased worker 63cdec109afebf7767e760112229c6448ef816f3a62b6946defefc16 is killed because the owner process f5fac0c180ab59247cb27a8ed1ba2a5d79046d624a586d38b3d874a0 died.
    [2025-07-16 11:53:05,424 I 278989 278989] (raylet) node_manager.cc:1523: Disconnecting client, graceful=true, disconnect_type=1, has_creation_task_exception=false worker_id=d4a1fb3554bcf90c872e5c5b702003c9bbb67f5099dcd8acaa9a7477
    [2025-07-16 11:53:05,428 I 278989 278989] (raylet) node_manager.cc:1523: Disconnecting client, graceful=true, disconnect_type=1, has_creation_task_exception=false worker_id=208522b7477fb4de025d0868c2123deb16ac4bab88474c12f55c3588
    [2025-07-16 11:53:05,429 I 278989 278989] (raylet) node_manager.cc:1018: The leased worker 58f3014df6f0d3855d33b1d0ee584334477230ee28a5798ad53301c1 is killed because the owner process 208522b7477fb4de025d0868c2123deb16ac4bab88474c12f55c3588 died.
    [2025-07-16 11:53:05,430 I 278989 278989] (raylet) node_manager.cc:1523: Disconnecting client, graceful=true, disconnect_type=1, has_creation_task_exception=false worker_id=361f73ade5c4da9edf56233b9c44f3296745820a3b77a91216087d29
    [2025-07-16 11:53:05,433 W 278989 279020] (raylet) store.cc:368: Disconnecting client due to connection error with code 2: End of file
    [2025-07-16 11:53:05,435 I 278989 278989] (raylet) node_manager.cc:1523: Disconnecting client, graceful=true, disconnect_type=1, has_creation_task_exception=false worker_id=63cdec109afebf7767e760112229c6448ef816f3a62b6946defefc16
    [2025-07-16 11:53:05,436 I 278989 278989] (raylet) node_manager.cc:1523: Disconnecting client, graceful=true, disconnect_type=1, has_creation_task_exception=false worker_id=ce3496e6427a1470720edfb4c2bba1b163832756493a48dba7b90a6e
    [2025-07-16 11:53:05,441 W 278989 279020] (raylet) store.cc:368: Disconnecting client due to connection error with code 2: End of file
    [2025-07-16 11:53:05,444 I 278989 278989] (raylet) node_manager.cc:1523: Disconnecting client, graceful=true, disconnect_type=1, has_creation_task_exception=false worker_id=58f3014df6f0d3855d33b1d0ee584334477230ee28a5798ad53301c1
    [2025-07-16 11:53:05,472 W 278989 279020] (raylet) store.cc:368: Disconnecting client due to connection error with code 2: End of file
    [2025-07-16 11:53:05,477 W 278989 279020] (raylet) store.cc:368: Disconnecting client due to connection error with code 2: End of file
    [2025-07-16 11:53:05,480 W 278989 279020] (raylet) store.cc:368: Disconnecting client due to connection error with code 2: End of file
    [2025-07-16 11:53:05,490 W 278989 279020] (raylet) store.cc:368: Disconnecting client due to connection error with code 2: End of file
    [2025-07-16 11:53:05,504 W 278989 279020] (raylet) store.cc:368: Disconnecting client due to connection error with code 2: End of file
    [2025-07-16 11:53:05,548 W 278989 279020] (raylet) store.cc:368: Disconnecting client due to connection error with code 2: End of file
    [2025-07-16 11:53:07,438 I 278989 278989] (raylet) node_manager.cc:1523: Disconnecting client, graceful=true, disconnect_type=3, has_creation_task_exception=false worker_id=01000000ffffffffffffffffffffffffffffffffffffffffffffffff
    [2025-07-16 11:53:07,438 I 278989 278989] (raylet) node_manager.cc:1615: Driver (pid=277376) is disconnected. worker_id=01000000ffffffffffffffffffffffffffffffffffffffffffffffff job_id=01000000
    [2025-07-16 11:53:07,439 I 278989 278989] (raylet) worker_pool.cc:724: Job 01000000 already started in worker pool.
    [2025-07-16 11:53:07,689 W 278989 279020] (raylet) store.cc:368: Disconnecting client due to connection error with code 2: End of file

[2025-07-16 11:56:25,998 E 284379 285720] gcs_rpc_client.h:196: Failed to connect to GCS within 60 seconds. GCS may have been killed. It's either GCS is terminated by `ray stop` or is killed unexpectedly. If it is killed unexpectedly, see the log file gcs_server.out. https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#logging-directory-structure. The program will terminate.
